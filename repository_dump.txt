File structure:
- .DS_Store
- .dockerignore
- .github/workflows/ci.yml
- .gitignore
- AGENTS.md
- CHANGELOG.md
- CITATION.cff
- Dockerfile
- README.md
- RELEASE.md
- brain-tumor-run/.DS_Store
- brain-tumor-run/best.pt
- brain-tumor-run/debug_alignment/debug_alignment/gt.png
- brain-tumor-run/debug_alignment/debug_alignment/input.png
- brain-tumor-run/debug_alignment/debug_alignment/overlay.png
- brain-tumor-run/events.out.tfevents.1768149702.gn-0007.489705.0
- brain-tumor-run/metrics.csv
- brain-tumor-run/train.log
- brain-tumor-run/train_config_resolved.yaml
- brain-tumor-run/vis/.DS_Store
- brain-tumor-run/vis/epoch_10/case_0_gt.png
- brain-tumor-run/vis/epoch_10/case_0_input.png
- brain-tumor-run/vis/epoch_10/case_0_overlay.png
- brain-tumor-run/vis/epoch_10/case_0_pred.png
- brain-tumor-run/vis/epoch_10/case_1_gt.png
- brain-tumor-run/vis/epoch_10/case_1_input.png
- brain-tumor-run/vis/epoch_10/case_1_overlay.png
- brain-tumor-run/vis/epoch_10/case_1_pred.png
- brain-tumor-run/vis/epoch_10/case_2_gt.png
- brain-tumor-run/vis/epoch_10/case_2_input.png
- brain-tumor-run/vis/epoch_10/case_2_overlay.png
- brain-tumor-run/vis/epoch_10/case_2_pred.png
- brain-tumor-run/vis/epoch_20/case_0_gt.png
- brain-tumor-run/vis/epoch_20/case_0_input.png
- brain-tumor-run/vis/epoch_20/case_0_overlay.png
- brain-tumor-run/vis/epoch_20/case_0_pred.png
- brain-tumor-run/vis/epoch_20/case_1_gt.png
- brain-tumor-run/vis/epoch_20/case_1_input.png
- brain-tumor-run/vis/epoch_20/case_1_overlay.png
- brain-tumor-run/vis/epoch_20/case_1_pred.png
- brain-tumor-run/vis/epoch_20/case_2_gt.png
- brain-tumor-run/vis/epoch_20/case_2_input.png
- brain-tumor-run/vis/epoch_20/case_2_overlay.png
- brain-tumor-run/vis/epoch_20/case_2_pred.png
- brain-tumor-run/vis/epoch_30/case_0_gt.png
- brain-tumor-run/vis/epoch_30/case_0_input.png
- brain-tumor-run/vis/epoch_30/case_0_overlay.png
- brain-tumor-run/vis/epoch_30/case_0_pred.png
- brain-tumor-run/vis/epoch_30/case_1_gt.png
- brain-tumor-run/vis/epoch_30/case_1_input.png
- brain-tumor-run/vis/epoch_30/case_1_overlay.png
- brain-tumor-run/vis/epoch_30/case_1_pred.png
- brain-tumor-run/vis/epoch_30/case_2_gt.png
- brain-tumor-run/vis/epoch_30/case_2_input.png
- brain-tumor-run/vis/epoch_30/case_2_overlay.png
- brain-tumor-run/vis/epoch_30/case_2_pred.png
- configs/config.yaml
- configs/config_3d.yaml
- configs/config_3d_baseline.yaml
- docs/assets/baseline_curves.png
- docs/assets/baseline_examples.png
- outputs/.DS_Store
- outputs/baseline_metrics.json
- outputs/baseline_v1/20260111_154537/best.pt
- outputs/baseline_v1/20260111_154537/config_3d_baseline.yaml
- outputs/baseline_v1/20260111_154537/env.txt
- outputs/baseline_v1/20260111_154537/git_commit.txt
- outputs/baseline_v1/20260111_154537/gpu.txt
- outputs/baseline_v1/20260111_154537/metrics.csv
- outputs/baseline_v1/20260111_154537/metrics_3d.json
- outputs/baseline_v1/20260111_154537/metrics_per_epoch.json
- outputs/baseline_v1/20260111_154537/train_config_resolved.yaml
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_gt.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_input.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_overlay.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_pred.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_gt.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_input.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_overlay.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_pred.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_gt.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_input.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_overlay.png
- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_pred.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_gt.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_input.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_overlay.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_pred.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_gt.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_input.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_overlay.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_pred.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_gt.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_input.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_overlay.png
- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_pred.png
- outputs/metrics.csv
- outputs/metrics_3d.json
- outputs/runs/.DS_Store
- outputs/runs/20260111_145626/env.txt
- outputs/runs/20260111_145626/events.out.tfevents.1768161388.gn-0007.542966.0
- outputs/runs/20260111_145626/git_commit.txt
- outputs/runs/20260111_145626/gpu.txt
- outputs/runs/20260111_145626/metrics.csv
- outputs/runs/20260111_145626/metrics_per_epoch.json
- outputs/runs/20260111_145626/train_config_resolved.yaml
- outputs/runs/20260111_145626/vis/epoch_10/case_0_gt.png
- outputs/runs/20260111_145626/vis/epoch_10/case_0_input.png
- outputs/runs/20260111_145626/vis/epoch_10/case_0_overlay.png
- outputs/runs/20260111_145626/vis/epoch_10/case_0_pred.png
- outputs/runs/20260111_145626/vis/epoch_10/case_1_gt.png
- outputs/runs/20260111_145626/vis/epoch_10/case_1_input.png
- outputs/runs/20260111_145626/vis/epoch_10/case_1_overlay.png
- outputs/runs/20260111_145626/vis/epoch_10/case_1_pred.png
- outputs/runs/20260111_145626/vis/epoch_10/case_2_gt.png
- outputs/runs/20260111_145626/vis/epoch_10/case_2_input.png
- outputs/runs/20260111_145626/vis/epoch_10/case_2_overlay.png
- outputs/runs/20260111_145626/vis/epoch_10/case_2_pred.png
- outputs/runs/20260111_145626/vis/epoch_20/case_0_gt.png
- outputs/runs/20260111_145626/vis/epoch_20/case_0_input.png
- outputs/runs/20260111_145626/vis/epoch_20/case_0_overlay.png
- outputs/runs/20260111_145626/vis/epoch_20/case_0_pred.png
- outputs/runs/20260111_145626/vis/epoch_20/case_1_gt.png
- outputs/runs/20260111_145626/vis/epoch_20/case_1_input.png
- outputs/runs/20260111_145626/vis/epoch_20/case_1_overlay.png
- outputs/runs/20260111_145626/vis/epoch_20/case_1_pred.png
- outputs/runs/20260111_145626/vis/epoch_20/case_2_gt.png
- outputs/runs/20260111_145626/vis/epoch_20/case_2_input.png
- outputs/runs/20260111_145626/vis/epoch_20/case_2_overlay.png
- outputs/runs/20260111_145626/vis/epoch_20/case_2_pred.png
- repository_dump.txt
- requirements-3d.txt
- requirements.txt
- scripts/debug_alignment.py
- scripts/debug_label_permutation.py
- scripts/download_msd_task01.py
- scripts/make_readme_figures.py
- scripts/run_baseline_3d.sh
- scripts/summarize_run.py
- src/.DS_Store
- src/__init__.py
- src/data/__init__.py
- src/data/augmentations.py
- src/data/bias_correction.py
- src/data/dataset.py
- src/data/msd_task01_3d.py
- src/data/prepare_slices.py
- src/eval.py
- src/eval_3d.py
- src/models/__init__.py
- src/models/unet.py
- src/service/__init__.py
- src/service/api.py
- src/train_3d.py
- src/training/__init__.py
- src/training/train.py
- src/utils/__init__.py
- src/utils/config.py
- tests/test_smoke_3d_pipeline.py
- tests/test_smoke_pipeline.py
- train.py

--- .DS_Store ---
(binary file, contents omitted)

--- .dockerignore ---
Dataset/
weights/
outputs/
logs/
__pycache__/
venv/
.git/
.vscode/

--- .github/workflows/ci.yml ---
name: ci

on:
  push:
  pull_request:

jobs:
  smoke:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.9"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-3d.txt

      - name: Import API module
        run: python -m src.service.api

      - name: Prepare slices help
        run: python -m src.data.prepare_slices --help

      - name: Run tests
        run: pytest -q

--- .gitignore ---
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
.Python

# Virtual environments
.venv/
venv/
env/

# Distribution / packaging
build/
dist/
*.egg-info/
*.egg

# IDE / editor
.DS_Store
.idea/
.vscode/

# Logs and cache
*.log
.pytest_cache/

# TensorBoard / checkpoints / outputs
outputs/runs/*/best.pt
brain-tumor-run/
logs/

# Weights and datasets
weights/
Dataset/
/data/
repository_dump.txt

# Docker
*.pid

# Misc
*.tmp
*.swp
smoke_dataset/

--- AGENTS.md ---
# Repository Guidelines

## Project Structure & Module Organization
- `src/` houses the core package.
  - `src/data/` preprocessing and dataset utilities (bias correction, slice prep, augmentations).
  - `src/models/` model definitions (U-Net).
  - `src/training/` training entrypoints and loops.
  - `src/service/` FastAPI inference service.
  - `src/utils/` config helpers.
- `configs/config.yaml` is the primary runtime configuration.
- `train.py` is the top-level training entrypoint.
- `requirements.txt` and `Dockerfile` define dependencies and container runtime.
- `Dataset/`, `weights/`, `outputs/`, and `logs/` are ignored and used for data, checkpoints, and artifacts.

## Build, Test, and Development Commands
- Install deps: `pip install -r requirements.txt`.
- Prepare slices: `python -m src.data.prepare_slices --dataset-root /path/to/brats2019 --output-root ./Dataset`.
- Optional bias correction: `python -m src.data.bias_correction --input-dir /path/to/brats2019 --output-dir /path/to/brats2019_preprocessed`.
- Train: `python train.py --config configs/config.yaml --data-root ./Dataset`.
- Run API (local): `uvicorn src.service.api:app --host 0.0.0.0 --port 8080`.
- Build container: `docker build -t brain-seg:latest .`.

## Coding Style & Naming Conventions
- Python code uses 4-space indentation and PEP 8-style naming (modules/functions `snake_case`, classes `CamelCase`).
- Keep filenames descriptive and colocate utilities with their domains (e.g., `src/data/*`).
- No formatter or linter is configured; keep changes minimal and readable.

## Testing Guidelines
- No automated test suite is present.
- If adding tests, document the framework and add a clear command in this file and `README.md`.

## Commit & Pull Request Guidelines
- Git history does not show a strict convention; use concise, descriptive commit messages (e.g., “add bias correction CLI flags”).
- PRs should include: a summary, linked issue (if any), and example commands or screenshots for user-facing changes (e.g., API responses).

## Configuration & Runtime Notes
- All runtime parameters live in `configs/config.yaml`; prefer config changes over hard-coded values.
- Model outputs and logs default to `./outputs/` (see `training` config). Keep large artifacts out of git.

--- CHANGELOG.md ---
# Changelog

## v1.0.0
- 3D MSD Task01 pipeline with alignment checks and robust metrics.
- Baseline run tooling: `scripts/run_baseline_3d.sh`, run summarization, README figures.
- Reproducibility artifacts (resolved config, env/gpu capture, metrics history).
- 2D pipeline retained for comparison.

--- CITATION.cff ---
cff-version: 1.2.0
title: "3D Brain Tumor Segmentation (MSD Task01) — Reproducible Baseline"
message: "If you use this repository, please cite it."
type: software
authors:
  - name: "Rahul Kadam"
version: "1.0.0"
date-released: "2026-01-11"

--- Dockerfile ---
# Lightweight production image for FastAPI inference
FROM python:3.9-slim

# Prevent Python from buffering stdout/stderr and writing .pyc files
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app

# System dependencies for OpenCV
RUN apt-get update && apt-get install -y --no-install-recommends \
    libgl1-mesa-glx \
    libglib2.0-0 \
 && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port for Vertex AI
EXPOSE 8080

# Start FastAPI with uvicorn
CMD ["uvicorn", "src.service.api:app", "--host", "0.0.0.0", "--port", "8080"]

--- README.md ---
# 3D Brain Tumor Segmentation (MSD Task01) — Reproducible Baseline

An end-to-end medical imaging pipeline for 3D brain tumor segmentation with strong correctness checks, reproducible configs, and GPU-ready training/evaluation.

## What This Repo Demonstrates
- **Data pipeline rigor**: NIfTI loading, modality normalization, ROI sampling safeguards, and label alignment checks.
- **Training correctness**: per-class Dice, foreground Dice (ignoring empty tumor), tumor-based checkpointing.
- **Reproducibility**: resolved configs, environment capture, and deterministic-ish runs.
- **Engineering hygiene**: scripts, structured outputs, and clear metrics artifacts.

## Quickstart (Baseline v1.0.0)

### 1) Create environment
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-3d.txt
```

### 2) Download dataset
```bash
python scripts/download_msd_task01.py
```

### 3) Run baseline (single GPU)
```bash
bash scripts/run_baseline_3d.sh
```

The script:
- Trains with `configs/config_3d_baseline.yaml`
- Evaluates the best checkpoint
- Copies artifacts into `outputs/baseline_v1/<timestamp>/`

Generate summary + figures:
```bash
python scripts/summarize_run.py --run-dir outputs/runs/<run_id>
python scripts/make_readme_figures.py --run-dir outputs/runs/<run_id>
```

## Results — Baseline v1.0.0

The baseline config uses **limited epochs/batches** to keep runtime under ~2 hours on a V100.

| GPU | ROI | Epochs | Best Epoch | Tumor Dice | Foreground Dice | Mean Dice | Background Dice | Dataset Split |
| --- | --- | ------ | ---------- | ---------- | ---------------- | --------- | --------------- | ------------- |
| Tesla V100-SXM2-16GB | 96³ | 20 | 19 | 0.2816 | 0.2816 | 0.5511 | 0.9445 | 70/20/10 |

![Baseline Examples](docs/assets/baseline_examples.png)
![Baseline Curves](docs/assets/baseline_curves.png)

## Monitoring
```bash
tensorboard --logdir outputs/runs
```
Logged scalars include loss, per-class Dice, foreground Dice, LR, and GPU memory. Visual overlays are saved every `vis_interval` epochs.

## 2D Baseline (Optional)
The original 2D slice pipeline remains available for comparison.
```bash
python -m src.data.prepare_slices --dataset-format msd_task01 --dataset-root ./data/raw/msd_task01/Task01_BrainTumour --output-root ./Dataset
python train.py --config configs/config.yaml --epochs 20
```

## Known Limitations / Next Improvements
- Longer training schedules and LR scheduling.
- Augmentations (3D flips/rotations/intensity jitter).
- Multiclass tumor labels (enhancing vs non-enhancing).
- Larger ROI for more context (if GPU memory allows).

## Serving (Local)
The FastAPI service supports **2D PNG inference** for quick demos:
```bash
uvicorn src.service.api:app --host 0.0.0.0 --port 8080
curl -F "file=@example.png" http://localhost:8080/predict -o mask.png
```

## Metric Conventions (3D)
- **Per-class Dice** is always reported (background + tumor).
- **Foreground Dice** ignores empty‑tumor patches/volumes.
- **Best checkpoint** is selected by tumor/foreground Dice (not background mean).

--- RELEASE.md ---
# Release Checklist (v1.0.0)

1) Run baseline:
   ```bash
   bash scripts/run_baseline_3d.sh
   ```
2) Summarize + figures:
   ```bash
   python scripts/summarize_run.py --run-dir outputs/runs/<run_id>
   python scripts/make_readme_figures.py --run-dir outputs/runs/<run_id>
   ```
3) Verify artifacts:
   - `outputs/baseline_v1/<timestamp>/` contains config, metrics, logs, and checkpoint.
   - `docs/assets/baseline_examples.png` and `docs/assets/baseline_curves.png` updated.
4) Update README table with values from `outputs/baseline_metrics.json`.
5) Tag and release:
   ```bash
   git tag -a v1.0.0 -m "v1.0.0 baseline"
   git push origin v1.0.0
   ```
6) Create GitHub Release notes (copy from CHANGELOG).

--- brain-tumor-run/.DS_Store ---
(binary file, contents omitted)

--- brain-tumor-run/best.pt ---
(binary file, contents omitted)

--- brain-tumor-run/debug_alignment/debug_alignment/gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/debug_alignment/debug_alignment/input.png ---
(binary file, contents omitted)

--- brain-tumor-run/debug_alignment/debug_alignment/overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/events.out.tfevents.1768149702.gn-0007.489705.0 ---
(binary file, contents omitted)

--- brain-tumor-run/metrics.csv ---
epoch,train_loss,val_mean_dice
1,1.178697,0.497266
2,0.966853,0.512520
3,0.840498,0.497801
4,0.778061,0.495862
5,0.723662,0.505168
6,0.734608,0.490460
7,0.714020,0.495984
8,0.701632,0.538606
9,0.679306,0.485429
10,0.678196,0.485926
11,0.689624,0.519951
12,0.703393,0.489391
13,0.692408,0.483383
14,0.695116,0.507068
15,0.658095,0.507900
16,0.683139,0.493844
17,0.701470,0.497138
18,0.669604,0.489056
19,0.679629,0.543669
20,0.671967,0.497269
21,0.651492,0.488529
22,0.684753,0.502255
23,0.660792,0.521872
24,0.677968,0.493707
25,0.646056,0.539977
26,0.660332,0.491729
27,0.677697,0.486315
28,0.619556,0.489235
29,0.636760,0.492267
30,0.665174,0.500275

--- brain-tumor-run/train.log ---
2026-01-11 11:41:42,319 | INFO | Train batches per epoch: 338 | Val batches: 97
2026-01-11 11:41:42,319 | INFO | Using device: cuda
2026-01-11 11:41:42,319 | INFO | ROI size: [96, 96, 96]
2026-01-11 11:41:42,320 | INFO | num_workers: 8
2026-01-11 11:41:42,320 | INFO | limit_train_batches: 150 | limit_val_batches: 20
2026-01-11 11:41:42,320 | INFO | GPU name: Tesla V100-SXM2-16GB
2026-01-11 11:41:42,320 | INFO | CUDA capability: (7, 0)
2026-01-11 11:41:42,320 | INFO | AMP enabled: True
2026-01-11 11:41:42,321 | WARNING | For best throughput set: OMP_NUM_THREADS=1 and MKL_NUM_THREADS=1
2026-01-11 11:41:42,323 | INFO | Epoch 1/30
2026-01-11 11:41:45,203 | INFO |   train step 1/338 loss=1.4284 avg_step=0.892s
2026-01-11 11:41:47,778 | INFO |   train step 21/338 loss=1.2677 avg_step=0.015s
2026-01-11 11:41:52,907 | INFO |   train step 41/338 loss=1.2469 avg_step=0.015s
2026-01-11 11:41:56,464 | INFO |   train step 61/338 loss=1.2062 avg_step=0.015s
2026-01-11 11:42:01,575 | INFO |   train step 81/338 loss=1.1738 avg_step=0.015s
2026-01-11 11:42:05,051 | INFO |   train step 101/338 loss=1.1429 avg_step=0.015s
2026-01-11 11:42:10,106 | INFO |   train step 121/338 loss=1.1230 avg_step=0.015s
2026-01-11 11:42:13,616 | INFO |   train step 141/338 loss=1.0670 avg_step=0.015s
2026-01-11 11:42:17,902 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:42:17,953 | INFO |   val step 1/97 mean_dice=0.4851
2026-01-11 11:42:21,213 | INFO | Epoch 1 summary | train_loss=1.1787 val_loss=1.0676 val_mean_dice=0.4973 lr=0.000100 gpu_mem_max_mb=142.8
2026-01-11 11:42:21,249 | INFO | Epoch 2/30
2026-01-11 11:42:23,167 | INFO |   train step 1/338 loss=1.0511 avg_step=0.022s
2026-01-11 11:42:26,696 | INFO |   train step 21/338 loss=1.0436 avg_step=0.016s
2026-01-11 11:42:31,738 | INFO |   train step 41/338 loss=0.9744 avg_step=0.015s
2026-01-11 11:42:35,231 | INFO |   train step 61/338 loss=0.9740 avg_step=0.015s
2026-01-11 11:42:40,281 | INFO |   train step 81/338 loss=0.9513 avg_step=0.015s
2026-01-11 11:42:43,738 | INFO |   train step 101/338 loss=0.9913 avg_step=0.015s
2026-01-11 11:42:48,683 | INFO |   train step 121/338 loss=0.9147 avg_step=0.015s
2026-01-11 11:42:52,251 | INFO |   train step 141/338 loss=0.9825 avg_step=0.015s
2026-01-11 11:42:55,969 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:42:55,970 | INFO |   val step 1/97 mean_dice=0.4829
2026-01-11 11:42:59,463 | INFO | Epoch 2 summary | train_loss=0.9669 val_loss=0.9223 val_mean_dice=0.5125 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:42:59,478 | INFO | Epoch 3/30
2026-01-11 11:43:01,265 | INFO |   train step 1/338 loss=0.9778 avg_step=0.019s
2026-01-11 11:43:04,923 | INFO |   train step 21/338 loss=0.8048 avg_step=0.016s
2026-01-11 11:43:09,848 | INFO |   train step 41/338 loss=0.8502 avg_step=0.016s
2026-01-11 11:43:13,534 | INFO |   train step 61/338 loss=0.8633 avg_step=0.015s
2026-01-11 11:43:18,502 | INFO |   train step 81/338 loss=0.7846 avg_step=0.015s
2026-01-11 11:43:22,114 | INFO |   train step 101/338 loss=0.7796 avg_step=0.015s
2026-01-11 11:43:27,162 | INFO |   train step 121/338 loss=1.0235 avg_step=0.015s
2026-01-11 11:43:30,671 | INFO |   train step 141/338 loss=0.8099 avg_step=0.015s
2026-01-11 11:43:34,306 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:43:34,307 | INFO |   val step 1/97 mean_dice=0.4919
2026-01-11 11:43:38,032 | INFO | Epoch 3 summary | train_loss=0.8405 val_loss=0.8128 val_mean_dice=0.4978 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:43:38,033 | INFO | Epoch 4/30
2026-01-11 11:43:39,899 | INFO |   train step 1/338 loss=0.8477 avg_step=0.020s
2026-01-11 11:43:43,483 | INFO |   train step 21/338 loss=0.7906 avg_step=0.015s
2026-01-11 11:43:48,642 | INFO |   train step 41/338 loss=0.7328 avg_step=0.015s
2026-01-11 11:43:52,136 | INFO |   train step 61/338 loss=0.7435 avg_step=0.014s
2026-01-11 11:43:57,312 | INFO |   train step 81/338 loss=0.7250 avg_step=0.015s
2026-01-11 11:44:00,799 | INFO |   train step 101/338 loss=0.7545 avg_step=0.016s
2026-01-11 11:44:05,899 | INFO |   train step 121/338 loss=0.8156 avg_step=0.016s
2026-01-11 11:44:09,396 | INFO |   train step 141/338 loss=0.7006 avg_step=0.016s
2026-01-11 11:44:12,954 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:44:12,955 | INFO |   val step 1/97 mean_dice=0.4725
2026-01-11 11:44:16,704 | INFO | Epoch 4 summary | train_loss=0.7781 val_loss=0.7832 val_mean_dice=0.4959 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:44:16,704 | INFO | Epoch 5/30
2026-01-11 11:44:18,531 | INFO |   train step 1/338 loss=0.7436 avg_step=0.023s
2026-01-11 11:44:22,086 | INFO |   train step 21/338 loss=0.7494 avg_step=0.016s
2026-01-11 11:44:27,085 | INFO |   train step 41/338 loss=0.6803 avg_step=0.015s
2026-01-11 11:44:30,640 | INFO |   train step 61/338 loss=0.7580 avg_step=0.015s
2026-01-11 11:44:35,577 | INFO |   train step 81/338 loss=0.7646 avg_step=0.015s
2026-01-11 11:44:39,222 | INFO |   train step 101/338 loss=0.6783 avg_step=0.015s
2026-01-11 11:44:44,258 | INFO |   train step 121/338 loss=0.7282 avg_step=0.015s
2026-01-11 11:44:47,832 | INFO |   train step 141/338 loss=0.8175 avg_step=0.016s
2026-01-11 11:44:51,401 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:44:51,402 | INFO |   val step 1/97 mean_dice=0.4915
2026-01-11 11:44:55,083 | INFO | Epoch 5 summary | train_loss=0.7237 val_loss=0.7017 val_mean_dice=0.5052 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:44:55,084 | INFO | Epoch 6/30
2026-01-11 11:44:56,845 | INFO |   train step 1/338 loss=0.6537 avg_step=0.020s
2026-01-11 11:45:00,436 | INFO |   train step 21/338 loss=0.6350 avg_step=0.016s
2026-01-11 11:45:05,456 | INFO |   train step 41/338 loss=0.7239 avg_step=0.015s
2026-01-11 11:45:08,986 | INFO |   train step 61/338 loss=0.8123 avg_step=0.015s
2026-01-11 11:45:14,066 | INFO |   train step 81/338 loss=0.9921 avg_step=0.015s
2026-01-11 11:45:17,574 | INFO |   train step 101/338 loss=0.7012 avg_step=0.015s
2026-01-11 11:45:22,678 | INFO |   train step 121/338 loss=0.7708 avg_step=0.015s
2026-01-11 11:45:26,239 | INFO |   train step 141/338 loss=0.6362 avg_step=0.016s
2026-01-11 11:45:29,792 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:45:29,793 | INFO |   val step 1/97 mean_dice=0.4696
2026-01-11 11:45:33,468 | INFO | Epoch 6 summary | train_loss=0.7346 val_loss=0.8065 val_mean_dice=0.4905 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:45:33,469 | INFO | Epoch 7/30
2026-01-11 11:45:35,303 | INFO |   train step 1/338 loss=0.7808 avg_step=0.019s
2026-01-11 11:45:38,852 | INFO |   train step 21/338 loss=0.6868 avg_step=0.016s
2026-01-11 11:45:43,768 | INFO |   train step 41/338 loss=0.6281 avg_step=0.015s
2026-01-11 11:45:47,355 | INFO |   train step 61/338 loss=0.6907 avg_step=0.015s
2026-01-11 11:45:52,258 | INFO |   train step 81/338 loss=0.7880 avg_step=0.016s
2026-01-11 11:45:55,914 | INFO |   train step 101/338 loss=0.8338 avg_step=0.015s
2026-01-11 11:46:00,741 | INFO |   train step 121/338 loss=0.6521 avg_step=0.015s
2026-01-11 11:46:04,454 | INFO |   train step 141/338 loss=0.7366 avg_step=0.015s
2026-01-11 11:46:07,992 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:46:07,993 | INFO |   val step 1/97 mean_dice=0.4846
2026-01-11 11:46:11,617 | INFO | Epoch 7 summary | train_loss=0.7140 val_loss=0.7628 val_mean_dice=0.4960 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:46:11,617 | INFO | Epoch 8/30
2026-01-11 11:46:13,529 | INFO |   train step 1/338 loss=0.8731 avg_step=0.018s
2026-01-11 11:46:16,998 | INFO |   train step 21/338 loss=0.6994 avg_step=0.015s
2026-01-11 11:46:21,998 | INFO |   train step 41/338 loss=0.5845 avg_step=0.015s
2026-01-11 11:46:25,569 | INFO |   train step 61/338 loss=0.6319 avg_step=0.015s
2026-01-11 11:46:30,593 | INFO |   train step 81/338 loss=0.7077 avg_step=0.015s
2026-01-11 11:46:34,107 | INFO |   train step 101/338 loss=0.6897 avg_step=0.015s
2026-01-11 11:46:38,977 | INFO |   train step 121/338 loss=0.8058 avg_step=0.015s
2026-01-11 11:46:42,612 | INFO |   train step 141/338 loss=0.9526 avg_step=0.015s
2026-01-11 11:46:46,145 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:46:46,146 | INFO |   val step 1/97 mean_dice=0.5000
2026-01-11 11:46:49,827 | INFO | Epoch 8 summary | train_loss=0.7016 val_loss=0.6960 val_mean_dice=0.5386 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:46:49,846 | INFO | Epoch 9/30
2026-01-11 11:46:51,783 | INFO |   train step 1/338 loss=0.7476 avg_step=0.018s
2026-01-11 11:46:55,431 | INFO |   train step 21/338 loss=0.5899 avg_step=0.016s
2026-01-11 11:47:00,333 | INFO |   train step 41/338 loss=0.5476 avg_step=0.016s
2026-01-11 11:47:03,926 | INFO |   train step 61/338 loss=0.4929 avg_step=0.016s
2026-01-11 11:47:08,850 | INFO |   train step 81/338 loss=0.6820 avg_step=0.016s
2026-01-11 11:47:12,506 | INFO |   train step 101/338 loss=0.5426 avg_step=0.016s
2026-01-11 11:47:17,351 | INFO |   train step 121/338 loss=0.8424 avg_step=0.015s
2026-01-11 11:47:21,016 | INFO |   train step 141/338 loss=0.6091 avg_step=0.015s
2026-01-11 11:47:24,568 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:47:24,574 | INFO |   val step 1/97 mean_dice=0.4979
2026-01-11 11:47:28,287 | INFO | Epoch 9 summary | train_loss=0.6793 val_loss=0.7719 val_mean_dice=0.4854 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:47:28,288 | INFO | Epoch 10/30
2026-01-11 11:47:30,272 | INFO |   train step 1/338 loss=0.6650 avg_step=0.017s
2026-01-11 11:47:33,842 | INFO |   train step 21/338 loss=0.6363 avg_step=0.016s
2026-01-11 11:47:38,896 | INFO |   train step 41/338 loss=0.5808 avg_step=0.015s
2026-01-11 11:47:42,425 | INFO |   train step 61/338 loss=0.6689 avg_step=0.015s
2026-01-11 11:47:47,496 | INFO |   train step 81/338 loss=0.6576 avg_step=0.015s
2026-01-11 11:47:50,945 | INFO |   train step 101/338 loss=0.6954 avg_step=0.015s
2026-01-11 11:47:56,108 | INFO |   train step 121/338 loss=0.5266 avg_step=0.015s
2026-01-11 11:47:59,595 | INFO |   train step 141/338 loss=0.7204 avg_step=0.015s
2026-01-11 11:48:03,110 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:48:03,111 | INFO |   val step 1/97 mean_dice=0.4879
2026-01-11 11:48:06,799 | INFO | Epoch 10 summary | train_loss=0.6782 val_loss=0.7519 val_mean_dice=0.4859 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:48:12,077 | INFO | Epoch 11/30
2026-01-11 11:48:13,889 | INFO |   train step 1/338 loss=0.9832 avg_step=0.018s
2026-01-11 11:48:17,462 | INFO |   train step 21/338 loss=0.6402 avg_step=0.015s
2026-01-11 11:48:22,362 | INFO |   train step 41/338 loss=0.7583 avg_step=0.015s
2026-01-11 11:48:25,941 | INFO |   train step 61/338 loss=0.7239 avg_step=0.015s
2026-01-11 11:48:30,830 | INFO |   train step 81/338 loss=0.8482 avg_step=0.015s
2026-01-11 11:48:34,426 | INFO |   train step 101/338 loss=0.5220 avg_step=0.015s
2026-01-11 11:48:39,519 | INFO |   train step 121/338 loss=0.5982 avg_step=0.015s
2026-01-11 11:48:43,087 | INFO |   train step 141/338 loss=0.6821 avg_step=0.015s
2026-01-11 11:48:46,603 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:48:46,604 | INFO |   val step 1/97 mean_dice=0.4859
2026-01-11 11:48:50,215 | INFO | Epoch 11 summary | train_loss=0.6896 val_loss=0.6800 val_mean_dice=0.5200 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:48:50,215 | INFO | Epoch 12/30
2026-01-11 11:48:52,190 | INFO |   train step 1/338 loss=0.9187 avg_step=0.016s
2026-01-11 11:48:55,713 | INFO |   train step 21/338 loss=0.6186 avg_step=0.015s
2026-01-11 11:49:00,779 | INFO |   train step 41/338 loss=0.5990 avg_step=0.015s
2026-01-11 11:49:04,212 | INFO |   train step 61/338 loss=0.6308 avg_step=0.015s
2026-01-11 11:49:09,235 | INFO |   train step 81/338 loss=0.6042 avg_step=0.015s
2026-01-11 11:49:12,673 | INFO |   train step 101/338 loss=0.6867 avg_step=0.015s
2026-01-11 11:49:17,732 | INFO |   train step 121/338 loss=0.9499 avg_step=0.015s
2026-01-11 11:49:21,208 | INFO |   train step 141/338 loss=0.6334 avg_step=0.015s
2026-01-11 11:49:24,741 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:49:24,744 | INFO |   val step 1/97 mean_dice=0.4793
2026-01-11 11:49:28,441 | INFO | Epoch 12 summary | train_loss=0.7034 val_loss=0.6981 val_mean_dice=0.4894 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:49:28,442 | INFO | Epoch 13/30
2026-01-11 11:49:30,291 | INFO |   train step 1/338 loss=0.6559 avg_step=0.017s
2026-01-11 11:49:33,829 | INFO |   train step 21/338 loss=0.7990 avg_step=0.016s
2026-01-11 11:49:38,774 | INFO |   train step 41/338 loss=0.7386 avg_step=0.016s
2026-01-11 11:49:42,301 | INFO |   train step 61/338 loss=0.8434 avg_step=0.016s
2026-01-11 11:49:47,363 | INFO |   train step 81/338 loss=0.5186 avg_step=0.016s
2026-01-11 11:49:50,835 | INFO |   train step 101/338 loss=0.8107 avg_step=0.015s
2026-01-11 11:49:55,930 | INFO |   train step 121/338 loss=0.5511 avg_step=0.015s
2026-01-11 11:49:59,415 | INFO |   train step 141/338 loss=0.6058 avg_step=0.015s
2026-01-11 11:50:02,940 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:50:02,941 | INFO |   val step 1/97 mean_dice=0.4926
2026-01-11 11:50:06,617 | INFO | Epoch 13 summary | train_loss=0.6924 val_loss=0.6934 val_mean_dice=0.4834 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:50:06,617 | INFO | Epoch 14/30
2026-01-11 11:50:08,381 | INFO |   train step 1/338 loss=0.6233 avg_step=0.022s
2026-01-11 11:50:11,999 | INFO |   train step 21/338 loss=0.4383 avg_step=0.017s
2026-01-11 11:50:16,980 | INFO |   train step 41/338 loss=0.8654 avg_step=0.015s
2026-01-11 11:50:20,623 | INFO |   train step 61/338 loss=0.6502 avg_step=0.015s
2026-01-11 11:50:25,552 | INFO |   train step 81/338 loss=0.6050 avg_step=0.015s
2026-01-11 11:50:29,151 | INFO |   train step 101/338 loss=0.5908 avg_step=0.015s
2026-01-11 11:50:34,002 | INFO |   train step 121/338 loss=0.6948 avg_step=0.015s
2026-01-11 11:50:37,634 | INFO |   train step 141/338 loss=0.6000 avg_step=0.016s
2026-01-11 11:50:41,177 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:50:41,178 | INFO |   val step 1/97 mean_dice=0.5823
2026-01-11 11:50:44,985 | INFO | Epoch 14 summary | train_loss=0.6951 val_loss=0.6657 val_mean_dice=0.5071 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:50:44,985 | INFO | Epoch 15/30
2026-01-11 11:50:46,777 | INFO |   train step 1/338 loss=0.5113 avg_step=0.019s
2026-01-11 11:50:50,393 | INFO |   train step 21/338 loss=0.4622 avg_step=0.016s
2026-01-11 11:50:55,461 | INFO |   train step 41/338 loss=0.6405 avg_step=0.015s
2026-01-11 11:50:58,896 | INFO |   train step 61/338 loss=0.8481 avg_step=0.015s
2026-01-11 11:51:04,000 | INFO |   train step 81/338 loss=0.7083 avg_step=0.015s
2026-01-11 11:51:07,437 | INFO |   train step 101/338 loss=0.6069 avg_step=0.015s
2026-01-11 11:51:12,468 | INFO |   train step 121/338 loss=0.5280 avg_step=0.015s
2026-01-11 11:51:15,971 | INFO |   train step 141/338 loss=0.6022 avg_step=0.015s
2026-01-11 11:51:19,543 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:51:19,543 | INFO |   val step 1/97 mean_dice=0.4927
2026-01-11 11:51:23,222 | INFO | Epoch 15 summary | train_loss=0.6581 val_loss=0.7064 val_mean_dice=0.5079 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:51:23,223 | INFO | Epoch 16/30
2026-01-11 11:51:25,164 | INFO |   train step 1/338 loss=0.5307 avg_step=0.018s
2026-01-11 11:51:28,733 | INFO |   train step 21/338 loss=0.6503 avg_step=0.015s
2026-01-11 11:51:33,639 | INFO |   train step 41/338 loss=0.7537 avg_step=0.015s
2026-01-11 11:51:37,249 | INFO |   train step 61/338 loss=0.8779 avg_step=0.015s
2026-01-11 11:51:42,147 | INFO |   train step 81/338 loss=0.5636 avg_step=0.015s
2026-01-11 11:51:45,784 | INFO |   train step 101/338 loss=0.6409 avg_step=0.015s
2026-01-11 11:51:50,453 | INFO |   train step 121/338 loss=0.5279 avg_step=0.015s
2026-01-11 11:51:54,364 | INFO |   train step 141/338 loss=0.6848 avg_step=0.015s
2026-01-11 11:51:57,870 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:51:57,871 | INFO |   val step 1/97 mean_dice=0.4867
2026-01-11 11:52:01,542 | INFO | Epoch 16 summary | train_loss=0.6831 val_loss=0.6630 val_mean_dice=0.4938 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:52:01,542 | INFO | Epoch 17/30
2026-01-11 11:52:03,347 | INFO |   train step 1/338 loss=0.6426 avg_step=0.017s
2026-01-11 11:52:06,854 | INFO |   train step 21/338 loss=0.5938 avg_step=0.015s
2026-01-11 11:52:11,881 | INFO |   train step 41/338 loss=0.8461 avg_step=0.016s
2026-01-11 11:52:15,436 | INFO |   train step 61/338 loss=0.8948 avg_step=0.015s
2026-01-11 11:52:20,336 | INFO |   train step 81/338 loss=0.7701 avg_step=0.015s
2026-01-11 11:52:23,871 | INFO |   train step 101/338 loss=0.5355 avg_step=0.015s
2026-01-11 11:52:28,847 | INFO |   train step 121/338 loss=0.6314 avg_step=0.015s
2026-01-11 11:52:32,532 | INFO |   train step 141/338 loss=0.5755 avg_step=0.015s
2026-01-11 11:52:36,079 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:52:36,080 | INFO |   val step 1/97 mean_dice=0.5088
2026-01-11 11:52:39,776 | INFO | Epoch 17 summary | train_loss=0.7015 val_loss=0.6392 val_mean_dice=0.4971 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:52:39,776 | INFO | Epoch 18/30
2026-01-11 11:52:41,623 | INFO |   train step 1/338 loss=0.5697 avg_step=0.021s
2026-01-11 11:52:45,186 | INFO |   train step 21/338 loss=0.5105 avg_step=0.015s
2026-01-11 11:52:50,191 | INFO |   train step 41/338 loss=0.7467 avg_step=0.015s
2026-01-11 11:52:53,640 | INFO |   train step 61/338 loss=0.4802 avg_step=0.015s
2026-01-11 11:52:58,609 | INFO |   train step 81/338 loss=0.7368 avg_step=0.015s
2026-01-11 11:53:02,024 | INFO |   train step 101/338 loss=0.5399 avg_step=0.015s
2026-01-11 11:53:06,925 | INFO |   train step 121/338 loss=0.6179 avg_step=0.015s
2026-01-11 11:53:10,477 | INFO |   train step 141/338 loss=0.7099 avg_step=0.015s
2026-01-11 11:53:14,289 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:53:14,290 | INFO |   val step 1/97 mean_dice=0.4852
2026-01-11 11:53:17,954 | INFO | Epoch 18 summary | train_loss=0.6696 val_loss=0.7085 val_mean_dice=0.4891 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:53:17,954 | INFO | Epoch 19/30
2026-01-11 11:53:19,871 | INFO |   train step 1/338 loss=0.5147 avg_step=0.017s
2026-01-11 11:53:23,365 | INFO |   train step 21/338 loss=0.5885 avg_step=0.015s
2026-01-11 11:53:28,400 | INFO |   train step 41/338 loss=0.6442 avg_step=0.015s
2026-01-11 11:53:31,821 | INFO |   train step 61/338 loss=0.4722 avg_step=0.015s
2026-01-11 11:53:36,862 | INFO |   train step 81/338 loss=0.6970 avg_step=0.015s
2026-01-11 11:53:40,334 | INFO |   train step 101/338 loss=0.8724 avg_step=0.015s
2026-01-11 11:53:45,354 | INFO |   train step 121/338 loss=0.5630 avg_step=0.015s
2026-01-11 11:53:48,913 | INFO |   train step 141/338 loss=0.5951 avg_step=0.015s
2026-01-11 11:53:52,540 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:53:52,541 | INFO |   val step 1/97 mean_dice=0.4873
2026-01-11 11:53:56,163 | INFO | Epoch 19 summary | train_loss=0.6796 val_loss=0.6571 val_mean_dice=0.5437 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:53:56,181 | INFO | Epoch 20/30
2026-01-11 11:53:57,999 | INFO |   train step 1/338 loss=0.6446 avg_step=0.020s
2026-01-11 11:54:01,596 | INFO |   train step 21/338 loss=0.5193 avg_step=0.016s
2026-01-11 11:54:06,638 | INFO |   train step 41/338 loss=0.9239 avg_step=0.015s
2026-01-11 11:54:10,013 | INFO |   train step 61/338 loss=0.9187 avg_step=0.016s
2026-01-11 11:54:15,027 | INFO |   train step 81/338 loss=0.5404 avg_step=0.016s
2026-01-11 11:54:18,614 | INFO |   train step 101/338 loss=0.6406 avg_step=0.016s
2026-01-11 11:54:23,687 | INFO |   train step 121/338 loss=0.4910 avg_step=0.015s
2026-01-11 11:54:27,094 | INFO |   train step 141/338 loss=0.6336 avg_step=0.015s
2026-01-11 11:54:30,611 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:54:30,611 | INFO |   val step 1/97 mean_dice=0.4920
2026-01-11 11:54:34,312 | INFO | Epoch 20 summary | train_loss=0.6720 val_loss=0.7149 val_mean_dice=0.4973 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:54:39,638 | INFO | Epoch 21/30
2026-01-11 11:54:41,571 | INFO |   train step 1/338 loss=0.6274 avg_step=0.020s
2026-01-11 11:54:45,032 | INFO |   train step 21/338 loss=0.5802 avg_step=0.015s
2026-01-11 11:54:50,043 | INFO |   train step 41/338 loss=0.7285 avg_step=0.015s
2026-01-11 11:54:53,510 | INFO |   train step 61/338 loss=0.4420 avg_step=0.015s
2026-01-11 11:54:58,564 | INFO |   train step 81/338 loss=0.6514 avg_step=0.015s
2026-01-11 11:55:02,085 | INFO |   train step 101/338 loss=0.6439 avg_step=0.015s
2026-01-11 11:55:07,194 | INFO |   train step 121/338 loss=0.5508 avg_step=0.015s
2026-01-11 11:55:10,649 | INFO |   train step 141/338 loss=0.4949 avg_step=0.015s
2026-01-11 11:55:14,181 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:55:14,182 | INFO |   val step 1/97 mean_dice=0.4919
2026-01-11 11:55:17,836 | INFO | Epoch 21 summary | train_loss=0.6515 val_loss=0.6743 val_mean_dice=0.4885 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:55:17,837 | INFO | Epoch 22/30
2026-01-11 11:55:19,587 | INFO |   train step 1/338 loss=0.8740 avg_step=0.018s
2026-01-11 11:55:23,381 | INFO |   train step 21/338 loss=0.6516 avg_step=0.017s
2026-01-11 11:55:28,196 | INFO |   train step 41/338 loss=0.5986 avg_step=0.016s
2026-01-11 11:55:31,909 | INFO |   train step 61/338 loss=0.6456 avg_step=0.016s
2026-01-11 11:55:36,707 | INFO |   train step 81/338 loss=0.5480 avg_step=0.016s
2026-01-11 11:55:40,364 | INFO |   train step 101/338 loss=0.7437 avg_step=0.016s
2026-01-11 11:55:45,127 | INFO |   train step 121/338 loss=0.6819 avg_step=0.016s
2026-01-11 11:55:48,956 | INFO |   train step 141/338 loss=0.6178 avg_step=0.016s
2026-01-11 11:55:52,574 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:55:52,575 | INFO |   val step 1/97 mean_dice=0.4985
2026-01-11 11:55:56,273 | INFO | Epoch 22 summary | train_loss=0.6848 val_loss=0.6951 val_mean_dice=0.5023 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:55:56,274 | INFO | Epoch 23/30
2026-01-11 11:55:58,103 | INFO |   train step 1/338 loss=0.6217 avg_step=0.021s
2026-01-11 11:56:01,624 | INFO |   train step 21/338 loss=0.5807 avg_step=0.016s
2026-01-11 11:56:06,702 | INFO |   train step 41/338 loss=0.4267 avg_step=0.016s
2026-01-11 11:56:10,157 | INFO |   train step 61/338 loss=0.7523 avg_step=0.016s
2026-01-11 11:56:15,217 | INFO |   train step 81/338 loss=1.0319 avg_step=0.015s
2026-01-11 11:56:18,671 | INFO |   train step 101/338 loss=0.7658 avg_step=0.015s
2026-01-11 11:56:23,680 | INFO |   train step 121/338 loss=0.7247 avg_step=0.015s
2026-01-11 11:56:27,199 | INFO |   train step 141/338 loss=0.5890 avg_step=0.015s
2026-01-11 11:56:30,715 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:56:30,716 | INFO |   val step 1/97 mean_dice=1.0000
2026-01-11 11:56:34,417 | INFO | Epoch 23 summary | train_loss=0.6608 val_loss=0.6115 val_mean_dice=0.5219 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:56:34,417 | INFO | Epoch 24/30
2026-01-11 11:56:36,300 | INFO |   train step 1/338 loss=0.7694 avg_step=0.024s
2026-01-11 11:56:39,757 | INFO |   train step 21/338 loss=0.5142 avg_step=0.015s
2026-01-11 11:56:44,584 | INFO |   train step 41/338 loss=0.5346 avg_step=0.015s
2026-01-11 11:56:48,400 | INFO |   train step 61/338 loss=0.5131 avg_step=0.015s
2026-01-11 11:56:53,163 | INFO |   train step 81/338 loss=1.0298 avg_step=0.015s
2026-01-11 11:56:57,067 | INFO |   train step 101/338 loss=0.9332 avg_step=0.016s
2026-01-11 11:57:01,580 | INFO |   train step 121/338 loss=0.8722 avg_step=0.016s
2026-01-11 11:57:05,555 | INFO |   train step 141/338 loss=0.6443 avg_step=0.016s
2026-01-11 11:57:09,030 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:57:09,031 | INFO |   val step 1/97 mean_dice=0.4967
2026-01-11 11:57:12,680 | INFO | Epoch 24 summary | train_loss=0.6780 val_loss=0.6507 val_mean_dice=0.4937 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:57:12,680 | INFO | Epoch 25/30
2026-01-11 11:57:14,560 | INFO |   train step 1/338 loss=0.5316 avg_step=0.017s
2026-01-11 11:57:18,070 | INFO |   train step 21/338 loss=0.5904 avg_step=0.016s
2026-01-11 11:57:23,162 | INFO |   train step 41/338 loss=0.6528 avg_step=0.015s
2026-01-11 11:57:26,728 | INFO |   train step 61/338 loss=0.8448 avg_step=0.015s
2026-01-11 11:57:31,770 | INFO |   train step 81/338 loss=0.4804 avg_step=0.015s
2026-01-11 11:57:35,221 | INFO |   train step 101/338 loss=0.6523 avg_step=0.015s
2026-01-11 11:57:40,138 | INFO |   train step 121/338 loss=0.7898 avg_step=0.015s
2026-01-11 11:57:43,687 | INFO |   train step 141/338 loss=0.5803 avg_step=0.016s
2026-01-11 11:57:47,250 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:57:47,251 | INFO |   val step 1/97 mean_dice=0.4981
2026-01-11 11:57:50,900 | INFO | Epoch 25 summary | train_loss=0.6461 val_loss=0.6718 val_mean_dice=0.5400 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:57:50,901 | INFO | Epoch 26/30
2026-01-11 11:57:52,791 | INFO |   train step 1/338 loss=0.5135 avg_step=0.017s
2026-01-11 11:57:56,317 | INFO |   train step 21/338 loss=0.7238 avg_step=0.015s
2026-01-11 11:58:01,327 | INFO |   train step 41/338 loss=0.6745 avg_step=0.015s
2026-01-11 11:58:04,831 | INFO |   train step 61/338 loss=0.7019 avg_step=0.015s
2026-01-11 11:58:09,863 | INFO |   train step 81/338 loss=0.5230 avg_step=0.015s
2026-01-11 11:58:13,248 | INFO |   train step 101/338 loss=0.8266 avg_step=0.015s
2026-01-11 11:58:18,360 | INFO |   train step 121/338 loss=0.6552 avg_step=0.015s
2026-01-11 11:58:21,820 | INFO |   train step 141/338 loss=0.5098 avg_step=0.015s
2026-01-11 11:58:25,343 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:58:25,344 | INFO |   val step 1/97 mean_dice=0.4970
2026-01-11 11:58:28,984 | INFO | Epoch 26 summary | train_loss=0.6603 val_loss=0.6318 val_mean_dice=0.4917 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:58:28,985 | INFO | Epoch 27/30
2026-01-11 11:58:30,730 | INFO |   train step 1/338 loss=0.6290 avg_step=0.018s
2026-01-11 11:58:34,233 | INFO |   train step 21/338 loss=0.4970 avg_step=0.015s
2026-01-11 11:58:39,209 | INFO |   train step 41/338 loss=0.6418 avg_step=0.015s
2026-01-11 11:58:42,759 | INFO |   train step 61/338 loss=0.5883 avg_step=0.015s
2026-01-11 11:58:47,692 | INFO |   train step 81/338 loss=0.6044 avg_step=0.016s
2026-01-11 11:58:51,526 | INFO |   train step 101/338 loss=0.6558 avg_step=0.017s
2026-01-11 11:58:56,193 | INFO |   train step 121/338 loss=0.5933 avg_step=0.016s
2026-01-11 11:58:59,929 | INFO |   train step 141/338 loss=0.5802 avg_step=0.016s
2026-01-11 11:59:03,418 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:59:03,419 | INFO |   val step 1/97 mean_dice=0.4966
2026-01-11 11:59:07,097 | INFO | Epoch 27 summary | train_loss=0.6777 val_loss=0.6549 val_mean_dice=0.4863 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:59:07,098 | INFO | Epoch 28/30
2026-01-11 11:59:09,225 | INFO |   train step 1/338 loss=0.6211 avg_step=0.017s
2026-01-11 11:59:12,730 | INFO |   train step 21/338 loss=0.6403 avg_step=0.015s
2026-01-11 11:59:17,776 | INFO |   train step 41/338 loss=0.5357 avg_step=0.015s
2026-01-11 11:59:21,265 | INFO |   train step 61/338 loss=0.5683 avg_step=0.015s
2026-01-11 11:59:26,357 | INFO |   train step 81/338 loss=0.6882 avg_step=0.015s
2026-01-11 11:59:29,804 | INFO |   train step 101/338 loss=0.6655 avg_step=0.015s
2026-01-11 11:59:34,823 | INFO |   train step 121/338 loss=0.7002 avg_step=0.015s
2026-01-11 11:59:38,302 | INFO |   train step 141/338 loss=0.6047 avg_step=0.015s
2026-01-11 11:59:41,810 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 11:59:41,811 | INFO |   val step 1/97 mean_dice=0.4866
2026-01-11 11:59:45,457 | INFO | Epoch 28 summary | train_loss=0.6196 val_loss=0.7216 val_mean_dice=0.4892 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 11:59:45,457 | INFO | Epoch 29/30
2026-01-11 11:59:47,247 | INFO |   train step 1/338 loss=0.6566 avg_step=0.017s
2026-01-11 11:59:50,819 | INFO |   train step 21/338 loss=0.5151 avg_step=0.015s
2026-01-11 11:59:55,727 | INFO |   train step 41/338 loss=0.8800 avg_step=0.015s
2026-01-11 11:59:59,290 | INFO |   train step 61/338 loss=0.6072 avg_step=0.015s
2026-01-11 12:00:04,304 | INFO |   train step 81/338 loss=0.6179 avg_step=0.015s
2026-01-11 12:00:07,854 | INFO |   train step 101/338 loss=0.6118 avg_step=0.015s
2026-01-11 12:00:12,857 | INFO |   train step 121/338 loss=0.6748 avg_step=0.015s
2026-01-11 12:00:16,363 | INFO |   train step 141/338 loss=0.5138 avg_step=0.015s
2026-01-11 12:00:19,930 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 12:00:19,931 | INFO |   val step 1/97 mean_dice=0.4972
2026-01-11 12:00:23,572 | INFO | Epoch 29 summary | train_loss=0.6368 val_loss=0.6400 val_mean_dice=0.4923 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 12:00:23,573 | INFO | Epoch 30/30
2026-01-11 12:00:25,358 | INFO |   train step 1/338 loss=0.6003 avg_step=0.016s
2026-01-11 12:00:28,956 | INFO |   train step 21/338 loss=0.6230 avg_step=0.015s
2026-01-11 12:00:33,876 | INFO |   train step 41/338 loss=0.7127 avg_step=0.016s
2026-01-11 12:00:37,527 | INFO |   train step 61/338 loss=1.1024 avg_step=0.015s
2026-01-11 12:00:42,452 | INFO |   train step 81/338 loss=0.8125 avg_step=0.015s
2026-01-11 12:00:46,076 | INFO |   train step 101/338 loss=0.7457 avg_step=0.015s
2026-01-11 12:00:50,851 | INFO |   train step 121/338 loss=0.4016 avg_step=0.015s
2026-01-11 12:00:54,651 | INFO |   train step 141/338 loss=0.3962 avg_step=0.015s
2026-01-11 12:00:58,293 | INFO | val shapes | images=(1, 4, 96, 96, 96) labels=(1, 2, 96, 96, 96) logits=(1, 2, 96, 96, 96) preds=(1, 2, 96, 96, 96)
2026-01-11 12:00:58,293 | INFO |   val step 1/97 mean_dice=0.4940
2026-01-11 12:01:01,894 | INFO | Epoch 30 summary | train_loss=0.6652 val_loss=0.6132 val_mean_dice=0.5003 lr=0.000100 gpu_mem_max_mb=156.3
2026-01-11 12:01:07,185 | INFO | Best val mean Dice: 0.5437
2026-01-11 12:01:07,191 | INFO | Run directory: outputs/runs/20260111_114141

--- brain-tumor-run/train_config_resolved.yaml ---
data:
  root: ./data/raw/msd_task01/Task01_BrainTumour
  train_ratio: 0.7
  val_ratio: 0.2
  seed: 42
  list_files:
    train: null
    val: null
    test: null
  label_mode: binary
  class_names:
  - background
  - tumor
  roi_size:
  - 96
  - 96
  - 96
  pos_ratio: 0.7
  percentiles:
  - 0.5
  - 99.5
model:
  in_channels: 4
  channels:
  - 16
  - 32
  - 64
  - 128
  - 128
  strides:
  - 2
  - 2
  - 2
  - 2
  num_res_units: 1
  norm: instance
training:
  batch_size: 1
  learning_rate: 0.0001
  max_epochs: 30
  num_workers: 8
  seed: 42
  deterministic: false
  output_dir: ./outputs/runs
  limit_train_batches: 150
  limit_val_batches: 20
  log_interval: 20
  vis_interval: 10
  max_vis_cases: 3
  debug_shapes: true
inference:
  roi_size:
  - 96
  - 96
  - 96
  overlap: 0.5
  sw_batch_size: 2

--- brain-tumor-run/vis/.DS_Store ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_0_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_0_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_0_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_0_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_1_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_1_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_1_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_1_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_2_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_2_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_2_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_10/case_2_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_0_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_0_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_0_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_0_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_1_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_1_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_1_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_1_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_2_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_2_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_2_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_20/case_2_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_0_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_0_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_0_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_0_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_1_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_1_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_1_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_1_pred.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_2_gt.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_2_input.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_2_overlay.png ---
(binary file, contents omitted)

--- brain-tumor-run/vis/epoch_30/case_2_pred.png ---
(binary file, contents omitted)

--- configs/config.yaml ---
# Default configuration for brain tumor segmentation

data:
  root: ./Dataset
  train_images: train_frames/train
  train_masks: train_masks/train
  val_images: val_frames/val
  val_masks: val_masks/val
  test_images: test_frames/test
  test_masks: test_masks/test
  class_names: [background, non-enhancing, edema, enhancing]
  image_size: 256

augmentation:
  enable: true

training:
  batch_size: 16
  epochs: 20
  learning_rate: 0.0001
  steps_per_epoch: null
  validation_steps: null
  early_stopping_patience: 10
  use_multiprocessing: true
  workers: 4
  seed: 42
  log_dir: ./outputs/logs
  checkpoint_dir: ./outputs/checkpoints
  checkpoint_filename: best_model_unet.h5

model:
  input_channels: 1
  base_filters: 32

--- configs/config_3d.yaml ---
# 3D pipeline configuration for MSD Task01_BrainTumour

data:
  root: ./data/raw/msd_task01/Task01_BrainTumour
  train_ratio: 0.7
  val_ratio: 0.2
  seed: 42
  list_files:
    train: null
    val: null
    test: null
  label_mode: binary
  class_names: [background, tumor]
  roi_size: [128, 128, 128]
  pos_ratio: 0.9
  max_pos_attempts: 10
  min_pos_voxel_frac: 0.01
  percentiles: [0.5, 99.5]

model:
  in_channels: 4
  channels: [16, 32, 64, 128, 128]
  strides: [2, 2, 2, 2]
  num_res_units: 1
  norm: instance

training:
  batch_size: 1
  learning_rate: 0.0001
  max_epochs: 30
  num_workers: 8
  seed: 42
  deterministic: false
  output_dir: ./outputs/runs
  limit_train_batches: 150
  limit_val_batches: 20
  log_interval: 20
  vis_interval: 10
  max_vis_cases: 3
  debug_shapes: false
  ignore_empty_foreground: true
  log_sanity_steps: 50
  prediction_threshold: 0.5
  pos_weight: 5.0

inference:
  roi_size: [96, 96, 96]
  overlap: 0.5
  sw_batch_size: 2
  ignore_empty_foreground: true

--- configs/config_3d_baseline.yaml ---
# 3D baseline configuration for MSD Task01_BrainTumour (v1.0.0)

data:
  root: ./data/raw/msd_task01/Task01_BrainTumour
  train_ratio: 0.7
  val_ratio: 0.2
  seed: 42
  list_files:
    train: null
    val: null
    test: null
  label_mode: binary
  class_names: [background, tumor]
  roi_size: [96, 96, 96]
  pos_ratio: 0.9
  max_pos_attempts: 10
  min_pos_voxel_frac: 0.01
  percentiles: [0.5, 99.5]

model:
  in_channels: 4
  channels: [16, 32, 64, 128, 128]
  strides: [2, 2, 2, 2]
  num_res_units: 1
  norm: instance

training:
  batch_size: 1
  learning_rate: 0.0001
  max_epochs: 20
  num_workers: 8
  seed: 42
  deterministic: false
  output_dir: ./outputs/runs
  limit_train_batches: 250
  limit_val_batches: 50
  log_interval: 20
  vis_interval: 10
  max_vis_cases: 3
  debug_shapes: false
  ignore_empty_foreground: true
  log_sanity_steps: 50
  prediction_threshold: 0.5
  pos_weight: 5.0

inference:
  roi_size: [96, 96, 96]
  overlap: 0.5
  sw_batch_size: 2
  ignore_empty_foreground: true

--- docs/assets/baseline_curves.png ---
(binary file, contents omitted)

--- docs/assets/baseline_examples.png ---
(binary file, contents omitted)

--- outputs/.DS_Store ---
(binary file, contents omitted)

--- outputs/baseline_metrics.json ---
{
  "dataset_format": "msd_task01",
  "label_mode": "binary",
  "roi_size": [
    96,
    96,
    96
  ],
  "epochs": 20,
  "best_epoch": 19,
  "dice_per_class": [
    0.944538414478302,
    0.2815742690649391
  ],
  "dice_background": 0.944538414478302,
  "dice_tumor": 0.2815742690649391,
  "foreground_dice": 0.2815742690649391,
  "ignore_empty_foreground": true,
  "prediction_threshold": 0.5,
  "sanity_tp_fp_fn": {
    "tp": 64886,
    "fp": 71118,
    "fn": 5895
  }
}

--- outputs/baseline_v1/20260111_154537/best.pt ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/config_3d_baseline.yaml ---
# 3D baseline configuration for MSD Task01_BrainTumour (v1.0.0)

data:
  root: ./data/raw/msd_task01/Task01_BrainTumour
  train_ratio: 0.7
  val_ratio: 0.2
  seed: 42
  list_files:
    train: null
    val: null
    test: null
  label_mode: binary
  class_names: [background, tumor]
  roi_size: [96, 96, 96]
  pos_ratio: 0.9
  max_pos_attempts: 10
  min_pos_voxel_frac: 0.01
  percentiles: [0.5, 99.5]

model:
  in_channels: 4
  channels: [16, 32, 64, 128, 128]
  strides: [2, 2, 2, 2]
  num_res_units: 1
  norm: instance

training:
  batch_size: 1
  learning_rate: 0.0001
  max_epochs: 20
  num_workers: 8
  seed: 42
  deterministic: false
  output_dir: ./outputs/runs
  limit_train_batches: 250
  limit_val_batches: 50
  log_interval: 20
  vis_interval: 10
  max_vis_cases: 3
  debug_shapes: false
  ignore_empty_foreground: true
  log_sanity_steps: 50
  prediction_threshold: 0.5
  pos_weight: 5.0

inference:
  roi_size: [96, 96, 96]
  overlap: 0.5
  sw_batch_size: 2
  ignore_empty_foreground: true

--- outputs/baseline_v1/20260111_154537/env.txt ---
Python 3.12.12

--- outputs/baseline_v1/20260111_154537/git_commit.txt ---
901211b

--- outputs/baseline_v1/20260111_154537/gpu.txt ---
GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-2bf47ea5-f6c0-205c-03c6-eff3d127cbf0)

--- outputs/baseline_v1/20260111_154537/metrics.csv ---
epoch,train_loss,val_loss,val_mean_dice,val_foreground_dice,val_dice_background,val_dice_tumor,lr
1,1.804618,1.809580,0.337177,0.069214,0.635595,0.069214,0.00010000
2,1.647916,1.748115,0.419185,0.099598,0.794547,0.099598,0.00010000
3,1.533839,1.691924,0.441040,0.094757,0.823331,0.094757,0.00010000
4,1.439287,1.640130,0.447292,0.079321,0.851750,0.079321,0.00010000
5,1.423300,1.574749,0.443360,0.087528,0.844706,0.087528,0.00010000
6,1.352130,1.554864,0.462256,0.100227,0.870391,0.100227,0.00010000
7,1.311753,1.505091,0.460095,0.132966,0.872322,0.132966,0.00010000
8,1.285457,1.499902,0.480972,0.128612,0.879632,0.128612,0.00010000
9,1.219036,1.524750,0.464430,0.096741,0.876620,0.096741,0.00010000
10,1.177963,1.463031,0.468883,0.086387,0.891118,0.086387,0.00010000
11,1.142838,1.361557,0.509454,0.173092,0.911592,0.173092,0.00010000
12,1.105199,1.361706,0.506310,0.171182,0.909911,0.171182,0.00010000
13,1.078035,1.378207,0.492081,0.149404,0.912448,0.149404,0.00010000
14,1.043892,1.330910,0.506870,0.183269,0.925771,0.183269,0.00010000
15,1.088492,1.258465,0.523854,0.168543,0.956696,0.168543,0.00010000
16,1.053251,1.226082,0.534858,0.226059,0.956686,0.226059,0.00010000
17,0.995685,1.326605,0.534173,0.199941,0.940384,0.199941,0.00010000
18,0.986123,1.313637,0.520848,0.167267,0.931300,0.167267,0.00010000
19,0.928755,1.223967,0.551110,0.281574,0.944538,0.281574,0.00010000
20,0.965269,1.260744,0.525334,0.190001,0.951868,0.190001,0.00010000

--- outputs/baseline_v1/20260111_154537/metrics_3d.json ---
{
  "dataset_format": "msd_task01",
  "label_mode": "binary",
  "val": {
    "dice_per_class": {
      "background": 0.98722887346425,
      "tumor": 0.016988756025741694
    },
    "mean_dice": 0.5021088147449959,
    "foreground_mean_dice": 0.016988756025741694,
    "dice_background": 0.98722887346425,
    "dice_tumor": 0.016988756025741694,
    "number_of_volumes": 97
  },
  "test": {
    "dice_per_class": {
      "background": 0.9879584555723229,
      "tumor": 0.023572556937305846
    },
    "mean_dice": 0.5057655062548143,
    "foreground_mean_dice": 0.023572556937305846,
    "dice_background": 0.9879584555723229,
    "dice_tumor": 0.023572556937305846,
    "number_of_volumes": 49
  },
  "inference": {
    "roi_size": [
      96,
      96,
      96
    ],
    "overlap": 0.5,
    "sw_batch_size": 2,
    "include_background": true,
    "ignore_empty_foreground": true,
    "pred_rule": "sigmoid>0.5"
  },
  "git_commit": "94bbd68"
}

--- outputs/baseline_v1/20260111_154537/metrics_per_epoch.json ---
[
  {
    "epoch": 1,
    "train_loss": 1.804617980480194,
    "val_loss": 1.809579575061798,
    "val_mean_dice": 0.33717746257781983,
    "val_foreground_dice": 0.06921386545582209,
    "val_dice_per_class": [
      0.635595155954361,
      0.06921386545582209
    ],
    "val_dice_background": 0.635595155954361,
    "val_dice_tumor": 0.06921386545582209,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 2,
    "train_loss": 1.6479155039787292,
    "val_loss": 1.7481150078773497,
    "val_mean_dice": 0.4191851818561554,
    "val_foreground_dice": 0.09959770811044356,
    "val_dice_per_class": [
      0.7945473730564118,
      0.09959770811044356
    ],
    "val_dice_background": 0.7945473730564118,
    "val_dice_tumor": 0.09959770811044356,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 3,
    "train_loss": 1.5338392219543457,
    "val_loss": 1.691923563480377,
    "val_mean_dice": 0.4410399258136749,
    "val_foreground_dice": 0.09475686833919157,
    "val_dice_per_class": [
      0.823330591917038,
      0.09475686833919157
    ],
    "val_dice_background": 0.823330591917038,
    "val_dice_tumor": 0.09475686833919157,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 4,
    "train_loss": 1.439286849975586,
    "val_loss": 1.6401302647590636,
    "val_mean_dice": 0.44729161739349366,
    "val_foreground_dice": 0.07932097819459025,
    "val_dice_per_class": [
      0.8517499053478241,
      0.07932097819459025
    ],
    "val_dice_background": 0.8517499053478241,
    "val_dice_tumor": 0.07932097819459025,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 5,
    "train_loss": 1.4233003630638124,
    "val_loss": 1.574749126434326,
    "val_mean_dice": 0.44335984528064726,
    "val_foreground_dice": 0.08752848583587063,
    "val_dice_per_class": [
      0.8447060132026672,
      0.08752848583587063
    ],
    "val_dice_background": 0.8447060132026672,
    "val_dice_tumor": 0.08752848583587063,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 6,
    "train_loss": 1.35212952709198,
    "val_loss": 1.5548641729354857,
    "val_mean_dice": 0.46225647151470184,
    "val_foreground_dice": 0.10022668323155678,
    "val_dice_per_class": [
      0.8703905320167542,
      0.10022668323155678
    ],
    "val_dice_background": 0.8703905320167542,
    "val_dice_tumor": 0.10022668323155678,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 7,
    "train_loss": 1.3117529258728027,
    "val_loss": 1.5050909090042115,
    "val_mean_dice": 0.46009465992450715,
    "val_foreground_dice": 0.1329657798907849,
    "val_dice_per_class": [
      0.8723216390609742,
      0.1329657798907849
    ],
    "val_dice_background": 0.8723216390609742,
    "val_dice_tumor": 0.1329657798907849,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 8,
    "train_loss": 1.2854572594165803,
    "val_loss": 1.4999022197723388,
    "val_mean_dice": 0.48097169160842895,
    "val_foreground_dice": 0.12861225873894,
    "val_dice_per_class": [
      0.8796315395832062,
      0.12861225873894
    ],
    "val_dice_background": 0.8796315395832062,
    "val_dice_tumor": 0.12861225873894,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 9,
    "train_loss": 1.2190356347560882,
    "val_loss": 1.5247501564025878,
    "val_mean_dice": 0.4644301176071167,
    "val_foreground_dice": 0.0967411597294914,
    "val_dice_per_class": [
      0.8766200077533722,
      0.0967411597294914
    ],
    "val_dice_background": 0.8766200077533722,
    "val_dice_tumor": 0.0967411597294914,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 10,
    "train_loss": 1.1779632704257965,
    "val_loss": 1.4630306625366212,
    "val_mean_dice": 0.46888339042663574,
    "val_foreground_dice": 0.08638708391420853,
    "val_dice_per_class": [
      0.891117752790451,
      0.08638708391420853
    ],
    "val_dice_background": 0.891117752790451,
    "val_dice_tumor": 0.08638708391420853,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 11,
    "train_loss": 1.142837985277176,
    "val_loss": 1.3615574550628662,
    "val_mean_dice": 0.5094544798135757,
    "val_foreground_dice": 0.1730924388374983,
    "val_dice_per_class": [
      0.9115916478633881,
      0.1730924388374983
    ],
    "val_dice_background": 0.9115916478633881,
    "val_dice_tumor": 0.1730924388374983,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 12,
    "train_loss": 1.1051985955238341,
    "val_loss": 1.361706383228302,
    "val_mean_dice": 0.5063101214170456,
    "val_foreground_dice": 0.17118202200290397,
    "val_dice_per_class": [
      0.9099110269546509,
      0.17118202200290397
    ],
    "val_dice_background": 0.9099110269546509,
    "val_dice_tumor": 0.17118202200290397,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 13,
    "train_loss": 1.07803541970253,
    "val_loss": 1.3782066988945008,
    "val_mean_dice": 0.49208119332790373,
    "val_foreground_dice": 0.14940423866695104,
    "val_dice_per_class": [
      0.9124483513832092,
      0.14940423866695104
    ],
    "val_dice_background": 0.9124483513832092,
    "val_dice_tumor": 0.14940423866695104,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 14,
    "train_loss": 1.0438917350769044,
    "val_loss": 1.330910118818283,
    "val_mean_dice": 0.5068700182437896,
    "val_foreground_dice": 0.18326893295670743,
    "val_dice_per_class": [
      0.9257709491252899,
      0.18326893295670743
    ],
    "val_dice_background": 0.9257709491252899,
    "val_dice_tumor": 0.18326893295670743,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 15,
    "train_loss": 1.0884918804168702,
    "val_loss": 1.2584649181365968,
    "val_mean_dice": 0.5238544768095017,
    "val_foreground_dice": 0.16854306785346665,
    "val_dice_per_class": [
      0.956695692539215,
      0.16854306785346665
    ],
    "val_dice_background": 0.956695692539215,
    "val_dice_tumor": 0.16854306785346665,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 16,
    "train_loss": 1.0532509520053863,
    "val_loss": 1.2260815334320068,
    "val_mean_dice": 0.5348580300807952,
    "val_foreground_dice": 0.22605915148786151,
    "val_dice_per_class": [
      0.9566864860057831,
      0.22605915148786151
    ],
    "val_dice_background": 0.9566864860057831,
    "val_dice_tumor": 0.22605915148786151,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 17,
    "train_loss": 0.9956852023601532,
    "val_loss": 1.3266047322750092,
    "val_mean_dice": 0.534173054099083,
    "val_foreground_dice": 0.19994129647966474,
    "val_dice_per_class": [
      0.9403836798667907,
      0.19994129647966474
    ],
    "val_dice_background": 0.9403836798667907,
    "val_dice_tumor": 0.19994129647966474,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 18,
    "train_loss": 0.9861232256889343,
    "val_loss": 1.3136370587348938,
    "val_mean_dice": 0.5208478999137879,
    "val_foreground_dice": 0.16726681167992039,
    "val_dice_per_class": [
      0.9312997055053711,
      0.16726681167992039
    ],
    "val_dice_background": 0.9312997055053711,
    "val_dice_tumor": 0.16726681167992039,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 19,
    "train_loss": 0.9287551293373107,
    "val_loss": 1.223967468738556,
    "val_mean_dice": 0.5511100041866303,
    "val_foreground_dice": 0.2815742690649391,
    "val_dice_per_class": [
      0.944538414478302,
      0.2815742690649391
    ],
    "val_dice_background": 0.944538414478302,
    "val_dice_tumor": 0.2815742690649391,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 20,
    "train_loss": 0.9652687962055206,
    "val_loss": 1.2607442045211792,
    "val_mean_dice": 0.5253342199325561,
    "val_foreground_dice": 0.1900008580129137,
    "val_dice_per_class": [
      0.9518679964542389,
      0.1900008580129137
    ],
    "val_dice_background": 0.9518679964542389,
    "val_dice_tumor": 0.1900008580129137,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  }
]

--- outputs/baseline_v1/20260111_154537/train_config_resolved.yaml ---
data:
  root: ./data/raw/msd_task01/Task01_BrainTumour
  train_ratio: 0.7
  val_ratio: 0.2
  seed: 42
  list_files:
    train: null
    val: null
    test: null
  label_mode: binary
  class_names:
  - background
  - tumor
  roi_size:
  - 96
  - 96
  - 96
  pos_ratio: 0.9
  max_pos_attempts: 10
  min_pos_voxel_frac: 0.01
  percentiles:
  - 0.5
  - 99.5
model:
  in_channels: 4
  channels:
  - 16
  - 32
  - 64
  - 128
  - 128
  strides:
  - 2
  - 2
  - 2
  - 2
  num_res_units: 1
  norm: instance
training:
  batch_size: 1
  learning_rate: 0.0001
  max_epochs: 20
  num_workers: 8
  seed: 42
  deterministic: false
  output_dir: ./outputs/runs
  limit_train_batches: 250
  limit_val_batches: 50
  log_interval: 20
  vis_interval: 10
  max_vis_cases: 3
  debug_shapes: false
  ignore_empty_foreground: true
  log_sanity_steps: 50
  prediction_threshold: 0.5
  pos_weight: 5.0
inference:
  roi_size:
  - 96
  - 96
  - 96
  overlap: 0.5
  sw_batch_size: 2
  ignore_empty_foreground: true

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_gt.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_input.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_overlay.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_0_pred.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_gt.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_input.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_overlay.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_1_pred.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_gt.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_input.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_overlay.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_10/case_2_pred.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_gt.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_input.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_overlay.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_0_pred.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_gt.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_input.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_overlay.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_1_pred.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_gt.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_input.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_overlay.png ---
(binary file, contents omitted)

--- outputs/baseline_v1/20260111_154537/vis/epoch_20/case_2_pred.png ---
(binary file, contents omitted)

--- outputs/metrics.csv ---
epoch,train_loss,val_loss,val_mean_dice,val_foreground_dice,val_dice_background,val_dice_tumor,lr
1,1.804618,1.809580,0.337177,0.069214,0.635595,0.069214,0.00010000
2,1.647916,1.748115,0.419185,0.099598,0.794547,0.099598,0.00010000
3,1.533839,1.691924,0.441040,0.094757,0.823331,0.094757,0.00010000
4,1.439287,1.640130,0.447292,0.079321,0.851750,0.079321,0.00010000
5,1.423300,1.574749,0.443360,0.087528,0.844706,0.087528,0.00010000
6,1.352130,1.554864,0.462256,0.100227,0.870391,0.100227,0.00010000
7,1.311753,1.505091,0.460095,0.132966,0.872322,0.132966,0.00010000
8,1.285457,1.499902,0.480972,0.128612,0.879632,0.128612,0.00010000
9,1.219036,1.524750,0.464430,0.096741,0.876620,0.096741,0.00010000
10,1.177963,1.463031,0.468883,0.086387,0.891118,0.086387,0.00010000
11,1.142838,1.361557,0.509454,0.173092,0.911592,0.173092,0.00010000
12,1.105199,1.361706,0.506310,0.171182,0.909911,0.171182,0.00010000
13,1.078035,1.378207,0.492081,0.149404,0.912448,0.149404,0.00010000
14,1.043892,1.330910,0.506870,0.183269,0.925771,0.183269,0.00010000
15,1.088492,1.258465,0.523854,0.168543,0.956696,0.168543,0.00010000
16,1.053251,1.226082,0.534858,0.226059,0.956686,0.226059,0.00010000
17,0.995685,1.326605,0.534173,0.199941,0.940384,0.199941,0.00010000
18,0.986123,1.313637,0.520848,0.167267,0.931300,0.167267,0.00010000
19,0.928755,1.223967,0.551110,0.281574,0.944538,0.281574,0.00010000
20,0.965269,1.260744,0.525334,0.190001,0.951868,0.190001,0.00010000

--- outputs/metrics_3d.json ---
{
  "dataset_format": "msd_task01",
  "label_mode": "binary",
  "val": {
    "dice_per_class": {
      "background": 0.98722887346425,
      "tumor": 0.016988756025741694
    },
    "mean_dice": 0.5021088147449959,
    "foreground_mean_dice": 0.016988756025741694,
    "dice_background": 0.98722887346425,
    "dice_tumor": 0.016988756025741694,
    "number_of_volumes": 97
  },
  "test": {
    "dice_per_class": {
      "background": 0.9879584555723229,
      "tumor": 0.023572556937305846
    },
    "mean_dice": 0.5057655062548143,
    "foreground_mean_dice": 0.023572556937305846,
    "dice_background": 0.9879584555723229,
    "dice_tumor": 0.023572556937305846,
    "number_of_volumes": 49
  },
  "inference": {
    "roi_size": [
      96,
      96,
      96
    ],
    "overlap": 0.5,
    "sw_batch_size": 2,
    "include_background": true,
    "ignore_empty_foreground": true,
    "pred_rule": "sigmoid>0.5"
  },
  "git_commit": "94bbd68"
}

--- outputs/runs/.DS_Store ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/env.txt ---
3.12.12
torch==2.7.1+cu118
monai==1.5.1

--- outputs/runs/20260111_145626/events.out.tfevents.1768161388.gn-0007.542966.0 ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/git_commit.txt ---
7addf7f

--- outputs/runs/20260111_145626/gpu.txt ---
GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-2bf47ea5-f6c0-205c-03c6-eff3d127cbf0)

--- outputs/runs/20260111_145626/metrics.csv ---
epoch,train_loss,val_loss,val_mean_dice,val_foreground_dice,val_dice_background,val_dice_tumor,lr
1,1.804618,1.809580,0.337177,0.069214,0.635595,0.069214,0.00010000
2,1.647916,1.748115,0.419185,0.099598,0.794547,0.099598,0.00010000
3,1.533839,1.691924,0.441040,0.094757,0.823331,0.094757,0.00010000
4,1.439287,1.640130,0.447292,0.079321,0.851750,0.079321,0.00010000
5,1.423300,1.574749,0.443360,0.087528,0.844706,0.087528,0.00010000
6,1.352130,1.554864,0.462256,0.100227,0.870391,0.100227,0.00010000
7,1.311753,1.505091,0.460095,0.132966,0.872322,0.132966,0.00010000
8,1.285457,1.499902,0.480972,0.128612,0.879632,0.128612,0.00010000
9,1.219036,1.524750,0.464430,0.096741,0.876620,0.096741,0.00010000
10,1.177963,1.463031,0.468883,0.086387,0.891118,0.086387,0.00010000
11,1.142838,1.361557,0.509454,0.173092,0.911592,0.173092,0.00010000
12,1.105199,1.361706,0.506310,0.171182,0.909911,0.171182,0.00010000
13,1.078035,1.378207,0.492081,0.149404,0.912448,0.149404,0.00010000
14,1.043892,1.330910,0.506870,0.183269,0.925771,0.183269,0.00010000
15,1.088492,1.258465,0.523854,0.168543,0.956696,0.168543,0.00010000
16,1.053251,1.226082,0.534858,0.226059,0.956686,0.226059,0.00010000
17,0.995685,1.326605,0.534173,0.199941,0.940384,0.199941,0.00010000
18,0.986123,1.313637,0.520848,0.167267,0.931300,0.167267,0.00010000
19,0.928755,1.223967,0.551110,0.281574,0.944538,0.281574,0.00010000
20,0.965269,1.260744,0.525334,0.190001,0.951868,0.190001,0.00010000

--- outputs/runs/20260111_145626/metrics_per_epoch.json ---
[
  {
    "epoch": 1,
    "train_loss": 1.804617980480194,
    "val_loss": 1.809579575061798,
    "val_mean_dice": 0.33717746257781983,
    "val_foreground_dice": 0.06921386545582209,
    "val_dice_per_class": [
      0.635595155954361,
      0.06921386545582209
    ],
    "val_dice_background": 0.635595155954361,
    "val_dice_tumor": 0.06921386545582209,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 2,
    "train_loss": 1.6479155039787292,
    "val_loss": 1.7481150078773497,
    "val_mean_dice": 0.4191851818561554,
    "val_foreground_dice": 0.09959770811044356,
    "val_dice_per_class": [
      0.7945473730564118,
      0.09959770811044356
    ],
    "val_dice_background": 0.7945473730564118,
    "val_dice_tumor": 0.09959770811044356,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 3,
    "train_loss": 1.5338392219543457,
    "val_loss": 1.691923563480377,
    "val_mean_dice": 0.4410399258136749,
    "val_foreground_dice": 0.09475686833919157,
    "val_dice_per_class": [
      0.823330591917038,
      0.09475686833919157
    ],
    "val_dice_background": 0.823330591917038,
    "val_dice_tumor": 0.09475686833919157,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 4,
    "train_loss": 1.439286849975586,
    "val_loss": 1.6401302647590636,
    "val_mean_dice": 0.44729161739349366,
    "val_foreground_dice": 0.07932097819459025,
    "val_dice_per_class": [
      0.8517499053478241,
      0.07932097819459025
    ],
    "val_dice_background": 0.8517499053478241,
    "val_dice_tumor": 0.07932097819459025,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 5,
    "train_loss": 1.4233003630638124,
    "val_loss": 1.574749126434326,
    "val_mean_dice": 0.44335984528064726,
    "val_foreground_dice": 0.08752848583587063,
    "val_dice_per_class": [
      0.8447060132026672,
      0.08752848583587063
    ],
    "val_dice_background": 0.8447060132026672,
    "val_dice_tumor": 0.08752848583587063,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 6,
    "train_loss": 1.35212952709198,
    "val_loss": 1.5548641729354857,
    "val_mean_dice": 0.46225647151470184,
    "val_foreground_dice": 0.10022668323155678,
    "val_dice_per_class": [
      0.8703905320167542,
      0.10022668323155678
    ],
    "val_dice_background": 0.8703905320167542,
    "val_dice_tumor": 0.10022668323155678,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 7,
    "train_loss": 1.3117529258728027,
    "val_loss": 1.5050909090042115,
    "val_mean_dice": 0.46009465992450715,
    "val_foreground_dice": 0.1329657798907849,
    "val_dice_per_class": [
      0.8723216390609742,
      0.1329657798907849
    ],
    "val_dice_background": 0.8723216390609742,
    "val_dice_tumor": 0.1329657798907849,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 8,
    "train_loss": 1.2854572594165803,
    "val_loss": 1.4999022197723388,
    "val_mean_dice": 0.48097169160842895,
    "val_foreground_dice": 0.12861225873894,
    "val_dice_per_class": [
      0.8796315395832062,
      0.12861225873894
    ],
    "val_dice_background": 0.8796315395832062,
    "val_dice_tumor": 0.12861225873894,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 9,
    "train_loss": 1.2190356347560882,
    "val_loss": 1.5247501564025878,
    "val_mean_dice": 0.4644301176071167,
    "val_foreground_dice": 0.0967411597294914,
    "val_dice_per_class": [
      0.8766200077533722,
      0.0967411597294914
    ],
    "val_dice_background": 0.8766200077533722,
    "val_dice_tumor": 0.0967411597294914,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 10,
    "train_loss": 1.1779632704257965,
    "val_loss": 1.4630306625366212,
    "val_mean_dice": 0.46888339042663574,
    "val_foreground_dice": 0.08638708391420853,
    "val_dice_per_class": [
      0.891117752790451,
      0.08638708391420853
    ],
    "val_dice_background": 0.891117752790451,
    "val_dice_tumor": 0.08638708391420853,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 11,
    "train_loss": 1.142837985277176,
    "val_loss": 1.3615574550628662,
    "val_mean_dice": 0.5094544798135757,
    "val_foreground_dice": 0.1730924388374983,
    "val_dice_per_class": [
      0.9115916478633881,
      0.1730924388374983
    ],
    "val_dice_background": 0.9115916478633881,
    "val_dice_tumor": 0.1730924388374983,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 12,
    "train_loss": 1.1051985955238341,
    "val_loss": 1.361706383228302,
    "val_mean_dice": 0.5063101214170456,
    "val_foreground_dice": 0.17118202200290397,
    "val_dice_per_class": [
      0.9099110269546509,
      0.17118202200290397
    ],
    "val_dice_background": 0.9099110269546509,
    "val_dice_tumor": 0.17118202200290397,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 13,
    "train_loss": 1.07803541970253,
    "val_loss": 1.3782066988945008,
    "val_mean_dice": 0.49208119332790373,
    "val_foreground_dice": 0.14940423866695104,
    "val_dice_per_class": [
      0.9124483513832092,
      0.14940423866695104
    ],
    "val_dice_background": 0.9124483513832092,
    "val_dice_tumor": 0.14940423866695104,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 14,
    "train_loss": 1.0438917350769044,
    "val_loss": 1.330910118818283,
    "val_mean_dice": 0.5068700182437896,
    "val_foreground_dice": 0.18326893295670743,
    "val_dice_per_class": [
      0.9257709491252899,
      0.18326893295670743
    ],
    "val_dice_background": 0.9257709491252899,
    "val_dice_tumor": 0.18326893295670743,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 15,
    "train_loss": 1.0884918804168702,
    "val_loss": 1.2584649181365968,
    "val_mean_dice": 0.5238544768095017,
    "val_foreground_dice": 0.16854306785346665,
    "val_dice_per_class": [
      0.956695692539215,
      0.16854306785346665
    ],
    "val_dice_background": 0.956695692539215,
    "val_dice_tumor": 0.16854306785346665,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 16,
    "train_loss": 1.0532509520053863,
    "val_loss": 1.2260815334320068,
    "val_mean_dice": 0.5348580300807952,
    "val_foreground_dice": 0.22605915148786151,
    "val_dice_per_class": [
      0.9566864860057831,
      0.22605915148786151
    ],
    "val_dice_background": 0.9566864860057831,
    "val_dice_tumor": 0.22605915148786151,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 17,
    "train_loss": 0.9956852023601532,
    "val_loss": 1.3266047322750092,
    "val_mean_dice": 0.534173054099083,
    "val_foreground_dice": 0.19994129647966474,
    "val_dice_per_class": [
      0.9403836798667907,
      0.19994129647966474
    ],
    "val_dice_background": 0.9403836798667907,
    "val_dice_tumor": 0.19994129647966474,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 18,
    "train_loss": 0.9861232256889343,
    "val_loss": 1.3136370587348938,
    "val_mean_dice": 0.5208478999137879,
    "val_foreground_dice": 0.16726681167992039,
    "val_dice_per_class": [
      0.9312997055053711,
      0.16726681167992039
    ],
    "val_dice_background": 0.9312997055053711,
    "val_dice_tumor": 0.16726681167992039,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 19,
    "train_loss": 0.9287551293373107,
    "val_loss": 1.223967468738556,
    "val_mean_dice": 0.5511100041866303,
    "val_foreground_dice": 0.2815742690649391,
    "val_dice_per_class": [
      0.944538414478302,
      0.2815742690649391
    ],
    "val_dice_background": 0.944538414478302,
    "val_dice_tumor": 0.2815742690649391,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  },
  {
    "epoch": 20,
    "train_loss": 0.9652687962055206,
    "val_loss": 1.2607442045211792,
    "val_mean_dice": 0.5253342199325561,
    "val_foreground_dice": 0.1900008580129137,
    "val_dice_per_class": [
      0.9518679964542389,
      0.1900008580129137
    ],
    "val_dice_background": 0.9518679964542389,
    "val_dice_tumor": 0.1900008580129137,
    "lr": 0.0001,
    "ignore_empty_foreground": true,
    "include_background": true,
    "pred_rule": "argmax over softmax logits",
    "class_names": [
      "background",
      "tumor"
    ]
  }
]

--- outputs/runs/20260111_145626/train_config_resolved.yaml ---
data:
  root: ./data/raw/msd_task01/Task01_BrainTumour
  train_ratio: 0.7
  val_ratio: 0.2
  seed: 42
  list_files:
    train: null
    val: null
    test: null
  label_mode: binary
  class_names:
  - background
  - tumor
  roi_size:
  - 96
  - 96
  - 96
  pos_ratio: 0.9
  max_pos_attempts: 10
  min_pos_voxel_frac: 0.01
  percentiles:
  - 0.5
  - 99.5
model:
  in_channels: 4
  channels:
  - 16
  - 32
  - 64
  - 128
  - 128
  strides:
  - 2
  - 2
  - 2
  - 2
  num_res_units: 1
  norm: instance
training:
  batch_size: 1
  learning_rate: 0.0001
  max_epochs: 20
  num_workers: 8
  seed: 42
  deterministic: false
  output_dir: ./outputs/runs
  limit_train_batches: 250
  limit_val_batches: 50
  log_interval: 20
  vis_interval: 10
  max_vis_cases: 3
  debug_shapes: false
  ignore_empty_foreground: true
  log_sanity_steps: 50
  prediction_threshold: 0.5
  pos_weight: 5.0
inference:
  roi_size:
  - 96
  - 96
  - 96
  overlap: 0.5
  sw_batch_size: 2
  ignore_empty_foreground: true

--- outputs/runs/20260111_145626/vis/epoch_10/case_0_gt.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_0_input.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_0_overlay.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_0_pred.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_1_gt.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_1_input.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_1_overlay.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_1_pred.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_2_gt.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_2_input.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_2_overlay.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_10/case_2_pred.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_0_gt.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_0_input.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_0_overlay.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_0_pred.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_1_gt.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_1_input.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_1_overlay.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_1_pred.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_2_gt.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_2_input.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_2_overlay.png ---
(binary file, contents omitted)

--- outputs/runs/20260111_145626/vis/epoch_20/case_2_pred.png ---
(binary file, contents omitted)

--- requirements-3d.txt ---
torch
monai
pyyaml
tensorboard

--- requirements.txt ---
albumentations
imageio
ipython
matplotlib
nibabel
numpy
opencv-python-headless
pillow
SimpleITK
tensorflow
fastapi
uvicorn
python-multipart
pytest

--- scripts/debug_alignment.py ---
import argparse
from pathlib import Path

import numpy as np
from PIL import Image

from src.data.msd_task01_3d import MSDTask01Dataset3D, list_msd_task01_cases


def parse_args():
    parser = argparse.ArgumentParser(description="Debug ROI alignment on a single patch")
    parser.add_argument(
        "--dataset-root",
        default="./data/raw/msd_task01/Task01_BrainTumour",
        help="MSD Task01 root with imagesTr/labelsTr",
    )
    parser.add_argument("--roi-size", default="96,96,96", help="ROI size as D,H,W")
    parser.add_argument("--case-index", type=int, default=0, help="Case index from imagesTr")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--pos-ratio", type=float, default=1.0, help="Probability of tumor-centered sampling")
    parser.add_argument("--max-pos-attempts", type=int, default=10, help="Max resamples for tumor patches")
    parser.add_argument("--out-dir", default="outputs/debug_alignment", help="Output folder")
    parser.add_argument("--channel", type=int, default=0, help="Which modality to visualize")
    return parser.parse_args()


def _normalize_slice(slice_array: np.ndarray) -> np.ndarray:
    vmin = float(slice_array.min())
    vmax = float(slice_array.max())
    if vmax <= vmin:
        return np.zeros_like(slice_array, dtype=np.uint8)
    scaled = (slice_array - vmin) / (vmax - vmin)
    return (scaled * 255.0).astype(np.uint8)


def main():
    args = parse_args()
    roi_size = tuple(int(x) for x in args.roi_size.split(","))
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    cases = list_msd_task01_cases(Path(args.dataset_root))
    case = cases[args.case_index]
    ds = MSDTask01Dataset3D(
        [case],
        roi_size=roi_size,
        label_mode="binary",
        num_classes=2,
        pos_ratio=args.pos_ratio,
        percentiles=(0.5, 99.5),
        mode="train",
        seed=args.seed,
        max_pos_attempts=args.max_pos_attempts,
    )

    img, lbl = ds[0]
    print("image shape", tuple(img.shape), "label shape", tuple(lbl.shape))
    print("label unique", np.unique(lbl.numpy()))
    print("label sum", float(lbl.sum().item()))

    gt = lbl[1].numpy()
    areas = gt.sum(axis=(1, 2))
    z = int(np.argmax(areas))
    img_slice = img[args.channel, z].numpy()
    gt_slice = gt[z]

    img_slice = _normalize_slice(img_slice)
    gt_slice = (gt_slice > 0).astype(np.uint8) * 255

    overlay = np.stack([img_slice] * 3, axis=-1).astype(np.uint8)
    overlay[gt_slice > 0, 1] = 255

    Image.fromarray(img_slice).save(out_dir / "input.png")
    Image.fromarray(gt_slice).save(out_dir / "gt.png")
    Image.fromarray(overlay).save(out_dir / "overlay.png")
    print(f"wrote {out_dir}")


if __name__ == "__main__":
    main()

--- scripts/debug_label_permutation.py ---
import argparse
from itertools import permutations
from pathlib import Path

import nibabel as nib
import numpy as np
from PIL import Image

from src.data.msd_task01_3d import _to_channel_first, list_msd_task01_cases, normalize_modalities


def parse_args():
    parser = argparse.ArgumentParser(description="Find label axis permutation that best aligns with images")
    parser.add_argument(
        "--dataset-root",
        default="./data/raw/msd_task01/Task01_BrainTumour",
        help="MSD Task01 root with imagesTr/labelsTr",
    )
    parser.add_argument("--case-index", type=int, default=0, help="Case index from imagesTr")
    parser.add_argument("--channel", type=int, default=0, help="Which modality to visualize")
    parser.add_argument("--out-dir", default="outputs/debug_alignment_perm", help="Output folder")
    return parser.parse_args()


def _normalize_slice(slice_array: np.ndarray) -> np.ndarray:
    vmin = float(slice_array.min())
    vmax = float(slice_array.max())
    if vmax <= vmin:
        return np.zeros_like(slice_array, dtype=np.uint8)
    scaled = (slice_array - vmin) / (vmax - vmin)
    return (scaled * 255.0).astype(np.uint8)


def main():
    args = parse_args()
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    cases = list_msd_task01_cases(Path(args.dataset_root))
    image_path, label_path = cases[args.case_index]
    image = nib.load(str(image_path)).get_fdata().astype(np.float32)
    label = nib.load(str(label_path)).get_fdata().astype(np.int64)

    image = _to_channel_first(image)
    image = normalize_modalities(image, (0.5, 99.5))
    brain = image[args.channel]
    mask = brain != 0
    if mask.any():
        thresh = np.percentile(brain[mask], 50)
        brain_mask = brain > thresh
    else:
        brain_mask = brain > 0

    perms = list(permutations((0, 1, 2)))
    scores = []
    for perm in perms:
        lbl_perm = np.transpose(label, perm)
        if lbl_perm.shape != brain.shape:
            scores.append((perm, -1))
            continue
        overlap = (lbl_perm > 0) & brain_mask
        scores.append((perm, int(overlap.sum())))

    scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)
    print("perm overlap scores:", scores_sorted)
    best_perm, best_score = scores_sorted[0]
    if best_score <= 0:
        print("No positive overlap found.")
        return

    lbl_best = np.transpose(label, best_perm)
    areas = lbl_best.sum(axis=(1, 2))
    z = int(np.argmax(areas))
    img_slice = brain[z]
    gt_slice = lbl_best[z]

    img_slice = _normalize_slice(img_slice)
    gt_slice = (gt_slice > 0).astype(np.uint8) * 255

    overlay = np.stack([img_slice] * 3, axis=-1).astype(np.uint8)
    overlay[gt_slice > 0, 1] = 255

    Image.fromarray(img_slice).save(out_dir / "input.png")
    Image.fromarray(gt_slice).save(out_dir / "gt.png")
    Image.fromarray(overlay).save(out_dir / "overlay.png")
    print(f"best_perm={best_perm} wrote {out_dir}")


if __name__ == "__main__":
    main()

--- scripts/download_msd_task01.py ---
import argparse
import sys
import tarfile
import urllib.request
from pathlib import Path


DEFAULT_URL = "https://msd-for-monai.s3-us-west-2.amazonaws.com/Task01_BrainTumour.tar"


def parse_args():
    parser = argparse.ArgumentParser(description="Download MSD Task01_BrainTumour dataset")
    parser.add_argument("--url", default=DEFAULT_URL, help="Dataset tar URL")
    parser.add_argument(
        "--output-dir",
        default="data/raw/msd_task01",
        help="Directory to extract Task01_BrainTumour into",
    )
    parser.add_argument("--force", action="store_true", help="Re-download and extract even if present")
    return parser.parse_args()


def _download_progress(blocks: int, block_size: int, total_size: int):
    if total_size <= 0:
        return
    downloaded = min(blocks * block_size, total_size)
    percent = downloaded / total_size * 100
    sys.stdout.write(f"\rDownloading: {percent:5.1f}%")
    sys.stdout.flush()


def download_file(url: str, dest: Path):
    dest.parent.mkdir(parents=True, exist_ok=True)
    if dest.exists():
        return
    print(f"Downloading {url} to {dest}")
    urllib.request.urlretrieve(url, dest, reporthook=_download_progress)
    sys.stdout.write("\n")


def extract_archive(archive_path: Path, output_dir: Path):
    print(f"Extracting {archive_path} to {output_dir}")
    with tarfile.open(archive_path, "r:*") as tar:
        members = tar.getmembers()
        total = len(members)
        for idx, member in enumerate(members, start=1):
            tar.extract(member, path=output_dir)
            percent = idx / total * 100
            sys.stdout.write(f"\rExtracting: {percent:5.1f}%")
            sys.stdout.flush()
    sys.stdout.write("\n")


def main():
    args = parse_args()
    output_dir = Path(args.output_dir).expanduser().resolve()
    archive_path = output_dir / "Task01_BrainTumour.tar"
    extracted_root = output_dir / "Task01_BrainTumour"

    if extracted_root.exists() and not args.force:
        print(f"Dataset already present at {extracted_root}")
        return

    download_file(args.url, archive_path)
    extract_archive(archive_path, output_dir)
    print(f"Ready: {extracted_root}")


if __name__ == "__main__":
    main()

--- scripts/make_readme_figures.py ---
import argparse
import csv
from pathlib import Path
from typing import List, Tuple

import numpy as np
from PIL import Image


def parse_args():
    parser = argparse.ArgumentParser(description="Export baseline figures for README")
    parser.add_argument("--run-dir", required=True, help="Path to outputs/runs/<run_id>")
    parser.add_argument("--max-cases", type=int, default=3, help="Max cases for grid")
    parser.add_argument("--out-dir", default="docs/assets", help="Output directory for figures")
    return parser.parse_args()


def _load_image(path: Path) -> Image.Image:
    return Image.open(path).convert("RGB")


def _overlay(mask: Image.Image, base: Image.Image, color: Tuple[int, int, int]) -> Image.Image:
    base_arr = np.array(base).astype(np.uint8)
    mask_arr = np.array(mask.convert("L"))
    overlay = base_arr.copy()
    overlay[mask_arr > 0] = color
    return Image.fromarray(overlay)


def _select_latest_vis_dir(run_dir: Path) -> Path:
    vis_root = run_dir / "vis"
    if not vis_root.exists():
        raise FileNotFoundError(f"No vis folder found in {run_dir}")
    epochs = sorted(vis_root.glob("epoch_*"))
    if not epochs:
        raise FileNotFoundError(f"No epoch folders found in {vis_root}")
    return epochs[-1]


def _collect_cases(vis_dir: Path, max_cases: int) -> List[int]:
    cases = []
    for path in vis_dir.glob("case_*_input.png"):
        name = path.stem
        case_id = int(name.split("_")[1])
        cases.append(case_id)
    return sorted(cases)[:max_cases]


def _make_grid(run_dir: Path, out_dir: Path, max_cases: int):
    vis_dir = _select_latest_vis_dir(run_dir)
    case_ids = _collect_cases(vis_dir, max_cases)
    if not case_ids:
        raise FileNotFoundError(f"No case images found in {vis_dir}")

    rows = []
    for case_id in case_ids:
        input_img = _load_image(vis_dir / f"case_{case_id}_input.png")
        gt_mask = Image.open(vis_dir / f"case_{case_id}_gt.png")
        pred_mask = Image.open(vis_dir / f"case_{case_id}_pred.png")
        gt_overlay = _overlay(gt_mask, input_img, (0, 255, 0))
        pred_overlay = _overlay(pred_mask, input_img, (255, 0, 0))
        row = Image.new("RGB", (input_img.width * 3, input_img.height))
        row.paste(input_img, (0, 0))
        row.paste(gt_overlay, (input_img.width, 0))
        row.paste(pred_overlay, (input_img.width * 2, 0))
        rows.append(row)

    grid = Image.new("RGB", (rows[0].width, rows[0].height * len(rows)))
    for idx, row in enumerate(rows):
        grid.paste(row, (0, idx * row.height))

    grid.save(out_dir / "baseline_examples.png")


def _make_curves(run_dir: Path, out_dir: Path):
    import matplotlib

    matplotlib.use("Agg")
    import matplotlib.pyplot as plt

    metrics_path = run_dir / "metrics.csv"
    if not metrics_path.exists():
        raise FileNotFoundError(f"metrics.csv not found in {run_dir}")

    epochs = []
    train_loss = []
    val_loss = []
    val_dice = []
    with metrics_path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            epochs.append(int(row["epoch"]))
            train_loss.append(float(row.get("train_loss", 0)))
            val_loss.append(float(row.get("val_loss", 0)))
            val_dice.append(float(row.get("val_dice_tumor", row.get("val_foreground_dice", 0))))

    fig, ax1 = plt.subplots(figsize=(8, 4))
    ax1.plot(epochs, train_loss, label="train_loss")
    ax1.plot(epochs, val_loss, label="val_loss")
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")

    ax2 = ax1.twinx()
    ax2.plot(epochs, val_dice, color="green", label="val_dice_tumor")
    ax2.set_ylabel("Dice")
    ax2.legend(loc="lower right")

    fig.tight_layout()
    fig.savefig(out_dir / "baseline_curves.png", dpi=150)
    plt.close(fig)


def main():
    args = parse_args()
    run_dir = Path(args.run_dir).expanduser().resolve()
    out_dir = Path(args.out_dir).expanduser().resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    _make_grid(run_dir, out_dir, args.max_cases)
    _make_curves(run_dir, out_dir)
    print(f"Wrote figures to {out_dir}")


if __name__ == "__main__":
    main()

--- scripts/run_baseline_3d.sh ---
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
PYTHON_BIN="${PYTHON_BIN:-python}"
if ! command -v "${PYTHON_BIN}" >/dev/null 2>&1; then
  if command -v python3 >/dev/null 2>&1; then
    PYTHON_BIN="python3"
  else
    echo "ERROR: python executable not found." >&2
    exit 1
  fi
fi
CONFIG_PATH="${ROOT_DIR}/configs/config_3d_baseline.yaml"
DATA_ROOT="${ROOT_DIR}/data/raw/msd_task01/Task01_BrainTumour"
RUNS_DIR="${ROOT_DIR}/outputs/runs"
BASELINE_DIR="${ROOT_DIR}/outputs/baseline_v1/$(date +%Y%m%d_%H%M%S)"

if ! command -v nvidia-smi >/dev/null 2>&1; then
  echo "ERROR: nvidia-smi not found. A CUDA-capable GPU is required." >&2
  exit 1
fi

GPU_NAME="$(nvidia-smi --query-gpu=name --format=csv,noheader | head -n1 || true)"
if [[ -z "${GPU_NAME}" ]]; then
  echo "ERROR: Unable to detect GPU name via nvidia-smi." >&2
  exit 1
fi
echo "GPU detected: ${GPU_NAME}"

if [[ ! -d "${DATA_ROOT}/imagesTr" || ! -d "${DATA_ROOT}/labelsTr" ]]; then
  echo "Dataset not found at ${DATA_ROOT}. Downloading MSD Task01..."
  "${PYTHON_BIN}" "${ROOT_DIR}/scripts/download_msd_task01.py"
fi

echo "Running training with ${CONFIG_PATH}"
"${PYTHON_BIN}" -m src.train_3d --config "${CONFIG_PATH}"

LATEST_RUN="$(ls -td "${RUNS_DIR}"/* | head -n1)"
if [[ -z "${LATEST_RUN}" || ! -d "${LATEST_RUN}" ]]; then
  echo "ERROR: No run directory found in ${RUNS_DIR}." >&2
  exit 1
fi

BEST_CHECKPOINT="${LATEST_RUN}/best.pt"
if [[ ! -f "${BEST_CHECKPOINT}" ]]; then
  echo "ERROR: best.pt not found at ${BEST_CHECKPOINT}." >&2
  exit 1
fi

echo "Running evaluation with ${BEST_CHECKPOINT}"
"${PYTHON_BIN}" -m src.eval_3d --config "${CONFIG_PATH}" --weights "${BEST_CHECKPOINT}"

mkdir -p "${BASELINE_DIR}"

cp "${CONFIG_PATH}" "${BASELINE_DIR}/config_3d_baseline.yaml"
cp "${LATEST_RUN}/train_config_resolved.yaml" "${BASELINE_DIR}/train_config_resolved.yaml"
cp "${LATEST_RUN}/train.log" "${BASELINE_DIR}/train.log"
cp "${LATEST_RUN}/metrics.csv" "${BASELINE_DIR}/metrics.csv"
cp "${LATEST_RUN}/metrics_per_epoch.json" "${BASELINE_DIR}/metrics_per_epoch.json"
cp "${BEST_CHECKPOINT}" "${BASELINE_DIR}/best.pt"

if [[ -d "${LATEST_RUN}/vis" ]]; then
  cp -R "${LATEST_RUN}/vis" "${BASELINE_DIR}/vis"
fi

if [[ -f "${ROOT_DIR}/outputs/metrics_3d.json" ]]; then
  cp "${ROOT_DIR}/outputs/metrics_3d.json" "${BASELINE_DIR}/metrics_3d.json"
fi

git -C "${ROOT_DIR}" rev-parse --short HEAD > "${BASELINE_DIR}/git_commit.txt"
{
  "${PYTHON_BIN}" --version 2>&1
  "${PYTHON_BIN}" - <<'PY'
import importlib
for pkg in ("torch", "monai"):
    try:
        mod = importlib.import_module(pkg)
        print(f"{pkg}=={getattr(mod, '__version__', 'unknown')}")
    except Exception:
        print(f"{pkg} not installed")
PY
} > "${BASELINE_DIR}/env.txt"

nvidia-smi -L > "${BASELINE_DIR}/gpu.txt"

SUMMARY_JSON="${ROOT_DIR}/outputs/metrics_3d.json"
if [[ -f "${SUMMARY_JSON}" ]]; then
  python - <<PY
import json
from pathlib import Path
path = Path("${SUMMARY_JSON}")
data = json.loads(path.read_text())
val = data.get("val", {})
print("Baseline summary:")
print(f"  best checkpoint: ${BEST_CHECKPOINT}")
print(f"  val dice_tumor: {val.get('dice_tumor')}")
print(f"  val foreground_dice: {val.get('foreground_mean_dice')}")
PY
fi

echo "Baseline artifacts copied to ${BASELINE_DIR}"

--- scripts/summarize_run.py ---
import argparse
import csv
import json
import re
from pathlib import Path
from typing import Any, Dict, List

import yaml


def parse_args():
    parser = argparse.ArgumentParser(description="Summarize a 3D run into baseline metrics artifacts")
    parser.add_argument("--run-dir", required=True, help="Path to outputs/runs/<run_id>")
    parser.add_argument(
        "--output-json",
        default="outputs/baseline_metrics.json",
        help="Output JSON path",
    )
    return parser.parse_args()


def _load_yaml(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def _load_metrics_json(path: Path) -> List[Dict[str, Any]]:
    if not path.exists():
        return []
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _load_metrics_csv(path: Path) -> List[Dict[str, Any]]:
    if not path.exists():
        return []
    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return list(reader)


def _best_epoch_from_history(history: List[Dict[str, Any]]) -> Dict[str, Any]:
    if not history:
        return {}
    best = None
    for row in history:
        try:
            score = float(row.get("val_foreground_dice", 0.0))
            loss = float(row.get("val_loss", 1e9))
        except (TypeError, ValueError):
            continue
        if best is None:
            best = row
            continue
        best_score = float(best.get("val_foreground_dice", 0.0))
        best_loss = float(best.get("val_loss", 1e9))
        if score > best_score or (abs(score - best_score) < 1e-8 and loss < best_loss):
            best = row
    return best or {}


def _extract_sanity_tp_fp_fn(train_log: Path) -> Dict[str, Any]:
    if not train_log.exists():
        return {}
    pattern = re.compile(r"tp=(\d+) fp=(\d+) fn=(\d+)")
    last = None
    with train_log.open("r", encoding="utf-8") as f:
        for line in f:
            match = pattern.search(line)
            if match:
                last = match
    if not last:
        return {}
    return {
        "tp": int(last.group(1)),
        "fp": int(last.group(2)),
        "fn": int(last.group(3)),
    }


def main():
    args = parse_args()
    run_dir = Path(args.run_dir).expanduser().resolve()
    if not run_dir.exists():
        raise FileNotFoundError(f"Run directory not found: {run_dir}")

    cfg_path = run_dir / "train_config_resolved.yaml"
    if not cfg_path.exists():
        raise FileNotFoundError(f"Missing train_config_resolved.yaml in {run_dir}")
    cfg = _load_yaml(cfg_path)

    metrics_json = _load_metrics_json(run_dir / "metrics_per_epoch.json")
    metrics_csv = _load_metrics_csv(run_dir / "metrics.csv")

    if not metrics_csv and metrics_json:
        csv_path = run_dir / "metrics.csv"
        with csv_path.open("w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(
                [
                    "epoch",
                    "train_loss",
                    "val_loss",
                    "val_mean_dice",
                    "val_foreground_dice",
                    "val_dice_background",
                    "val_dice_tumor",
                    "lr",
                ]
            )
            for row in metrics_json:
                writer.writerow(
                    [
                        row.get("epoch"),
                        row.get("train_loss"),
                        row.get("val_loss"),
                        row.get("val_mean_dice"),
                        row.get("val_foreground_dice"),
                        row.get("val_dice_background"),
                        row.get("val_dice_tumor"),
                        row.get("lr"),
                    ]
                )
        metrics_csv = _load_metrics_csv(csv_path)

    best_row = _best_epoch_from_history(metrics_json or metrics_csv)
    best_epoch = int(best_row.get("epoch", 0)) if best_row else 0

    dice_per_class = best_row.get("val_dice_per_class") if best_row else None
    if isinstance(dice_per_class, list):
        dice_per_class = [float(x) for x in dice_per_class]

    train_log = run_dir / "train.log"
    sanity = _extract_sanity_tp_fp_fn(train_log)

    data_cfg = cfg.get("data", {})
    training_cfg = cfg.get("training", {})

    summary = {
        "dataset_format": "msd_task01",
        "label_mode": data_cfg.get("label_mode", "unknown"),
        "roi_size": data_cfg.get("roi_size"),
        "epochs": training_cfg.get("max_epochs"),
        "best_epoch": best_epoch,
        "dice_per_class": dice_per_class,
        "dice_background": float(best_row.get("val_dice_background", 0.0)) if best_row else 0.0,
        "dice_tumor": float(best_row.get("val_dice_tumor", 0.0)) if best_row else 0.0,
        "foreground_dice": float(best_row.get("val_foreground_dice", 0.0)) if best_row else 0.0,
        "ignore_empty_foreground": training_cfg.get("ignore_empty_foreground", True),
        "prediction_threshold": training_cfg.get("prediction_threshold", 0.5),
    }
    if sanity:
        summary["sanity_tp_fp_fn"] = sanity

    output_path = Path(args.output_json).expanduser().resolve()
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2)

    outputs_dir = output_path.parent
    metrics_csv_out = outputs_dir / "metrics.csv"
    if not metrics_csv_out.exists() and metrics_csv:
        with metrics_csv_out.open("w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=metrics_csv[0].keys())
            writer.writeheader()
            writer.writerows(metrics_csv)

    print(f"Wrote {output_path}")


if __name__ == "__main__":
    main()

--- src/.DS_Store ---
(binary file, contents omitted)

--- src/__init__.py ---


--- src/data/__init__.py ---


--- src/data/augmentations.py ---
import albumentations as A


def get_training_augmentation():
    """Return albumentations Compose for training images and masks."""
    return A.Compose([
        A.OneOf([
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.Rotate(limit=(0, 90), p=0.5),
            A.ShiftScaleRotate(shift_limit=(0, 0.1), rotate_limit=(0, 0), scale_limit=(0, 0), p=0.5),
            A.Transpose(p=0.5),
        ], p=1),
    ])

--- src/data/bias_correction.py ---
import argparse
import glob
import os
import shutil
from pathlib import Path
from typing import Iterable

import SimpleITK as sitk

MODALITIES = ("flair", "t1", "t1ce", "t2")


def correct_bias(in_path, out_path, image_type=sitk.sitkFloat64):
    input_image = sitk.ReadImage(in_path, image_type)
    output_image = sitk.N4BiasFieldCorrection(input_image, input_image > 0)
    sitk.WriteImage(output_image, out_path)
    return os.path.abspath(out_path)


def get_image_path(subject_folder, name):
    file_name = os.path.join(subject_folder, f"*{name}.nii.gz")
    matches = glob.glob(file_name)
    if not matches:
        raise FileNotFoundError(f"Could not find modality {name} in {subject_folder}")
    return matches[0]


def normalize_image(in_path, out_path, bias_correction=True):
    if bias_correction:
        correct_bias(in_path, out_path)
    else:
        shutil.copy(in_path, out_path)


def preprocess_brats_folder(
    in_folder: Path,
    out_folder: Path,
    modalities: Iterable[str],
    truth_name: str,
    no_bias_correction_modalities: Iterable[str],
):
    for name in modalities:
        image_image = get_image_path(in_folder, name)
        case_id = os.path.basename(out_folder)
        out_path = os.path.abspath(os.path.join(out_folder, f"{case_id}_{name}.nii.gz"))
        perform_bias_correction = name not in no_bias_correction_modalities
        normalize_image(image_image, out_path, bias_correction=perform_bias_correction)

    truth_image = get_image_path(in_folder, truth_name)
    out_path = os.path.abspath(os.path.join(out_folder, f"{case_id}_truth.nii.gz"))
    shutil.copy(truth_image, out_path)


def preprocess_brats_data(
    brats_folder: Path,
    out_folder: Path,
    overwrite: bool = False,
    no_bias_correction_modalities: Iterable[str] = ("flair",),
    modalities: Iterable[str] = MODALITIES,
):
    for subject_folder in glob.glob(os.path.join(brats_folder, "*", "*")):
        if os.path.isdir(subject_folder):
            subject = os.path.basename(subject_folder)
            new_subject_folder = os.path.join(out_folder, os.path.basename(os.path.dirname(subject_folder)), subject)
            if not os.path.exists(new_subject_folder) or overwrite:
                if not os.path.exists(new_subject_folder):
                    os.makedirs(new_subject_folder)
                preprocess_brats_folder(
                    Path(subject_folder),
                    Path(new_subject_folder),
                    modalities=modalities,
                    truth_name="seg",
                    no_bias_correction_modalities=no_bias_correction_modalities,
                )


def parse_args():
    parser = argparse.ArgumentParser(description="Run N4 bias-field correction on BraTS data")
    parser.add_argument("--input-dir", required=True, help="Path to raw BraTS dataset")
    parser.add_argument("--output-dir", required=True, help="Where to write corrected data")
    parser.add_argument("--skip-modalities", nargs="*", default=["flair"], help="Modalities to skip bias correction")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing output")
    return parser.parse_args()


def main():
    args = parse_args()
    input_dir = Path(args.input_dir).expanduser().resolve()
    output_dir = Path(args.output_dir).expanduser().resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    preprocess_brats_data(
        brats_folder=input_dir,
        out_folder=output_dir,
        overwrite=args.overwrite,
        no_bias_correction_modalities=args.skip_modalities,
    )


if __name__ == "__main__":
    main()

--- src/data/dataset.py ---
import math
from pathlib import Path
from typing import Iterable
import cv2
import numpy as np
from tensorflow.keras.utils import Sequence, to_categorical


class SliceDataset:
    def __init__(
        self,
        images_dir: Path,
        masks_dir: Path,
        class_names: Iterable[str],
        image_size: int = 256,
        augmentation=None,
    ):
        self.images_dir = Path(images_dir)
        self.masks_dir = Path(masks_dir)
        self.image_ids = sorted(self.images_dir.glob("*.png"))
        self.mask_ids = sorted(self.masks_dir.glob("*.png"))
        if len(self.image_ids) != len(self.mask_ids):
            raise ValueError("Number of images and masks does not match")
        self.num_classes = len(list(class_names))
        self.image_size = (image_size, image_size)
        self.augmentation = augmentation

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx: int):
        image_path = self.image_ids[idx]
        mask_path = self.mask_ids[idx]

        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
        if image is None or mask is None:
            raise ValueError(f"Failed to read image or mask for index {idx}")

        mask = np.where(mask == 4, 3, mask)
        image = cv2.resize(image, self.image_size, interpolation=cv2.INTER_NEAREST)
        mask = cv2.resize(mask, self.image_size, interpolation=cv2.INTER_NEAREST)

        if self.augmentation:
            sample = self.augmentation(image=image, mask=mask)
            image, mask = sample["image"], sample["mask"]

        image = image.astype("float32") / 255.0
        image = np.expand_dims(image, axis=-1)
        mask = to_categorical(mask, num_classes=self.num_classes).astype("float32")
        return image, mask


class DataLoader(Sequence):
    def __init__(
        self,
        dataset: SliceDataset,
        batch_size: int = 1,
        shuffle: bool = False,
    ):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexes = np.arange(len(dataset))
        self.on_epoch_end()

    def __len__(self):
        return math.ceil(len(self.indexes) / self.batch_size)

    def __getitem__(self, idx):
        batch_indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]
        batch = [self.dataset[i] for i in batch_indexes]
        images, masks = zip(*batch)
        return np.stack(images, axis=0), np.stack(masks, axis=0)

    def on_epoch_end(self):
        if self.shuffle:
            self.indexes = np.random.permutation(self.indexes)
        else:
            self.indexes = np.arange(len(self.dataset))


def build_dataloader(
    images_dir: Path,
    masks_dir: Path,
    class_names: Iterable[str],
    batch_size: int,
    image_size: int,
    augmentation=None,
    shuffle: bool = False,
) -> DataLoader:
    dataset = SliceDataset(images_dir, masks_dir, class_names, image_size, augmentation)
    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)

--- src/data/msd_task01_3d.py ---
import random
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple

import nibabel as nib
import numpy as np
import torch


def list_msd_task01_cases(dataset_root: Path) -> List[Tuple[Path, Path]]:
    images_dir = dataset_root / "imagesTr"
    labels_dir = dataset_root / "labelsTr"
    if not images_dir.exists() or not labels_dir.exists():
        raise FileNotFoundError("Expected imagesTr/ and labelsTr/ in MSD Task01 root")
    images = sorted(p for p in images_dir.glob("*.nii.gz") if not p.name.startswith("._"))
    labels = {
        p.stem.replace(".nii", ""): p
        for p in labels_dir.glob("*.nii.gz")
        if not p.name.startswith("._")
    }
    pairs = []
    for image_path in images:
        key = image_path.stem.replace(".nii", "")
        if key not in labels:
            raise FileNotFoundError(f"Missing label for {image_path.name}")
        pairs.append((image_path, labels[key]))
    return pairs


def split_cases(
    cases: Sequence[Tuple[Path, Path]],
    train_ratio: float,
    val_ratio: float,
    seed: int,
) -> Dict[str, List[Tuple[Path, Path]]]:
    rng = random.Random(seed)
    indices = list(range(len(cases)))
    rng.shuffle(indices)
    shuffled = [cases[i] for i in indices]
    train_split = int(train_ratio * len(shuffled))
    val_split = int((train_ratio + val_ratio) * len(shuffled))
    return {
        "train": shuffled[:train_split],
        "val": shuffled[train_split:val_split],
        "test": shuffled[val_split:],
    }


def _case_id(path: Path) -> str:
    return path.stem.replace(".nii", "")


def load_case_ids(list_path: Path) -> List[str]:
    with list_path.open("r", encoding="utf-8") as f:
        lines = [line.strip() for line in f.readlines()]
    return [line for line in lines if line and not line.startswith("#")]


def build_splits(
    cases: Sequence[Tuple[Path, Path]],
    train_ratio: float,
    val_ratio: float,
    seed: int,
    list_files: Optional[Dict[str, Optional[str]]] = None,
) -> Dict[str, List[Tuple[Path, Path]]]:
    if list_files and any(list_files.values()):
        required = ["train", "val", "test"]
        if not all(list_files.get(key) for key in required):
            raise ValueError("list_files must include train, val, and test lists when provided")
        case_map = {_case_id(img): (img, lbl) for img, lbl in cases}
        splits = {}
        for key in required:
            ids = load_case_ids(Path(list_files[key]))
            missing = [case_id for case_id in ids if case_id not in case_map]
            if missing:
                raise FileNotFoundError(f"Missing cases for split '{key}': {missing[:3]}")
            splits[key] = [case_map[case_id] for case_id in ids]
        return splits
    return split_cases(cases, train_ratio=train_ratio, val_ratio=val_ratio, seed=seed)


def _to_channel_first(volume: np.ndarray) -> np.ndarray:
    if volume.ndim != 4:
        raise ValueError(f"Expected 4D volume, got shape {volume.shape}")
    if volume.shape[-1] == 4:
        return np.transpose(volume, (3, 2, 0, 1))
    if volume.shape[0] == 4:
        return np.transpose(volume, (0, 3, 1, 2))
    raise ValueError(f"Unexpected channel dimension for volume with shape {volume.shape}")


def normalize_modalities(
    volume: np.ndarray,
    percentiles: Tuple[float, float],
) -> np.ndarray:
    normalized = np.zeros_like(volume, dtype=np.float32)
    for idx in range(volume.shape[0]):
        channel = volume[idx]
        mask = channel != 0
        if mask.any():
            lo, hi = np.percentile(channel[mask], percentiles)
            channel = np.clip(channel, lo, hi)
            mean = channel[mask].mean()
            std = channel[mask].std()
        else:
            mean = channel.mean()
            std = channel.std()
        std = std if std > 0 else 1.0
        normalized[idx] = (channel - mean) / std
    return normalized


def one_hot_encode(label: np.ndarray, num_classes: int) -> np.ndarray:
    label = label.astype(np.int64)
    if label.ndim != 3:
        raise ValueError(f"Expected 3D label volume, got shape {label.shape}")
    one_hot = np.eye(num_classes, dtype=np.float32)[label]
    return np.transpose(one_hot, (3, 2, 0, 1))


def _pad_to_shape(volume: np.ndarray, target_shape: Sequence[int]) -> np.ndarray:
    pad_width = []
    for dim, target in zip(volume.shape, target_shape):
        total = max(target - dim, 0)
        pad_before = total // 2
        pad_after = total - pad_before
        pad_width.append((pad_before, pad_after))
    return np.pad(volume, pad_width, mode="constant")


def crop_or_pad(volume: np.ndarray, center: Sequence[int], roi_size: Sequence[int]) -> np.ndarray:
    volume = _pad_to_shape(volume, roi_size)
    slices = []
    for dim, c, size in zip(volume.shape, center, roi_size):
        start = int(c - size // 2)
        start = max(start, 0)
        end = start + size
        if end > dim:
            end = dim
            start = end - size
        slices.append(slice(start, end))
    return volume[tuple(slices)]


def sample_center(
    label: np.ndarray,
    roi_size: Sequence[int],
    pos_ratio: float,
    rng: np.random.Generator,
) -> Tuple[int, int, int]:
    foreground = np.argwhere(label > 0)
    if foreground.size > 0 and rng.random() < pos_ratio:
        center = foreground[rng.integers(0, len(foreground))]
    else:
        center = np.array([rng.integers(0, dim) for dim in label.shape])
    center = np.maximum(center, np.array(roi_size) // 2)
    center = np.minimum(center, np.array(label.shape) - np.array(roi_size) // 2 - 1)
    return tuple(center.tolist())


class MSDTask01Dataset3D(torch.utils.data.Dataset):
    def __init__(
        self,
        cases: Sequence[Tuple[Path, Path]],
        roi_size: Optional[Sequence[int]],
        label_mode: str,
        num_classes: int,
        pos_ratio: float,
        percentiles: Tuple[float, float],
        mode: str = "train",
        seed: int = 42,
        max_pos_attempts: int = 10,
        min_pos_voxel_frac: float = 0.0,
    ):
        self.cases = list(cases)
        self.roi_size = tuple(roi_size) if roi_size is not None else None
        self.label_mode = label_mode
        self.num_classes = num_classes
        self.pos_ratio = pos_ratio
        self.percentiles = percentiles
        self.mode = mode
        self.rng = np.random.default_rng(seed)
        self.max_pos_attempts = max_pos_attempts
        self.min_pos_voxel_frac = min_pos_voxel_frac

    def __len__(self):
        return len(self.cases)

    def __getitem__(self, idx: int):
        image_path, label_path = self.cases[idx]
        image = nib.load(str(image_path)).get_fdata().astype(np.float32)
        label = nib.load(str(label_path)).get_fdata().astype(np.int64)

        image = _to_channel_first(image)
        image = normalize_modalities(image, self.percentiles)
        label = label.astype(np.int64)
        if label.ndim != 3:
            raise ValueError(f"Expected 3D label volume, got shape {label.shape}")
        if label.shape == image.shape[1:]:
            pass
        else:
            label = np.transpose(label, (2, 0, 1))
            if label.shape != image.shape[1:]:
                raise ValueError(
                    f"Label shape {label.shape} does not match image spatial shape {image.shape[1:]}"
                )

        if self.label_mode == "binary":
            label = (label > 0).astype(np.int64)
            num_classes = 2
        else:
            num_classes = self.num_classes

        if self.roi_size is not None:
            has_foreground = np.any(label > 0)
            want_pos = has_foreground and self.rng.random() < self.pos_ratio
            center = sample_center(label, self.roi_size, 1.0 if want_pos else 0.0, self.rng)
            if want_pos:
                for _ in range(self.max_pos_attempts):
                    center = sample_center(label, self.roi_size, 1.0, self.rng)
                    label_patch = crop_or_pad(label, center, self.roi_size)
                    if np.any(label_patch > 0):
                        if self.min_pos_voxel_frac > 0:
                            frac = float(np.mean(label_patch > 0))
                            if frac < self.min_pos_voxel_frac:
                                continue
                        break
            image = np.stack([crop_or_pad(ch, center, self.roi_size) for ch in image], axis=0)
            label = crop_or_pad(label, center, self.roi_size)

        label_one_hot = one_hot_encode(label, num_classes)
        image_tensor = torch.from_numpy(image.astype(np.float32))
        label_tensor = torch.from_numpy(label_one_hot.astype(np.float32))
        return image_tensor, label_tensor

--- src/data/prepare_slices.py ---
import argparse
from pathlib import Path
from typing import Dict, List, Tuple

import imageio
import nibabel as nib
import numpy as np


CHANNEL_TO_INDEX = {"t1": 0, "t1ce": 1, "t2": 2, "flair": 3}


def normalize_to_uint8(volume: np.ndarray) -> np.ndarray:
    vmin = float(volume.min())
    vmax = float(volume.max())
    if vmax <= vmin:
        return np.zeros_like(volume, dtype="uint8")
    scaled = (volume - vmin) / (vmax - vmin)
    return (scaled * 255.0).astype("uint8")


def select_slice_indices(image_array: np.ndarray, slices_per_volume: int) -> List[int]:
    """Select slice indices with highest voxel sums (proxy for information content)."""
    sums = [np.sum(image_array[:, :, i]) for i in range(image_array.shape[2])]
    top_indices = np.argsort(sums)[::-1][:slices_per_volume]
    return top_indices.tolist()


def select_mask_biased_indices(
    mask_array: np.ndarray,
    slices_per_volume: int,
    rng: np.random.Generator,
) -> List[int]:
    """Prefer slices with non-empty masks, then backfill with highest totals."""
    sums = np.array([np.sum(mask_array[:, :, i]) for i in range(mask_array.shape[2])])
    non_empty = np.where(sums > 0)[0]
    if len(non_empty) >= slices_per_volume:
        chosen = rng.choice(non_empty, size=slices_per_volume, replace=False)
        return sorted(chosen.tolist())
    remaining = np.setdiff1d(np.arange(mask_array.shape[2]), non_empty)
    ranked = remaining[np.argsort(sums[remaining])[::-1]]
    filled = np.concatenate([non_empty, ranked])[:slices_per_volume]
    return filled.tolist()


def save_slices(
    array: np.ndarray,
    slice_indices: List[int],
    output_dir: Path,
    prefix: str,
    counter_offset: int,
):
    for idx, slice_idx in enumerate(slice_indices):
        data = array[:, :, slice_idx]
        filename = f"{prefix}_{counter_offset + idx:05d}.png"
        output_path = output_dir / filename
        imageio.imwrite(output_path, data)


def build_file_lists_brats(dataset_root: Path) -> Tuple[List[Path], List[Path]]:
    modalities = []
    for pattern in ["*t1.nii.gz", "*t1ce.nii.gz", "*t2.nii.gz", "*flair.nii.gz"]:
        modalities.extend(sorted(dataset_root.rglob(pattern)))
    segmentations = sorted(dataset_root.rglob("*seg.nii.gz"))
    # replicate segmentations to align with 4 modalities per case
    segmentations = segmentations * 4
    return modalities, segmentations


def build_file_lists_msd(dataset_root: Path) -> List[Tuple[Path, Path]]:
    images_dir = dataset_root / "imagesTr"
    labels_dir = dataset_root / "labelsTr"
    if not images_dir.exists() or not labels_dir.exists():
        raise FileNotFoundError("Expected imagesTr/ and labelsTr/ in MSD Task01 root")
    images = sorted(images_dir.glob("*.nii.gz"))
    labels = {p.name.replace(".nii.gz", ""): p for p in labels_dir.glob("*.nii.gz")}
    pairs = []
    for image_path in images:
        key = image_path.name.replace(".nii.gz", "")
        if key not in labels:
            raise FileNotFoundError(f"Missing label for {image_path.name}")
        pairs.append((image_path, labels[key]))
    return pairs


def load_msd_channel(image_path: Path, channel: str) -> np.ndarray:
    image = nib.load(str(image_path)).get_fdata()
    if image.ndim != 4:
        raise ValueError(f"Expected 4D image volume for MSD, got shape {image.shape}")
    channel_idx = CHANNEL_TO_INDEX[channel]
    # MSD Task01 is commonly stored as (H, W, D, C) with order [t1, t1ce, t2, flair].
    if image.shape[-1] == 4:
        return image[..., channel_idx]
    if image.shape[0] == 4:
        return image[channel_idx, ...]
    raise ValueError(f"Unexpected MSD channel dimension for {image_path}")


def convert_label_mode(mask_array: np.ndarray, label_mode: str) -> np.ndarray:
    if label_mode == "binary":
        mask_array = (mask_array > 0).astype("uint8")
    else:
        mask_array = mask_array.astype("uint8")
    return mask_array


def build_output_paths(output_root: Path) -> Dict[str, Path]:
    output_paths = {
        "train_images": output_root / "train_frames" / "train",
        "train_masks": output_root / "train_masks" / "train",
        "val_images": output_root / "val_frames" / "val",
        "val_masks": output_root / "val_masks" / "val",
        "test_images": output_root / "test_frames" / "test",
        "test_masks": output_root / "test_masks" / "test",
    }
    for path in output_paths.values():
        path.mkdir(parents=True, exist_ok=True)
    return output_paths


def split_items(items: List, train_ratio: float, val_ratio: float) -> Dict[str, List]:
    total = len(items)
    train_split = int(train_ratio * total)
    val_split = int((train_ratio + val_ratio) * total)
    return {
        "train": items[:train_split],
        "val": items[train_split:val_split],
        "test": items[val_split:],
    }


def prepare_brats(
    dataset_root: Path,
    output_root: Path,
    slices_per_volume: int,
    label_mode: str,
    train_ratio: float,
    val_ratio: float,
):
    output_paths = build_output_paths(output_root)
    brains, segs = build_file_lists_brats(dataset_root)
    if len(brains) != len(segs):
        raise ValueError("Number of modality volumes does not match segmentation volumes")

    splits = split_items(list(zip(brains, segs)), train_ratio, val_ratio)
    slice_cache: Dict[Path, List[int]] = {}

    for split_name, pairs in splits.items():
        for idx, (brain_path, seg_path) in enumerate(pairs):
            if seg_path not in slice_cache:
                image_array = nib.load(str(seg_path)).get_fdata()
                slice_cache[seg_path] = select_slice_indices(image_array, slices_per_volume)

            slice_indices = slice_cache[seg_path]
            counter_offset = idx * slices_per_volume

            img_dir_key = f"{split_name}_images"
            mask_dir_key = f"{split_name}_masks"

            brain_array = nib.load(str(brain_path)).get_fdata()
            mask_array = nib.load(str(seg_path)).get_fdata()
            mask_array = convert_label_mode(mask_array, label_mode)

            save_slices(brain_array, slice_indices, output_paths[img_dir_key], f"{split_name}_frame", counter_offset)
            save_slices(mask_array, slice_indices, output_paths[mask_dir_key], f"{split_name}_mask", counter_offset)


def prepare_msd_task01(
    dataset_root: Path,
    output_root: Path,
    slices_per_volume: int,
    channel: str,
    label_mode: str,
    train_ratio: float,
    val_ratio: float,
):
    output_paths = build_output_paths(output_root)
    pairs = build_file_lists_msd(dataset_root)
    splits = split_items(pairs, train_ratio, val_ratio)
    rng = np.random.default_rng(42)

    for split_name, split_pairs in splits.items():
        for idx, (image_path, label_path) in enumerate(split_pairs):
            image_array = load_msd_channel(image_path, channel)
            label_array = nib.load(str(label_path)).get_fdata()
            label_array = convert_label_mode(label_array, label_mode)

            slice_indices = select_mask_biased_indices(label_array, slices_per_volume, rng)
            counter_offset = idx * slices_per_volume

            img_dir_key = f"{split_name}_images"
            mask_dir_key = f"{split_name}_masks"

            image_uint8 = normalize_to_uint8(image_array)
            save_slices(image_uint8, slice_indices, output_paths[img_dir_key], f"{split_name}_frame", counter_offset)
            save_slices(label_array, slice_indices, output_paths[mask_dir_key], f"{split_name}_mask", counter_offset)


def prepare_dataset(
    dataset_root: Path,
    output_root: Path,
    slices_per_volume: int = 20,
    dataset_format: str = "brats",
    channel: str = "flair",
    label_mode: str = "binary",
    train_ratio: float = 0.7,
    val_ratio: float = 0.2,
):
    if dataset_format == "brats":
        prepare_brats(dataset_root, output_root, slices_per_volume, label_mode, train_ratio, val_ratio)
    elif dataset_format == "msd_task01":
        prepare_msd_task01(dataset_root, output_root, slices_per_volume, channel, label_mode, train_ratio, val_ratio)
    else:
        raise ValueError(f"Unsupported dataset format: {dataset_format}")

    print(f"Finished preparing dataset at {output_root}")


def parse_args():
    parser = argparse.ArgumentParser(description="Convert NIfTI volumes to PNG slices")
    parser.add_argument(
        "--dataset-format",
        choices=["brats", "msd_task01"],
        default="brats",
        help="Dataset format to process",
    )
    parser.add_argument("--dataset-root", required=True, help="Path to dataset root")
    parser.add_argument("--output-root", default="./Dataset", help="Where to store PNG slices")
    parser.add_argument("--slices-per-volume", type=int, default=20, help="Number of slices to export per volume")
    parser.add_argument(
        "--channel",
        choices=sorted(CHANNEL_TO_INDEX.keys()),
        default="flair",
        help="MRI channel to extract for msd_task01",
    )
    parser.add_argument(
        "--label-mode",
        choices=["binary", "multiclass"],
        default="binary",
        help="Export masks as binary or multiclass labels",
    )
    return parser.parse_args()


def main():
    args = parse_args()
    dataset_root = Path(args.dataset_root).expanduser().resolve()
    output_root = Path(args.output_root).expanduser().resolve()

    prepare_dataset(
        dataset_root=dataset_root,
        output_root=output_root,
        slices_per_volume=args.slices_per_volume,
        dataset_format=args.dataset_format,
        channel=args.channel,
        label_mode=args.label_mode,
    )


if __name__ == "__main__":
    main()

--- src/eval.py ---
import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import tensorflow as tf

from src.data.dataset import build_dataloader
from src.models.unet import build_unet
from src.utils.config import apply_overrides, load_config, resolve_paths


def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate U-Net on val/test splits")
    parser.add_argument("--config", default="configs/config.yaml", help="Path to YAML config file")
    parser.add_argument("--data-root", dest="data_root", help="Override dataset root directory")
    parser.add_argument("--batch-size", type=int, help="Override batch size")
    parser.add_argument("--weights", help="Path to model weights .h5 file")
    parser.add_argument(
        "--dataset-format",
        choices=["brats", "msd_task01"],
        default="brats",
        help="Dataset format used to generate slices",
    )
    parser.add_argument(
        "--label-mode",
        choices=["binary", "multiclass"],
        default="binary",
        help="Label mode used to generate masks",
    )
    return parser.parse_args()


def build_model(cfg) -> tf.keras.Model:
    image_size = cfg["data"]["image_size"]
    input_channels = cfg["model"]["input_channels"]
    class_names = cfg["data"]["class_names"]
    model = build_unet(
        input_size=(image_size, image_size, input_channels),
        num_classes=len(class_names),
        base_filters=cfg["model"].get("base_filters", 32),
        learning_rate=cfg["training"]["learning_rate"],
    )
    return model


def resolve_weights_path(cfg, weights_override: str = None) -> Path:
    if weights_override:
        return Path(weights_override).expanduser().resolve()
    checkpoint_dir = Path(cfg["training"]["checkpoint_dir"])
    checkpoint_name = cfg["training"]["checkpoint_filename"]
    return (checkpoint_dir / checkpoint_name).expanduser().resolve()


def accumulate_dice(
    model: tf.keras.Model,
    loader,
    num_classes: int,
) -> Tuple[List[float], float]:
    intersections = np.zeros(num_classes, dtype=np.float64)
    totals = np.zeros(num_classes, dtype=np.float64)
    for images, masks in loader:
        preds = model.predict(images, verbose=0)
        pred_classes = np.argmax(preds, axis=-1)
        true_classes = np.argmax(masks, axis=-1)
        for class_idx in range(num_classes):
            pred_mask = pred_classes == class_idx
            true_mask = true_classes == class_idx
            intersections[class_idx] += np.logical_and(pred_mask, true_mask).sum()
            totals[class_idx] += pred_mask.sum() + true_mask.sum()

    dice_per_class = []
    for class_idx in range(num_classes):
        if totals[class_idx] == 0:
            dice = 1.0
        else:
            dice = (2.0 * intersections[class_idx]) / totals[class_idx]
        dice_per_class.append(float(dice))
    mean_dice = float(np.mean(dice_per_class)) if dice_per_class else 0.0
    return dice_per_class, mean_dice


def evaluate_split(model: tf.keras.Model, cfg, split_name: str) -> Dict[str, float]:
    class_names = cfg["data"]["class_names"]
    images_dir = cfg["data"][f"{split_name}_images"]
    masks_dir = cfg["data"][f"{split_name}_masks"]
    batch_size = cfg["training"]["batch_size"]
    if not images_dir.exists() or not masks_dir.exists():
        raise FileNotFoundError(f"Missing {split_name} data at {images_dir} or {masks_dir}")
    loader = build_dataloader(
        images_dir,
        masks_dir,
        class_names,
        batch_size=batch_size,
        image_size=cfg["data"]["image_size"],
        augmentation=None,
        shuffle=False,
    )
    if len(loader) == 0:
        raise ValueError(f"No samples found for {split_name} split")
    dice_per_class, mean_dice = accumulate_dice(model, loader, len(class_names))
    return {
        "dice_per_class": dict(zip(class_names, dice_per_class)),
        "mean_dice": mean_dice,
    }


def main():
    args = parse_args()
    cfg = load_config(args.config)
    cfg = apply_overrides(cfg, args)
    if args.batch_size:
        cfg["training"]["batch_size"] = args.batch_size
    cfg = resolve_paths(cfg)

    weights_path = resolve_weights_path(cfg, args.weights)
    if not weights_path.exists():
        raise FileNotFoundError(f"Model weights not found at {weights_path}")

    model = build_model(cfg)
    model.load_weights(weights_path)

    metrics = {
        "dataset_format": args.dataset_format,
        "label_mode": args.label_mode,
        "val": evaluate_split(model, cfg, "val"),
        "test": evaluate_split(model, cfg, "test"),
    }

    output_dir = Path("outputs").resolve()
    output_dir.mkdir(parents=True, exist_ok=True)
    metrics_path = output_dir / "metrics.json"
    with metrics_path.open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    print(f"Wrote metrics to {metrics_path}")


if __name__ == "__main__":
    main()

--- src/eval_3d.py ---
import argparse
import json
import logging
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch
import yaml
from monai.inferers import sliding_window_inference
from monai.networks.nets import UNet

from src.data.msd_task01_3d import MSDTask01Dataset3D, build_splits, list_msd_task01_cases


def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate 3D U-Net on MSD Task01")
    parser.add_argument("--config", default="configs/config_3d.yaml", help="Path to YAML config file")
    parser.add_argument("--weights", required=True, help="Path to model weights .pt")
    return parser.parse_args()


def load_config(path: str) -> Dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def build_model(cfg: Dict, num_classes: int) -> UNet:
    model_cfg = cfg["model"]
    return UNet(
        spatial_dims=3,
        in_channels=model_cfg["in_channels"],
        out_channels=num_classes,
        channels=model_cfg["channels"],
        strides=model_cfg["strides"],
        num_res_units=model_cfg["num_res_units"],
        norm=model_cfg["norm"],
    )


def get_git_hash() -> str:
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--short", "HEAD"],
            capture_output=True,
            text=True,
            check=True,
        )
        return result.stdout.strip()
    except Exception:
        return ""


def _ensure_5d(tensor: torch.Tensor, name: str) -> torch.Tensor:
    if tensor.ndim == 4:
        return tensor.unsqueeze(0)
    if tensor.ndim == 5:
        return tensor
    raise ValueError(f"{name} has unexpected shape {tuple(tensor.shape)}")


def _compute_dice(pred: torch.Tensor, target: torch.Tensor, include_background: bool) -> torch.Tensor:
    pred = _ensure_5d(pred, "pred")
    target = _ensure_5d(target, "target")
    if target.shape[2:] != pred.shape[2:]:
        raise ValueError(
            f"Spatial mismatch: pred {tuple(pred.shape)} vs target {tuple(target.shape)}"
        )
    if target.shape[1] != pred.shape[1]:
        target = torch.argmax(target, dim=1, keepdim=True)
        target = torch.nn.functional.one_hot(
            target.long().squeeze(1), num_classes=pred.shape[1]
        ).permute(0, 4, 1, 2, 3).float()
    target = target.to(pred.device)
    pred = pred.float()
    target = target.float()
    if not include_background and pred.shape[1] > 1:
        pred = pred[:, 1:]
        target = target[:, 1:]
    intersection = (pred * target).sum(dim=(2, 3, 4))
    denom = pred.sum(dim=(2, 3, 4)) + target.sum(dim=(2, 3, 4))
    dice = torch.where(denom > 0, (2.0 * intersection) / denom, torch.ones_like(denom))
    return dice


def _align_label_spatial(label: torch.Tensor, pred: torch.Tensor) -> torch.Tensor:
    if label.shape[2:] == pred.shape[2:]:
        return label
    perms = [
        (2, 3, 4),
        (2, 4, 3),
        (3, 2, 4),
        (3, 4, 2),
        (4, 2, 3),
        (4, 3, 2),
    ]
    for perm in perms:
        candidate = label.permute(0, 1, *perm)
        if candidate.shape[2:] == pred.shape[2:]:
            return candidate
    raise ValueError(
        f"Unable to align label spatial dims {tuple(label.shape)} to pred {tuple(pred.shape)}"
    )


def _foreground_mean(dice_per_class: List[float]) -> float:
    if not dice_per_class:
        return 0.0
    if len(dice_per_class) == 1:
        return float(dice_per_class[0])
    return float(np.mean(dice_per_class[1:]))


def _accumulate_dice(
    dice: torch.Tensor,
    label: torch.Tensor,
    num_classes: int,
    ignore_empty_foreground: bool,
) -> Tuple[np.ndarray, np.ndarray]:
    label = _ensure_5d(label, "label")
    target_sum = label.sum(dim=(2, 3, 4))
    dice_sum = np.zeros(num_classes, dtype=np.float64)
    dice_count = np.zeros(num_classes, dtype=np.float64)
    for cls_idx in range(num_classes):
        if cls_idx == 0:
            valid = np.ones(target_sum.shape[0], dtype=bool)
        else:
            valid = target_sum[:, cls_idx].cpu().numpy() > 0
            if not ignore_empty_foreground:
                valid = np.ones_like(valid, dtype=bool)
        dice_vals = dice[:, cls_idx].detach().cpu().numpy()
        if valid.any():
            dice_sum[cls_idx] += dice_vals[valid].sum()
            dice_count[cls_idx] += valid.sum()
    return dice_sum, dice_count


def setup_logging():
    logger = logging.getLogger("eval_3d")
    logger.setLevel(logging.INFO)
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


def evaluate_split(
    model: torch.nn.Module,
    dataset: torch.utils.data.Dataset,
    device: torch.device,
    roi_size: Tuple[int, int, int],
    overlap: float,
    sw_batch_size: int,
    num_classes: int,
    is_binary: bool,
    logger: logging.Logger,
    split_name: str,
    ignore_empty_foreground: bool,
    prediction_threshold: float,
    log_interval: int = 1,
) -> Tuple[List[float], float, int]:
    model.eval()

    dice_scores = []
    dice_sum = np.zeros(num_classes, dtype=np.float64)
    dice_count = np.zeros(num_classes, dtype=np.float64)
    with torch.no_grad():
        start_time = time.perf_counter()
        for idx, (image, label) in enumerate(dataset, start=1):
            image = image.unsqueeze(0).to(device)
            label = label.unsqueeze(0).to(device)
            logits = sliding_window_inference(
                image, roi_size=roi_size, sw_batch_size=sw_batch_size, predictor=model, overlap=overlap
            )
            if is_binary:
                logit_tumor = logits[:, 1:2] if logits.shape[1] >= 2 else logits
                probs = torch.sigmoid(logit_tumor)
                pred = (probs > prediction_threshold).float()
                if pred.shape[1] == 1 and num_classes == 2:
                    pred = torch.cat([1.0 - pred, pred], dim=1)
            else:
                pred_labels = torch.argmax(logits, dim=1, keepdim=True)
                pred = torch.nn.functional.one_hot(
                    pred_labels.squeeze(1), num_classes=num_classes
                ).permute(0, 4, 1, 2, 3).float()
            label = _align_label_spatial(label, pred)
            dice = _compute_dice(pred, label, include_background=True)
            dice_scores.append(dice.mean(dim=0).cpu().numpy())
            batch_sum, batch_count = _accumulate_dice(dice, label, num_classes, ignore_empty_foreground)
            dice_sum += batch_sum
            dice_count += batch_count
            if log_interval and idx % log_interval == 0:
                elapsed = time.perf_counter() - start_time
                logger.info(
                    "%s volume %s/%s | avg_time=%.2fs",
                    split_name,
                    idx,
                    len(dataset),
                    elapsed / idx,
                )

    if not dice_scores:
        return [0.0] * num_classes, 0.0, 0
    dice_scores = np.stack(dice_scores, axis=0)
    dice_per_class = np.where(dice_count > 0, dice_sum / np.maximum(dice_count, 1.0), 0.0)
    foreground_dice = _foreground_mean(dice_per_class.tolist())
    return dice_per_class.tolist(), float(foreground_dice), len(dice_scores)


def main():
    args = parse_args()
    cfg = load_config(args.config)
    data_cfg = cfg["data"]
    inference_cfg = cfg["inference"]

    cases = list_msd_task01_cases(Path(data_cfg["root"]))
    splits = build_splits(
        cases,
        train_ratio=data_cfg["train_ratio"],
        val_ratio=data_cfg["val_ratio"],
        seed=data_cfg.get("seed", 42),
        list_files=data_cfg.get("list_files"),
    )

    num_classes = len(data_cfg.get("class_names", ["background", "tumor"]))
    if data_cfg["label_mode"] == "binary":
        num_classes = 2

    percentiles = tuple(data_cfg.get("percentiles", [0.5, 99.5]))
    val_dataset = MSDTask01Dataset3D(
        splits["val"],
        roi_size=None,
        label_mode=data_cfg["label_mode"],
        num_classes=num_classes,
        pos_ratio=0.0,
        percentiles=percentiles,
        mode="val",
        seed=data_cfg.get("seed", 42),
    )
    test_dataset = MSDTask01Dataset3D(
        splits["test"],
        roi_size=None,
        label_mode=data_cfg["label_mode"],
        num_classes=num_classes,
        pos_ratio=0.0,
        percentiles=percentiles,
        mode="test",
        seed=data_cfg.get("seed", 42),
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = build_model(cfg, num_classes).to(device)
    model.load_state_dict(torch.load(args.weights, map_location=device))

    roi_size = tuple(inference_cfg["roi_size"])
    overlap = inference_cfg["overlap"]
    sw_batch_size = inference_cfg["sw_batch_size"]

    logger = setup_logging()
    logger.info("Using device: %s", device)
    logger.info("ROI size: %s | overlap: %s | sw_batch_size: %s", roi_size, overlap, sw_batch_size)
    logger.info("Val volumes: %s | Test volumes: %s", len(val_dataset), len(test_dataset))
    ignore_empty_foreground = inference_cfg.get("ignore_empty_foreground", True)
    is_binary = data_cfg["label_mode"] == "binary"
    prediction_threshold = cfg.get("training", {}).get("prediction_threshold", 0.5)
    logger.info(
        "ignore_empty_foreground: %s | pred_rule: %s",
        ignore_empty_foreground,
        f"sigmoid>{prediction_threshold}" if is_binary else "argmax over softmax logits",
    )
    val_dice, val_foreground, val_count = evaluate_split(
        model,
        val_dataset,
        device,
        roi_size,
        overlap,
        sw_batch_size,
        num_classes,
        is_binary,
        logger,
        "val",
        ignore_empty_foreground,
        prediction_threshold,
    )
    test_dice, test_foreground, test_count = evaluate_split(
        model,
        test_dataset,
        device,
        roi_size,
        overlap,
        sw_batch_size,
        num_classes,
        is_binary,
        logger,
        "test",
        ignore_empty_foreground,
        prediction_threshold,
    )

    class_names = data_cfg.get("class_names", [f"class_{i}" for i in range(num_classes)])
    if len(class_names) != num_classes:
        class_names = [f"class_{i}" for i in range(num_classes)]

    metrics = {
        "dataset_format": "msd_task01",
        "label_mode": data_cfg["label_mode"],
        "val": {
            "dice_per_class": dict(zip(class_names, [float(x) for x in val_dice])),
            "mean_dice": float(np.mean(val_dice)) if val_dice else 0.0,
            "foreground_mean_dice": float(val_foreground),
            "dice_background": float(val_dice[0]) if val_dice else 0.0,
            "dice_tumor": float(val_dice[1]) if len(val_dice) > 1 else 0.0,
            "number_of_volumes": val_count,
        },
        "test": {
            "dice_per_class": dict(zip(class_names, [float(x) for x in test_dice])),
            "mean_dice": float(np.mean(test_dice)) if test_dice else 0.0,
            "foreground_mean_dice": float(test_foreground),
            "dice_background": float(test_dice[0]) if test_dice else 0.0,
            "dice_tumor": float(test_dice[1]) if len(test_dice) > 1 else 0.0,
            "number_of_volumes": test_count,
        },
        "inference": {
            "roi_size": list(roi_size),
            "overlap": overlap,
            "sw_batch_size": sw_batch_size,
            "include_background": True,
            "ignore_empty_foreground": bool(ignore_empty_foreground),
            "pred_rule": f"sigmoid>{prediction_threshold}" if is_binary else "argmax over softmax logits",
        },
    }

    git_hash = get_git_hash()
    if git_hash:
        metrics["git_commit"] = git_hash

    output_dir = Path("outputs")
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "metrics_3d.json"
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    print(f"Wrote metrics to {output_path}")


if __name__ == "__main__":
    main()

--- src/models/__init__.py ---


--- src/models/unet.py ---
import tensorflow as tf
from tensorflow.keras.layers import Activation, Concatenate, Conv2D, Conv2DTranspose, Input, MaxPooling2D
from tensorflow.keras.optimizers import Adam


def dice_coefficient(y_true, y_pred, smooth: float = 1e-6):
    """Dice metric for one-hot encoded masks."""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    numerator = 2.0 * tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])
    denominator = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3])
    dice = (numerator + smooth) / (denominator + smooth)
    return tf.reduce_mean(dice)


def soft_dice_loss(y_true, y_pred, smooth: float = 1e-6):
    dice = dice_coefficient(y_true, y_pred, smooth)
    return 1.0 - dice


_cce = tf.keras.losses.CategoricalCrossentropy()


def combined_cce_dice_loss(y_true, y_pred):
    """Categorical crossentropy + soft dice to sharpen boundaries."""
    return _cce(y_true, y_pred) + soft_dice_loss(y_true, y_pred)


def build_unet(
    input_size=(256, 256, 1),
    num_classes: int = 4,
    base_filters: int = 32,
    learning_rate: float = 1e-4,
) -> tf.keras.Model:
    initializer = "he_normal"

    inputs = Input(shape=input_size)

    conv1 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(inputs)
    conv1 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool1)
    conv2 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool2)
    conv3 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool3)
    conv4 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    conv5 = Conv2D(base_filters * 16, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool4)
    conv5 = Conv2D(base_filters * 16, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv5)

    up6 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters * 8, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv5),
        conv4,
    ])
    conv6 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up6)
    conv6 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv6)

    up7 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters * 4, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv6),
        conv3,
    ])
    conv7 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up7)
    conv7 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv7)

    up8 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters * 2, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv7),
        conv2,
    ])
    conv8 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up8)
    conv8 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv8)

    up9 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv8),
        conv1,
    ])
    conv9 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up9)
    conv9 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv9)

    conv10 = Conv2D(num_classes, (1, 1), activation="relu", kernel_initializer=initializer)(conv9)
    outputs = Activation("softmax")(conv10)

    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss=combined_cce_dice_loss,
        metrics=[dice_coefficient, "accuracy"],
    )
    return model

--- src/service/__init__.py ---


--- src/service/api.py ---
import base64
import io
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict

import numpy as np
from fastapi import Body, FastAPI, File, HTTPException, UploadFile
from fastapi.responses import JSONResponse, StreamingResponse
from PIL import Image

from src.models.unet import build_unet

app = FastAPI(title="Brain Tumor Segmentation API", version="1.0")

WEIGHTS_PATH = Path("./weights/best_model_unet.h5").expanduser().resolve()
IMAGE_SIZE = 256
INPUT_CHANNELS = 1
NUM_CLASSES = 4


@lru_cache(maxsize=1)
def load_model():
    if not WEIGHTS_PATH.exists():
        raise FileNotFoundError(f"Model weights not found at {WEIGHTS_PATH}")
    model = build_unet(
        input_size=(IMAGE_SIZE, IMAGE_SIZE, INPUT_CHANNELS),
        num_classes=NUM_CLASSES,
        base_filters=32,
    )
    model.load_weights(WEIGHTS_PATH)
    return model


def preprocess_image(file_bytes: bytes) -> np.ndarray:
    try:
        image = Image.open(io.BytesIO(file_bytes)).convert("L")
    except Exception as exc:  # pillow-specific errors vary
        raise HTTPException(status_code=400, detail=f"Invalid image: {exc}")
    image = image.resize((IMAGE_SIZE, IMAGE_SIZE))
    array = np.asarray(image).astype("float32") / 255.0
    array = np.expand_dims(array, axis=(0, -1))  # shape: (1, H, W, 1)
    return array


def postprocess_mask(prediction: np.ndarray) -> Image.Image:
    """Convert softmax output to single-channel mask PNG."""
    # prediction shape: (1, H, W, num_classes)
    class_map = np.argmax(prediction, axis=-1)[0].astype("uint8")
    mask = Image.fromarray(class_map, mode="L")
    return mask


def decode_vertex_instance(instance: Any) -> bytes:
    if isinstance(instance, str):
        payload = instance
    elif isinstance(instance, dict):
        if "b64" in instance:
            payload = instance["b64"]
        elif "image_bytes" in instance and isinstance(instance["image_bytes"], dict):
            payload = instance["image_bytes"].get("b64")
        else:
            raise HTTPException(status_code=400, detail="Unsupported instance format")
    else:
        raise HTTPException(status_code=400, detail="Unsupported instance type")

    if not payload:
        raise HTTPException(status_code=400, detail="Missing base64 payload")

    try:
        return base64.b64decode(payload, validate=True)
    except Exception as exc:
        raise HTTPException(status_code=400, detail=f"Invalid base64 payload: {exc}")


def validate_prediction(prediction: np.ndarray):
    if prediction.ndim != 4 or prediction.shape[-1] != NUM_CLASSES:
        raise HTTPException(status_code=400, detail="Model output has unexpected shape")


@app.get("/health")
def health():
    return {"status": "ok"}


@app.post("/predict")
def predict(file: UploadFile = File(...)):
    model = load_model()
    file_bytes = file.file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Empty file")

    input_tensor = preprocess_image(file_bytes)
    prediction = model.predict(input_tensor)
    validate_prediction(prediction)
    mask_img = postprocess_mask(prediction)

    buffer = io.BytesIO()
    mask_img.save(buffer, format="PNG")
    buffer.seek(0)

    headers = {"Content-Disposition": f"inline; filename=\"mask_{file.filename or 'output'}.png\""}
    return StreamingResponse(buffer, media_type="image/png", headers=headers)


@app.post("/predict-json")
def predict_json(file: UploadFile = File(...)):
    """Alternative JSON response returning mask values."""
    model = load_model()
    file_bytes = file.file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Empty file")

    input_tensor = preprocess_image(file_bytes)
    prediction = model.predict(input_tensor)
    validate_prediction(prediction)
    class_map = np.argmax(prediction, axis=-1)[0].astype(int).tolist()
    return JSONResponse({"mask": class_map})


@app.post("/vertex/predict")
def vertex_predict(payload: Dict[str, Any] = Body(...)):
    """Vertex AI-compatible prediction endpoint using base64-encoded image bytes."""
    model = load_model()
    instances = payload.get("instances")
    if not isinstance(instances, list) or not instances:
        raise HTTPException(status_code=400, detail="Payload must include non-empty 'instances' list")

    predictions = []
    for instance in instances:
        file_bytes = decode_vertex_instance(instance)
        input_tensor = preprocess_image(file_bytes)
        prediction = model.predict(input_tensor, verbose=0)
        validate_prediction(prediction)
        class_map = np.argmax(prediction, axis=-1)[0].astype(int).tolist()
        predictions.append({"mask": class_map})

    return JSONResponse({"predictions": predictions})

--- src/train_3d.py ---
import argparse
import csv
import json
import random
import logging
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch
import yaml
from monai.losses import DiceCELoss, DiceLoss
from monai.networks.nets import UNet
from PIL import Image
from torch.utils.tensorboard import SummaryWriter

from src.data.msd_task01_3d import MSDTask01Dataset3D, build_splits, list_msd_task01_cases


def setup_logging(log_path: Path):
    logger = logging.getLogger("train_3d")
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    file_handler = logging.FileHandler(log_path)
    file_handler.setFormatter(formatter)
    if not logger.handlers:
        logger.addHandler(stream_handler)
        logger.addHandler(file_handler)
    return logger


def parse_args():
    parser = argparse.ArgumentParser(description="Train 3D U-Net on MSD Task01")
    parser.add_argument("--config", default="configs/config_3d.yaml", help="Path to YAML config file")
    parser.add_argument("--max-epochs", type=int, help="Override max epochs")
    parser.add_argument("--limit-train-batches", type=int, help="Limit train batches per epoch")
    parser.add_argument("--limit-val-batches", type=int, help="Limit val batches per epoch")
    parser.add_argument("--deterministic", action="store_true", help="Enable deterministic training")
    return parser.parse_args()


def set_seeds(seed: int, deterministic: bool):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def load_config(path: str) -> Dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def resolve_config(cfg: Dict, args) -> Dict:
    cfg = json.loads(json.dumps(cfg))
    if args.max_epochs is not None:
        cfg["training"]["max_epochs"] = args.max_epochs
    if args.limit_train_batches is not None:
        cfg["training"]["limit_train_batches"] = args.limit_train_batches
    if args.limit_val_batches is not None:
        cfg["training"]["limit_val_batches"] = args.limit_val_batches
    if args.deterministic:
        cfg["training"]["deterministic"] = True
    return cfg


def build_model(cfg: Dict, num_classes: int) -> UNet:
    model_cfg = cfg["model"]
    return UNet(
        spatial_dims=3,
        in_channels=model_cfg["in_channels"],
        out_channels=num_classes,
        channels=model_cfg["channels"],
        strides=model_cfg["strides"],
        num_res_units=model_cfg["num_res_units"],
        norm=model_cfg["norm"],
    )


def build_dataloaders(cfg: Dict) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
    data_cfg = cfg["data"]
    cases = list_msd_task01_cases(Path(data_cfg["root"]))
    splits = build_splits(
        cases,
        train_ratio=data_cfg["train_ratio"],
        val_ratio=data_cfg["val_ratio"],
        seed=data_cfg.get("seed", 42),
        list_files=data_cfg.get("list_files"),
    )
    num_classes = len(data_cfg.get("class_names", ["background", "tumor"]))
    if data_cfg["label_mode"] == "binary":
        num_classes = 2
    roi_size = data_cfg.get("roi_size")
    percentiles = tuple(data_cfg.get("percentiles", [0.5, 99.5]))

    train_dataset = MSDTask01Dataset3D(
        splits["train"],
        roi_size=roi_size,
        label_mode=data_cfg["label_mode"],
        num_classes=num_classes,
        pos_ratio=data_cfg.get("pos_ratio", 0.5),
        percentiles=percentiles,
        mode="train",
        seed=data_cfg.get("seed", 42),
        max_pos_attempts=data_cfg.get("max_pos_attempts", 10),
        min_pos_voxel_frac=data_cfg.get("min_pos_voxel_frac", 0.0),
    )
    val_dataset = MSDTask01Dataset3D(
        splits["val"],
        roi_size=roi_size,
        label_mode=data_cfg["label_mode"],
        num_classes=num_classes,
        pos_ratio=0.0,
        percentiles=percentiles,
        mode="val",
        seed=data_cfg.get("seed", 42),
    )

    num_workers = cfg["training"]["num_workers"]
    pin_memory = torch.cuda.is_available()
    loader_kwargs = {
        "num_workers": num_workers,
        "pin_memory": pin_memory,
        "persistent_workers": num_workers > 0,
    }
    if num_workers > 0:
        loader_kwargs["prefetch_factor"] = 2
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=cfg["training"]["batch_size"],
        shuffle=True,
        drop_last=True,
        **loader_kwargs,
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=cfg["training"]["batch_size"],
        shuffle=False,
        **loader_kwargs,
    )
    return train_loader, val_loader


def _ensure_5d(tensor: torch.Tensor, name: str) -> torch.Tensor:
    if tensor.ndim == 4:
        return tensor.unsqueeze(0)
    if tensor.ndim == 5:
        return tensor
    if tensor.ndim == 6 and tensor.shape[1] == 1:
        return tensor.squeeze(1)
    raise ValueError(f"{name} has unexpected shape {tuple(tensor.shape)}")


def compute_dice(pred: torch.Tensor, target: torch.Tensor, include_background: bool) -> torch.Tensor:
    pred = _ensure_5d(pred, "pred")
    target = _ensure_5d(target, "target")
    if pred.shape[0] != target.shape[0]:
        raise ValueError(f"Batch mismatch: pred {pred.shape} vs target {target.shape}")
    if target.shape[1] != pred.shape[1]:
        target = torch.argmax(target, dim=1, keepdim=True)
        target = torch.nn.functional.one_hot(
            target.long().squeeze(1), num_classes=pred.shape[1]
        ).permute(0, 4, 1, 2, 3).float()
    target = target.to(pred.device)
    pred = pred.float()
    target = target.float()
    if not include_background and pred.shape[1] > 1:
        pred = pred[:, 1:]
        target = target[:, 1:]
    intersection = (pred * target).sum(dim=(2, 3, 4))
    denom = pred.sum(dim=(2, 3, 4)) + target.sum(dim=(2, 3, 4))
    dice = torch.where(denom > 0, (2.0 * intersection) / denom, torch.ones_like(denom))
    return dice


def _validate_labels(labels: torch.Tensor, num_classes: int, is_binary: bool) -> None:
    if labels.ndim != 5:
        raise ValueError(f"Expected labels shape (B,C,D,H,W), got {tuple(labels.shape)}")
    if is_binary:
        if labels.shape[1] not in (1, 2):
            raise ValueError(
                f"Binary labels must have C=1 or C=2, got C={labels.shape[1]}"
            )
    elif labels.shape[1] != num_classes:
        raise ValueError(
            f"Label channel mismatch: expected C={num_classes}, got C={labels.shape[1]}"
        )
    if labels.min().item() < -1e-3 or labels.max().item() > 1 + 1e-3:
        raise ValueError("Labels must be in [0,1] for one-hot encoding")
    if not is_binary or labels.shape[1] == 2:
        channel_sum = labels.sum(dim=1)
        if not torch.allclose(channel_sum, torch.ones_like(channel_sum), atol=1e-3):
            raise ValueError("Labels must be one-hot encoded (sum across channels = 1).")


def _pred_to_onehot(
    logits: torch.Tensor,
    num_classes: int,
    is_binary: bool,
    threshold: float = 0.5,
) -> torch.Tensor:
    if is_binary:
        logit_tumor = logits[:, 1:2] if logits.shape[1] >= 2 else logits
        probs = torch.sigmoid(logit_tumor)
        pred = (probs > threshold).float()
        if pred.shape[1] == 1 and num_classes == 2:
            pred = torch.cat([1.0 - pred, pred], dim=1)
        return pred
    pred_labels = torch.argmax(logits, dim=1, keepdim=True)
    return torch.nn.functional.one_hot(
        pred_labels.squeeze(1), num_classes=num_classes
    ).permute(0, 4, 1, 2, 3).float()


def _foreground_mask(labels: torch.Tensor) -> torch.Tensor:
    if labels.shape[1] == 1:
        return labels[:, 0] > 0.5
    return labels[:, 1:].sum(dim=1) > 0


def _log_sanity_stats(
    logger: logging.Logger,
    preds: torch.Tensor,
    labels: torch.Tensor,
    prefix: str,
) -> None:
    foreground_gt = _foreground_mask(labels)
    foreground_pred = _foreground_mask(preds)
    tumor_voxel_frac = foreground_gt.float().mean().item()
    pred_tumor_frac = foreground_pred.float().mean().item()
    tp = (foreground_pred & foreground_gt).sum().item()
    fp = (foreground_pred & ~foreground_gt).sum().item()
    fn = (~foreground_pred & foreground_gt).sum().item()
    logger.info(
        "%s tumor_voxel_frac=%.6f pred_tumor_voxel_frac=%.6f tp=%s fp=%s fn=%s",
        prefix,
        tumor_voxel_frac,
        pred_tumor_frac,
        int(tp),
        int(fp),
        int(fn),
    )


def _normalize_slice(slice_array: np.ndarray) -> np.ndarray:
    vmin = float(slice_array.min())
    vmax = float(slice_array.max())
    if vmax <= vmin:
        return np.zeros_like(slice_array, dtype=np.uint8)
    scaled = (slice_array - vmin) / (vmax - vmin)
    return (scaled * 255.0).astype(np.uint8)


def _select_slice_index(label_one_hot: np.ndarray) -> int:
    if label_one_hot.shape[0] > 1:
        foreground = label_one_hot[1:].sum(axis=0)
    else:
        foreground = label_one_hot[0]
    areas = foreground.sum(axis=(1, 2))
    if areas.max() == 0:
        return label_one_hot.shape[1] // 2
    return int(np.argmax(areas))


def _make_overlay(input_slice: np.ndarray, gt_mask: np.ndarray, pred_mask: np.ndarray) -> np.ndarray:
    base = np.stack([input_slice] * 3, axis=-1).astype(np.float32)
    overlay = base.copy()
    overlay[gt_mask > 0, 1] = 255
    overlay[pred_mask > 0, 0] = 255
    return overlay.astype(np.uint8)


def _save_vis_images(vis_dir: Path, case_idx: int, input_slice, gt_slice, pred_slice, overlay):
    Image.fromarray(input_slice).save(vis_dir / f"case_{case_idx}_input.png")
    Image.fromarray(gt_slice).save(vis_dir / f"case_{case_idx}_gt.png")
    Image.fromarray(pred_slice).save(vis_dir / f"case_{case_idx}_pred.png")
    Image.fromarray(overlay).save(vis_dir / f"case_{case_idx}_overlay.png")


def _log_visuals(
    model: torch.nn.Module,
    val_loader: torch.utils.data.DataLoader,
    device: torch.device,
    writer: SummaryWriter,
    run_dir: Path,
    epoch: int,
    num_classes: int,
    max_cases: int,
):
    vis_dir = run_dir / "vis" / f"epoch_{epoch:02d}"
    vis_dir.mkdir(parents=True, exist_ok=True)
    overlays = []
    model.eval()
    case_idx = 0
    with torch.no_grad():
        for images, labels in val_loader:
            for b in range(images.shape[0]):
                if case_idx >= max_cases:
                    break
                image = images[b : b + 1].to(device)
                label = labels[b].cpu().numpy()
                logits = model(image)
                pred = torch.softmax(logits, dim=1).argmax(dim=1).cpu().numpy()[0]

                slice_idx = _select_slice_index(label)
                input_slice = images[b, 0, slice_idx].cpu().numpy()
                input_slice = _normalize_slice(input_slice)

                gt_slice = np.argmax(label[:, slice_idx], axis=0).astype(np.uint8)
                pred_slice = pred[slice_idx].astype(np.uint8)

                if num_classes > 1:
                    scale = 255 // max(1, num_classes - 1)
                else:
                    scale = 255
                pred_viz = (pred_slice * scale).astype(np.uint8)
                gt_viz = (gt_slice * scale).astype(np.uint8)

                overlay = _make_overlay(input_slice, gt_slice, pred_slice)
                _save_vis_images(vis_dir, case_idx, input_slice, gt_viz, pred_viz, overlay)
                overlays.append(overlay)
                case_idx += 1
            if case_idx >= max_cases:
                break

    if overlays:
        grid = np.concatenate(overlays, axis=1)
        writer.add_image("vis/overlay", grid, epoch, dataformats="HWC")


def main():
    args = parse_args()
    cfg = resolve_config(load_config(args.config), args)
    run_dir = Path(cfg["training"]["output_dir"]) / datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir.mkdir(parents=True, exist_ok=True)
    logger = setup_logging(run_dir / "train.log")

    (run_dir / "git_commit.txt").write_text(
        subprocess.getoutput("git rev-parse --short HEAD").strip() or "unknown",
        encoding="utf-8",
    )
    env_lines = [sys.version.split()[0]]
    for pkg in ("torch", "monai"):
        try:
            mod = __import__(pkg)
            env_lines.append(f"{pkg}=={getattr(mod, '__version__', 'unknown')}")
        except Exception:
            env_lines.append(f"{pkg} not installed")
    (run_dir / "env.txt").write_text("\n".join(env_lines) + "\n", encoding="utf-8")
    gpu_info = subprocess.getoutput("nvidia-smi -L") if torch.cuda.is_available() else "cpu"
    (run_dir / "gpu.txt").write_text(gpu_info + "\n", encoding="utf-8")

    with (run_dir / "train_config_resolved.yaml").open("w", encoding="utf-8") as f:
        yaml.safe_dump(cfg, f, sort_keys=False)

    set_seeds(cfg["training"].get("seed", 42), cfg["training"].get("deterministic", False))

    data_cfg = cfg["data"]
    num_classes = len(data_cfg.get("class_names", ["background", "tumor"]))
    if data_cfg["label_mode"] == "binary":
        num_classes = 2

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = build_model(cfg, num_classes).to(device)

    is_binary = data_cfg["label_mode"] == "binary"
    if is_binary:
        pos_weight_value = float(cfg["training"].get("pos_weight", 5.0))
        pos_weight = torch.tensor([pos_weight_value], device=device)
        bce_loss = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        dice_loss = DiceLoss(sigmoid=True, include_background=False)

        def loss_fn(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
            if labels.shape[1] == 2:
                target = labels[:, 1:2]
            elif labels.shape[1] == 1:
                target = labels
            else:
                raise ValueError(
                    f"Expected binary labels with C=1 or C=2, got shape {tuple(labels.shape)}"
                )
            logit_tumor = logits[:, 1:2] if logits.shape[1] >= 2 else logits
            return dice_loss(logit_tumor, target) + bce_loss(logit_tumor, target)

    else:
        loss_fn = DiceCELoss(
            to_onehot_y=False,
            softmax=True,
            include_background=False,
        )
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg["training"]["learning_rate"])
    scaler = torch.amp.GradScaler("cuda", enabled=torch.cuda.is_available())

    train_loader, val_loader = build_dataloaders(cfg)
    log_interval = cfg["training"].get("log_interval", 0)
    vis_interval = cfg["training"].get("vis_interval", 5)
    max_vis_cases = cfg["training"].get("max_vis_cases", 3)
    ignore_empty_foreground = cfg["training"].get("ignore_empty_foreground", True)
    log_sanity_steps = int(cfg["training"].get("log_sanity_steps", 50))
    logger.info("Train batches per epoch: %s | Val batches: %s", len(train_loader), len(val_loader))
    logger.info("Using device: %s", device)
    logger.info("ROI size: %s", data_cfg.get("roi_size"))
    logger.info("num_workers: %s", cfg["training"]["num_workers"])
    logger.info(
        "limit_train_batches: %s | limit_val_batches: %s",
        cfg["training"].get("limit_train_batches"),
        cfg["training"].get("limit_val_batches"),
    )
    logger.info(
        "ignore_empty_foreground: %s | pred_rule: %s",
        ignore_empty_foreground,
        f"sigmoid>{cfg['training'].get('prediction_threshold', 0.5)}" if is_binary else "argmax over softmax logits",
    )
    if torch.cuda.is_available():
        logger.info("GPU name: %s", torch.cuda.get_device_name(0))
        logger.info("CUDA capability: %s", torch.cuda.get_device_capability(0))
    logger.info("AMP enabled: %s", torch.cuda.is_available())
    logger.info("prediction_threshold: %s", cfg["training"].get("prediction_threshold", 0.5))
    logger.info(
        "pred_rule: %s",
        f"sigmoid>{cfg['training'].get('prediction_threshold', 0.5)}" if is_binary else "argmax over softmax logits",
    )
    if "OMP_NUM_THREADS" not in os.environ or "MKL_NUM_THREADS" not in os.environ:
        logger.warning(
            "For best throughput set: OMP_NUM_THREADS=1 and MKL_NUM_THREADS=1"
        )

    best_fg_dice = -1.0
    best_val_loss = float("inf")
    best_epoch = 0
    metrics_path = run_dir / "metrics.csv"
    tb_writer = SummaryWriter(log_dir=run_dir)
    metrics_json_path = run_dir / "metrics_per_epoch.json"
    metrics_history = []
    with metrics_path.open("w", newline="", encoding="utf-8") as f:
        csv_writer = csv.writer(f)
        csv_writer.writerow(
            [
                "epoch",
                "train_loss",
                "val_loss",
                "val_mean_dice",
                "val_foreground_dice",
                "val_dice_background",
                "val_dice_tumor",
                "lr",
            ]
        )

        global_step = 0
        for epoch in range(cfg["training"]["max_epochs"]):
            logger.info("Epoch %s/%s", epoch + 1, cfg["training"]["max_epochs"])
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
            model.train()
            train_loss = 0.0
            batch_count = 0
            step_time = 0.0
            step_window = 0
            pos_batches = 0
            total_batches = 0
            pos_tumor_fracs: List[float] = []
            logged_device = False
            for batch_idx, (images, labels) in enumerate(train_loader):
                if cfg["training"].get("limit_train_batches") and batch_idx >= cfg["training"]["limit_train_batches"]:
                    break
                start = time.perf_counter()
                images = images.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)
                if not logged_device:
                    logger.info("Model device: %s | Batch device: %s", next(model.parameters()).device, images.device)
                    logged_device = True
                _validate_labels(labels, num_classes, is_binary)
                optimizer.zero_grad(set_to_none=True)
                with torch.autocast(device_type=device.type, enabled=torch.cuda.is_available()):
                    logits = model(images)
                    loss = loss_fn(logits, labels)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                train_loss += loss.item()
                batch_count += 1
                total_batches += 1
                preds = _pred_to_onehot(logits, num_classes, is_binary, cfg["training"].get("prediction_threshold", 0.5))
                if _foreground_mask(labels).any():
                    pos_batches += 1
                    pos_tumor_fracs.append(_foreground_mask(labels).float().mean().item())
                step_time += time.perf_counter() - start
                step_window += 1
                if global_step < log_sanity_steps or batch_idx == 0:
                    _log_sanity_stats(logger, preds, labels, "train")
                if log_interval and batch_idx % log_interval == 0:
                    avg_step = step_time / max(1, step_window)
                    logger.info(
                        "  train step %s/%s loss=%.4f avg_step=%.3fs",
                        batch_idx + 1,
                        len(train_loader),
                        loss.item(),
                        avg_step,
                    )
                    step_time = 0.0
                    step_window = 0
                global_step += 1

            train_loss /= max(1, batch_count)
            observed_pos_ratio = pos_batches / max(1, total_batches)
            logger.info("Observed train pos_ratio (patches w/ tumor): %.3f", observed_pos_ratio)
            tb_writer.add_scalar("data/pos_ratio", observed_pos_ratio, epoch + 1)
            if pos_tumor_fracs:
                median_pos_frac = float(np.median(pos_tumor_fracs))
                logger.info("Median tumor_voxel_frac (pos patches): %.6f", median_pos_frac)
                tb_writer.add_scalar("data/median_tumor_frac_pos", median_pos_frac, epoch + 1)

            model.eval()
            dice_scores: List[float] = []
            dice_per_class: List[np.ndarray] = []
            dice_sum = np.zeros(num_classes, dtype=np.float64)
            dice_count = np.zeros(num_classes, dtype=np.float64)
            val_loss = 0.0
            val_batches = 0
            debug_shapes = cfg["training"].get("debug_shapes", False)
            with torch.no_grad():
                for val_idx, (images, labels) in enumerate(val_loader):
                    if cfg["training"].get("limit_val_batches") and val_idx >= cfg["training"]["limit_val_batches"]:
                        break
                    images = images.to(device)
                    labels = labels.to(device)
                    _validate_labels(labels, num_classes, is_binary)
                    logits = model(images)
                    val_loss += loss_fn(logits, labels).item()
                    val_batches += 1
                    preds = _pred_to_onehot(logits, num_classes, is_binary, cfg["training"].get("prediction_threshold", 0.5))
                    if debug_shapes and val_idx == 0:
                        logger.info(
                            "val shapes | images=%s labels=%s logits=%s preds=%s",
                            tuple(images.shape),
                            tuple(labels.shape),
                            tuple(logits.shape),
                            tuple(preds.shape),
                        )
                    dice = compute_dice(preds, labels, include_background=True)
                    dice_scores.append(dice.mean().item())
                    dice_per_class.append(dice.mean(dim=0).cpu().numpy())
                    target_sum = labels.sum(dim=(2, 3, 4))
                    for cls_idx in range(num_classes):
                        if cls_idx == 0:
                            valid = np.ones(target_sum.shape[0], dtype=bool)
                        else:
                            valid = target_sum[:, cls_idx].cpu().numpy() > 0
                            if not ignore_empty_foreground:
                                valid = np.ones_like(valid, dtype=bool)
                        dice_vals = dice[:, cls_idx].detach().cpu().numpy()
                        if valid.any():
                            dice_sum[cls_idx] += dice_vals[valid].sum()
                            dice_count[cls_idx] += valid.sum()
                    if log_interval and val_idx % log_interval == 0:
                        logger.info(
                            "  val step %s/%s mean_dice=%.4f",
                            val_idx + 1,
                            len(val_loader),
                            dice.mean().item(),
                        )

            val_mean_dice = float(np.mean(dice_scores)) if dice_scores else 0.0
            val_loss = val_loss / max(1, val_batches)
            lr = optimizer.param_groups[0]["lr"]
            val_dice_per_class = np.where(
                dice_count > 0, dice_sum / np.maximum(dice_count, 1.0), 0.0
            )
            val_dice_background = float(val_dice_per_class[0]) if num_classes > 0 else 0.0
            val_dice_tumor = float(val_dice_per_class[1]) if num_classes > 1 else 0.0
            if num_classes > 1:
                val_foreground_dice = float(np.mean(val_dice_per_class[1:]))
            else:
                val_foreground_dice = float(val_dice_per_class[0]) if num_classes > 0 else 0.0
            csv_writer.writerow(
                [
                    epoch + 1,
                    f"{train_loss:.6f}",
                    f"{val_loss:.6f}",
                    f"{val_mean_dice:.6f}",
                    f"{val_foreground_dice:.6f}",
                    f"{val_dice_background:.6f}",
                    f"{val_dice_tumor:.6f}",
                    f"{lr:.8f}",
                ]
            )
            f.flush()

            tb_writer.add_scalar("loss/train", train_loss, epoch + 1)
            tb_writer.add_scalar("loss/val", val_loss, epoch + 1)
            tb_writer.add_scalar("dice_mean/val", val_mean_dice, epoch + 1)
            tb_writer.add_scalar("dice_foreground/val", val_foreground_dice, epoch + 1)
            if dice_per_class:
                for idx in range(num_classes):
                    tb_writer.add_scalar(
                        f"dice/val_class_{idx}",
                        float(val_dice_per_class[idx]) if idx < len(val_dice_per_class) else 0.0,
                        epoch + 1,
                    )
            if num_classes > 1:
                tb_writer.add_scalar("dice/val_background", val_dice_background, epoch + 1)
                tb_writer.add_scalar("dice/val_tumor", val_dice_tumor, epoch + 1)
            tb_writer.add_scalar("lr", lr, epoch + 1)
            if torch.cuda.is_available():
                gpu_mem = torch.cuda.max_memory_allocated() / (1024**2)
                tb_writer.add_scalar("gpu_mem_max_mb", gpu_mem, epoch + 1)

            if torch.cuda.is_available():
                gpu_mem = torch.cuda.max_memory_allocated() / (1024**2)
            else:
                gpu_mem = 0.0
            logger.info(
                "Epoch %s summary | train_loss=%.4f val_loss=%.4f val_mean_dice=%.4f val_foreground_dice=%.4f val_dice_background=%.4f val_dice_tumor=%.4f lr=%.6f gpu_mem_max_mb=%.1f",
                epoch + 1,
                train_loss,
                val_loss,
                val_mean_dice,
                val_foreground_dice,
                val_dice_background,
                val_dice_tumor,
                lr,
                gpu_mem,
            )

            metrics_history.append(
                {
                    "epoch": epoch + 1,
                    "train_loss": float(train_loss),
                    "val_loss": float(val_loss),
                    "val_mean_dice": float(val_mean_dice),
                    "val_foreground_dice": float(val_foreground_dice),
                    "val_dice_per_class": val_dice_per_class.tolist(),
                    "val_dice_background": float(val_dice_background),
                    "val_dice_tumor": float(val_dice_tumor),
                    "lr": float(lr),
                    "ignore_empty_foreground": bool(ignore_empty_foreground),
                    "include_background": True,
                    "pred_rule": "argmax over softmax logits",
                    "class_names": data_cfg.get("class_names", []),
                }
            )
            with metrics_json_path.open("w", encoding="utf-8") as json_f:
                json.dump(metrics_history, json_f, indent=2)

            improved = False
            if val_foreground_dice > best_fg_dice:
                improved = True
            elif abs(val_foreground_dice - best_fg_dice) < 1e-6 and val_loss < best_val_loss:
                improved = True
            if improved:
                best_fg_dice = val_foreground_dice
                best_val_loss = val_loss
                best_epoch = epoch + 1
                torch.save(model.state_dict(), run_dir / "best.pt")
                logger.info(
                    "New best model | val_tumor_dice=%.4f val_foreground_dice=%.4f epoch=%s",
                    val_dice_tumor,
                    val_foreground_dice,
                    best_epoch,
                )

            if (epoch + 1) % vis_interval == 0:
                _log_visuals(
                    model,
                    val_loader,
                    device,
                    tb_writer,
                    run_dir,
                    epoch + 1,
                    num_classes,
                    max_vis_cases,
                )

    logger.info("Best val tumor/foreground Dice: %.4f at epoch %s", best_fg_dice, best_epoch)
    logger.info("Run directory: %s", run_dir)
    tb_writer.close()


if __name__ == "__main__":
    main()

--- src/training/__init__.py ---


--- src/training/train.py ---
import argparse
import random
from pathlib import Path
import numpy as np
import tensorflow as tf

from src.data.augmentations import get_training_augmentation
from src.data.dataset import build_dataloader
from src.models.unet import build_unet
from src.utils.config import apply_overrides, load_config, resolve_paths


def set_random_seeds(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def create_callbacks(cfg):
    log_dir = cfg["training"]["log_dir"]
    checkpoint_dir = cfg["training"]["checkpoint_dir"]
    checkpoint_path = Path(checkpoint_dir) / cfg["training"]["checkpoint_filename"]

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)
    early_stopping = tf.keras.callbacks.EarlyStopping(
        patience=cfg["training"]["early_stopping_patience"], verbose=1, restore_best_weights=True
    )
    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True
    )
    return [tensorboard_callback, early_stopping, model_checkpoint]


def parse_args():
    parser = argparse.ArgumentParser(description="Train U-Net for brain tumor segmentation")
    parser.add_argument("--config", default="configs/config.yaml", help="Path to YAML config file")
    parser.add_argument("--data-root", dest="data_root", help="Override dataset root directory")
    parser.add_argument("--epochs", type=int, help="Override number of epochs")
    parser.add_argument("--batch-size", type=int, help="Override batch size")
    parser.add_argument("--learning-rate", type=float, dest="learning_rate", help="Override learning rate")
    return parser.parse_args()


def main():
    args = parse_args()
    cfg = load_config(args.config)
    cfg = apply_overrides(cfg, args)
    cfg = resolve_paths(cfg)

    set_random_seeds(cfg["training"].get("seed", 42))

    image_size = cfg["data"]["image_size"]
    input_channels = cfg["model"]["input_channels"]
    class_names = cfg["data"]["class_names"]

    train_aug = get_training_augmentation() if cfg.get("augmentation", {}).get("enable", False) else None

    train_loader = build_dataloader(
        cfg["data"]["train_images"],
        cfg["data"]["train_masks"],
        class_names,
        batch_size=cfg["training"]["batch_size"],
        image_size=image_size,
        augmentation=train_aug,
        shuffle=True,
    )
    val_loader = build_dataloader(
        cfg["data"]["val_images"],
        cfg["data"]["val_masks"],
        class_names,
        batch_size=cfg["training"]["batch_size"],
        image_size=image_size,
        augmentation=None,
        shuffle=False,
    )

    model = build_unet(
        input_size=(image_size, image_size, input_channels),
        num_classes=len(class_names),
        base_filters=cfg["model"].get("base_filters", 32),
        learning_rate=cfg["training"]["learning_rate"],
    )

    steps_per_epoch = cfg["training"].get("steps_per_epoch") or len(train_loader)
    validation_steps = cfg["training"].get("validation_steps") or len(val_loader)

    callbacks = create_callbacks(cfg)

    model.fit(
        train_loader,
        validation_data=val_loader,
        epochs=cfg["training"]["epochs"],
        steps_per_epoch=steps_per_epoch,
        validation_steps=validation_steps,
        callbacks=callbacks,
        use_multiprocessing=cfg["training"].get("use_multiprocessing", False),
        workers=cfg["training"].get("workers", 1),
    )


if __name__ == "__main__":
    main()

--- src/utils/__init__.py ---


--- src/utils/config.py ---
import argparse
import copy
from pathlib import Path
from typing import Any, Dict
import yaml

def load_config(config_path: str) -> Dict[str, Any]:
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    return cfg

def apply_overrides(cfg: Dict[str, Any], args: argparse.Namespace) -> Dict[str, Any]:
    cfg = copy.deepcopy(cfg)
    if getattr(args, "data_root", None):
        cfg["data"]["root"] = args.data_root
    if getattr(args, "epochs", None):
        cfg["training"]["epochs"] = args.epochs
    if getattr(args, "batch_size", None):
        cfg["training"]["batch_size"] = args.batch_size
    if getattr(args, "learning_rate", None):
        cfg["training"]["learning_rate"] = args.learning_rate
    return cfg

def resolve_paths(cfg: Dict[str, Any]) -> Dict[str, Any]:
    cfg = copy.deepcopy(cfg)
    data_root = Path(cfg["data"]["root"]).expanduser().resolve()
    cfg["data"]["root"] = data_root
    for key in [
        "train_images",
        "train_masks",
        "val_images",
        "val_masks",
        "test_images",
        "test_masks",
    ]:
        cfg["data"][key] = (data_root / cfg["data"][key]).resolve()
    training = cfg.get("training", {})
    for path_key in ["log_dir", "checkpoint_dir"]:
        if path_key in training:
            training[path_key] = Path(training[path_key]).expanduser().resolve()
            training[path_key].mkdir(parents=True, exist_ok=True)
    return cfg

--- tests/test_smoke_3d_pipeline.py ---
import json
from pathlib import Path

import nibabel as nib
import numpy as np
import torch
import yaml
from monai.networks.nets import UNet

from src.data.msd_task01_3d import MSDTask01Dataset3D, list_msd_task01_cases
from src.eval_3d import main as eval_main


def _write_msd_case(root: Path):
    images_dir = root / "imagesTr"
    labels_dir = root / "labelsTr"
    images_dir.mkdir(parents=True)
    labels_dir.mkdir(parents=True)

    image = np.random.rand(16, 16, 16, 4).astype("float32")
    label = np.zeros((16, 16, 16), dtype="uint8")
    label[4:8, 4:8, 4:8] = 1

    image_path = images_dir / "case_000.nii.gz"
    label_path = labels_dir / "case_000.nii.gz"
    nib.save(nib.Nifti1Image(image, affine=np.eye(4)), str(image_path))
    nib.save(nib.Nifti1Image(label, affine=np.eye(4)), str(label_path))


def test_3d_dataset_model_eval(tmp_path: Path, monkeypatch):
    _write_msd_case(tmp_path)
    cases = list_msd_task01_cases(tmp_path)
    dataset = MSDTask01Dataset3D(
        cases,
        roi_size=(16, 16, 16),
        label_mode="binary",
        num_classes=2,
        pos_ratio=1.0,
        percentiles=(0.5, 99.5),
        mode="train",
        seed=123,
    )

    image, label = dataset[0]
    assert image.shape == (4, 16, 16, 16)
    assert label.shape == (2, 16, 16, 16)

    model = UNet(
        spatial_dims=3,
        in_channels=4,
        out_channels=2,
        channels=(8, 16),
        strides=(2,),
        num_res_units=1,
        norm="batch",
    )
    with torch.no_grad():
        output = model(image.unsqueeze(0))
    assert output.shape[1] == 2

    weights_path = tmp_path / "weights.pt"
    torch.save(model.state_dict(), weights_path)

    config = {
        "data": {
            "root": str(tmp_path),
            "train_ratio": 0.0,
            "val_ratio": 1.0,
            "seed": 42,
            "label_mode": "binary",
            "class_names": ["background", "tumor"],
            "roi_size": [16, 16, 16],
            "pos_ratio": 1.0,
            "percentiles": [0.5, 99.5],
        },
        "model": {
            "in_channels": 4,
            "channels": [8, 16],
            "strides": [2],
            "num_res_units": 1,
            "norm": "batch",
        },
        "training": {
            "batch_size": 1,
            "learning_rate": 0.0001,
            "max_epochs": 1,
            "num_workers": 0,
            "seed": 42,
            "deterministic": True,
            "output_dir": "./outputs/runs",
            "limit_train_batches": 1,
            "limit_val_batches": 1,
        },
        "inference": {"roi_size": [16, 16, 16], "overlap": 0.0, "sw_batch_size": 1},
    }
    config_path = tmp_path / "config_3d.yaml"
    with config_path.open("w", encoding="utf-8") as f:
        yaml.safe_dump(config, f, sort_keys=False)

    monkeypatch.chdir(tmp_path)
    monkeypatch.setattr(
        "sys.argv",
        ["python", "-m", "src.eval_3d", "--config", str(config_path), "--weights", str(weights_path)],
    )
    eval_main()

    metrics_path = tmp_path / "outputs" / "metrics_3d.json"
    assert metrics_path.exists()
    metrics = json.loads(metrics_path.read_text())
    assert "val" in metrics and "test" in metrics

--- tests/test_smoke_pipeline.py ---
from pathlib import Path

import nibabel as nib
import numpy as np

import imageio

from src.data import prepare_slices


def test_prepare_slices_msd_task01(tmp_path: Path):
    images_dir = tmp_path / "imagesTr"
    labels_dir = tmp_path / "labelsTr"
    images_dir.mkdir(parents=True)
    labels_dir.mkdir(parents=True)

    image = np.random.rand(32, 32, 8, 4).astype("float32")
    label = np.zeros((32, 32, 8), dtype="uint8")
    label[8:16, 8:16, 3] = 1

    image_path = images_dir / "brain_000.nii.gz"
    label_path = labels_dir / "brain_000.nii.gz"
    nib.save(nib.Nifti1Image(image, affine=np.eye(4)), str(image_path))
    nib.save(nib.Nifti1Image(label, affine=np.eye(4)), str(label_path))

    output_root = tmp_path / "Dataset"
    prepare_slices.prepare_dataset(
        dataset_root=tmp_path,
        output_root=output_root,
        slices_per_volume=2,
        dataset_format="msd_task01",
        channel="flair",
        label_mode="binary",
    )

    pngs = sorted(output_root.rglob("*.png"))
    assert len(pngs) == 4

    mask_paths = sorted((output_root / "test_masks" / "test").glob("*.png"))
    assert mask_paths
    mask = imageio.imread(mask_paths[0])
    assert mask.ndim == 2
    assert mask.dtype == np.uint8
    assert set(np.unique(mask)).issubset({0, 1})

--- train.py ---
"""Entry point for training using the modular pipeline."""
from src.training.train import main


if __name__ == "__main__":
    main()
