File structure:
- .gitignore
- .dockerignore
- README.md
- Dockerfile
- requirements.txt
- configs/config.yaml
- train.py
- src/utils/config.py
- src/data/augmentations.py
- src/data/dataset.py
- src/data/prepare_slices.py
- src/data/bias_correction.py
- src/models/unet.py
- src/training/train.py
- src/service/api.py

--- .gitignore ---
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
.Python

# Virtual environments
.venv/
venv/
env/

# Distribution / packaging
build/
dist/
*.egg-info/
*.egg

# IDE / editor
.DS_Store
.idea/
.vscode/

# Logs and cache
*.log
.pytest_cache/

# TensorBoard / checkpoints / outputs
outputs/
logs/

# Weights and datasets
weights/
Dataset/

# Docker
*.pid

# Misc
*.tmp
*.swp

--- .dockerignore ---
Dataset/
weights/
outputs/
logs/
__pycache__/
venv/
.git/
.vscode/

--- README.md ---
# Brain Tumor Segmentation Pipeline (BraTS 2019)

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/) [![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/) [![Docker](https://img.shields.io/badge/Docker-ready-2496ED.svg)](https://www.docker.com/) [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE) [![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](#)

## Project Overview (Why)
Glioma segmentation on MRI is critical for surgical planning and treatment response tracking, but manual delineation is slow and subjective. This pipeline automates multi-modal MRI segmentation (T1, T1ce, T2, FLAIR) from BraTS 2019 using a U-Net tailored for medical images. It tackles class imbalance and boundary precision with hybrid Dice + categorical crossentropy loss, while robust augmentations and N4 bias correction improve signal consistency across scanners.

## System Architecture
```mermaid
graph TD
    A[NIfTI Volumes] --> B[Preprocessing<br/>Bias Correction / Normalization]
    B --> C[Data Augmentation]
    C --> D[U-Net Model]
    D --> E[Inference API<br/>(FastAPI)]
    E --> F[Output<br/>Segmentation Mask]
```

## Key Features
- N4 Bias Field Correction with SimpleITK to stabilize intensity profiles.
- Custom Data Loader for PNG slices with class-consistent pairing and one-hot masks.
- Hybrid Loss Function (Dice + Categorical Crossentropy) to sharpen tumor boundaries under class imbalance.
- Containerized Inference via FastAPI + Docker, deployable on GCP Vertex AI.

## Getting Started (Local)
1) Clone: `git clone https://github.com/your-org/brain-tumor-segmentation-pipeline.git && cd brain-tumor-segmentation-pipeline`
2) Install deps: `pip install -r requirements.txt`
3) Train: `python train.py --config configs/config.yaml --data-root ./Dataset`
   - Prepare data first with `python -m src.data.prepare_slices --dataset-root /path/to/brats2019 --output-root ./Dataset`
   - Optional bias correction: `python -m src.data.bias_correction --input-dir /path/to/brats2019 --output-dir /path/to/brats2019_preprocessed`

## Docker & Deployment
- Build image: `docker build -t brain-seg:latest .`
- Run container (serving on 8080): `docker run -p 8080:8080 brain-seg:latest`
- FastAPI endpoints:
  - `/health` — readiness check (Vertex AI compatible).
  - `/predict` — returns PNG mask for an uploaded image (`UploadFile`).
  - `/predict-json` — returns class map as JSON.

## Results
**Qualitative Results** (drop in GIFs or side-by-side examples here)

---
Tech stack: TensorFlow/Keras, SimpleITK, Nibabel, Albumentations, FastAPI, Docker, Vertex AI.***

--- Dockerfile ---
# Lightweight production image for FastAPI inference
FROM python:3.9-slim

# Prevent Python from buffering stdout/stderr and writing .pyc files
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app

# System dependencies for OpenCV
RUN apt-get update && apt-get install -y --no-install-recommends \
    libgl1-mesa-glx \
    libglib2.0-0 \
 && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port for Vertex AI
EXPOSE 8080

# Start FastAPI with uvicorn
CMD ["uvicorn", "src.service.api:app", "--host", "0.0.0.0", "--port", "8080"]

--- requirements.txt ---
albumentations
imageio
ipython
matplotlib
nibabel
numpy
opencv-python-headless
pillow
pyyaml
SimpleITK
tensorflow
fastapi
uvicorn
python-multipart

--- configs/config.yaml ---
# Default configuration for brain tumor segmentation

data:
  root: ./Dataset
  train_images: train_frames/train
  train_masks: train_masks/train
  val_images: val_frames/val
  val_masks: val_masks/val
  test_images: test_frames/test
  test_masks: test_masks/test
  class_names: [background, non-enhancing, edema, enhancing]
  image_size: 256

augmentation:
  enable: true

training:
  batch_size: 16
  epochs: 20
  learning_rate: 0.0001
  steps_per_epoch: null
  validation_steps: null
  early_stopping_patience: 10
  use_multiprocessing: true
  workers: 4
  seed: 42
  log_dir: ./outputs/logs
  checkpoint_dir: ./outputs/checkpoints
  checkpoint_filename: best_model_unet.h5

model:
  input_channels: 1
  base_filters: 32

--- train.py ---
"""Entry point for training using the modular pipeline."""
from src.training.train import main


if __name__ == "__main__":
    main()

--- src/utils/config.py ---
import argparse
import copy
from pathlib import Path
from typing import Any, Dict
import yaml

def load_config(config_path: str) -> Dict[str, Any]:
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    return cfg

def apply_overrides(cfg: Dict[str, Any], args: argparse.Namespace) -> Dict[str, Any]:
    cfg = copy.deepcopy(cfg)
    if getattr(args, "data_root", None):
        cfg["data"]["root"] = args.data_root
    if getattr(args, "epochs", None):
        cfg["training"]["epochs"] = args.epochs
    if getattr(args, "batch_size", None):
        cfg["training"]["batch_size"] = args.batch_size
    if getattr(args, "learning_rate", None):
        cfg["training"]["learning_rate"] = args.learning_rate
    return cfg

def resolve_paths(cfg: Dict[str, Any]) -> Dict[str, Any]:
    cfg = copy.deepcopy(cfg)
    data_root = Path(cfg["data"]["root"]).expanduser().resolve()
    cfg["data"]["root"] = data_root
    for key in [
        "train_images",
        "train_masks",
        "val_images",
        "val_masks",
        "test_images",
        "test_masks",
    ]:
        cfg["data"][key] = (data_root / cfg["data"][key]).resolve()
    training = cfg.get("training", {})
    for path_key in ["log_dir", "checkpoint_dir"]:
        if path_key in training:
            training[path_key] = Path(training[path_key]).expanduser().resolve()
            training[path_key].mkdir(parents=True, exist_ok=True)
    return cfg

--- src/data/augmentations.py ---
import albumentations as A


def get_training_augmentation():
    """Return albumentations Compose for training images and masks."""
    return A.Compose([
        A.OneOf([
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.Rotate(limit=(0, 90), p=0.5),
            A.ShiftScaleRotate(shift_limit=(0, 0.1), rotate_limit=(0, 0), scale_limit=(0, 0), p=0.5),
            A.Transpose(p=0.5),
        ], p=1),
    ])

--- src/data/dataset.py ---
import math
from pathlib import Path
from typing import Iterable
import cv2
import numpy as np
from tensorflow.keras.utils import Sequence, to_categorical


class SliceDataset:
    def __init__(
        self,
        images_dir: Path,
        masks_dir: Path,
        class_names: Iterable[str],
        image_size: int = 256,
        augmentation=None,
    ):
        self.images_dir = Path(images_dir)
        self.masks_dir = Path(masks_dir)
        self.image_ids = sorted(self.images_dir.glob("*.png"))
        self.mask_ids = sorted(self.masks_dir.glob("*.png"))
        if len(self.image_ids) != len(self.mask_ids):
            raise ValueError("Number of images and masks does not match")
        self.num_classes = len(list(class_names))
        self.image_size = (image_size, image_size)
        self.augmentation = augmentation

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx: int):
        image_path = self.image_ids[idx]
        mask_path = self.mask_ids[idx]

        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
        if image is None or mask is None:
            raise ValueError(f"Failed to read image or mask for index {idx}")

        mask = np.where(mask == 4, 3, mask)
        image = cv2.resize(image, self.image_size, interpolation=cv2.INTER_NEAREST)
        mask = cv2.resize(mask, self.image_size, interpolation=cv2.INTER_NEAREST)

        if self.augmentation:
            sample = self.augmentation(image=image, mask=mask)
            image, mask = sample["image"], sample["mask"]

        image = image.astype("float32") / 255.0
        image = np.expand_dims(image, axis=-1)
        mask = to_categorical(mask, num_classes=self.num_classes).astype("float32")
        return image, mask


class DataLoader(Sequence):
    def __init__(
        self,
        dataset: SliceDataset,
        batch_size: int = 1,
        shuffle: bool = False,
    ):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexes = np.arange(len(dataset))
        self.on_epoch_end()

    def __len__(self):
        return math.ceil(len(self.indexes) / self.batch_size)

    def __getitem__(self, idx):
        batch_indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]
        batch = [self.dataset[i] for i in batch_indexes]
        images, masks = zip(*batch)
        return np.stack(images, axis=0), np.stack(masks, axis=0)

    def on_epoch_end(self):
        if self.shuffle:
            self.indexes = np.random.permutation(self.indexes)
        else:
            self.indexes = np.arange(len(self.dataset))


def build_dataloader(
    images_dir: Path,
    masks_dir: Path,
    class_names: Iterable[str],
    batch_size: int,
    image_size: int,
    augmentation=None,
    shuffle: bool = False,
) -> DataLoader:
    dataset = SliceDataset(images_dir, masks_dir, class_names, image_size, augmentation)
    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)

--- src/data/prepare_slices.py ---
import argparse
from pathlib import Path
from typing import Dict, List, Tuple

import imageio
import nibabel as nib
import numpy as np


def select_slice_indices(image_array: np.ndarray, slices_per_volume: int) -> List[int]:
    """Select slice indices with highest voxel sums (proxy for information content)."""
    sums = [np.sum(image_array[:, :, i]) for i in range(image_array.shape[2])]
    top_indices = np.argsort(sums)[::-1][:slices_per_volume]
    return top_indices.tolist()


def save_slices(
    nii_path: Path,
    slice_indices: List[int],
    output_dir: Path,
    prefix: str,
    counter_offset: int,
    cast_uint8: bool = False,
):
    image_array = nib.load(str(nii_path)).get_fdata()
    for idx, slice_idx in enumerate(slice_indices):
        data = image_array[:, :, slice_idx]
        filename = f"{prefix}_{counter_offset + idx:05d}.png"
        output_path = output_dir / filename
        if cast_uint8:
            imageio.imwrite(output_path, data.astype("uint8"))
        else:
            imageio.imwrite(output_path, data)


def build_file_lists(dataset_root: Path) -> Tuple[List[Path], List[Path]]:
    modalities = []
    for pattern in ["*t1.nii.gz", "*t1ce.nii.gz", "*t2.nii.gz", "*flair.nii.gz"]:
        modalities.extend(sorted(dataset_root.rglob(pattern)))
    segmentations = sorted(dataset_root.rglob("*seg.nii.gz"))
    # replicate segmentations to align with 4 modalities per case
    segmentations = segmentations * 4
    return modalities, segmentations


def prepare_dataset(
    dataset_root: Path,
    output_root: Path,
    slices_per_volume: int = 20,
    train_ratio: float = 0.7,
    val_ratio: float = 0.2,
):
    output_paths = {
        "train_images": output_root / "train_frames" / "train",
        "train_masks": output_root / "train_masks" / "train",
        "val_images": output_root / "val_frames" / "val",
        "val_masks": output_root / "val_masks" / "val",
        "test_images": output_root / "test_frames" / "test",
        "test_masks": output_root / "test_masks" / "test",
    }
    for path in output_paths.values():
        path.mkdir(parents=True, exist_ok=True)

    brains, segs = build_file_lists(dataset_root)
    if len(brains) != len(segs):
        raise ValueError("Number of modality volumes does not match segmentation volumes")

    total = len(brains)
    train_split = int(train_ratio * total)
    val_split = int((train_ratio + val_ratio) * total)

    splits = {
        "train": (brains[:train_split], segs[:train_split]),
        "val": (brains[train_split:val_split], segs[train_split:val_split]),
        "test": (brains[val_split:], segs[val_split:]),
    }

    slice_cache: Dict[Path, List[int]] = {}

    for split_name, (brain_paths, seg_paths) in splits.items():
        for idx, (brain_path, seg_path) in enumerate(zip(brain_paths, seg_paths)):
            if seg_path not in slice_cache:
                image_array = nib.load(str(seg_path)).get_fdata()
                slice_cache[seg_path] = select_slice_indices(image_array, slices_per_volume)

            slice_indices = slice_cache[seg_path]
            counter_offset = idx * slices_per_volume

            img_dir_key = f"{split_name}_images"
            mask_dir_key = f"{split_name}_masks"

            save_slices(brain_path, slice_indices, output_paths[img_dir_key], f"{split_name}_frame", counter_offset)
            save_slices(seg_path, slice_indices, output_paths[mask_dir_key], f"{split_name}_mask", counter_offset, cast_uint8=True)

    print(f"Finished preparing dataset at {output_root}")


def parse_args():
    parser = argparse.ArgumentParser(description="Convert BraTS NIfTI volumes to PNG slices")
    parser.add_argument("--dataset-root", required=True, help="Path to raw BraTS dataset root")
    parser.add_argument("--output-root", default="./Dataset", help="Where to store PNG slices")
    parser.add_argument("--slices-per-volume", type=int, default=20, help="Number of slices to export per volume")
    return parser.parse_args()


def main():
    args = parse_args()
    dataset_root = Path(args.dataset_root).expanduser().resolve()
    output_root = Path(args.output_root).expanduser().resolve()

    prepare_dataset(dataset_root=dataset_root, output_root=output_root, slices_per_volume=args.slices_per_volume)


if __name__ == "__main__":
    main()

--- src/data/bias_correction.py ---
import argparse
import glob
import os
import shutil
from pathlib import Path
from typing import Iterable

import SimpleITK as sitk

MODALITIES = ("flair", "t1", "t1ce", "t2")


def correct_bias(in_path, out_path, image_type=sitk.sitkFloat64):
    input_image = sitk.ReadImage(in_path, image_type)
    output_image = sitk.N4BiasFieldCorrection(input_image, input_image > 0)
    sitk.WriteImage(output_image, out_path)
    return os.path.abspath(out_path)


def get_image_path(subject_folder, name):
    file_name = os.path.join(subject_folder, f"*{name}.nii.gz")
    matches = glob.glob(file_name)
    if not matches:
        raise FileNotFoundError(f"Could not find modality {name} in {subject_folder}")
    return matches[0]


def normalize_image(in_path, out_path, bias_correction=True):
    if bias_correction:
        correct_bias(in_path, out_path)
    else:
        shutil.copy(in_path, out_path)


def preprocess_brats_folder(
    in_folder: Path,
    out_folder: Path,
    modalities: Iterable[str],
    truth_name: str,
    no_bias_correction_modalities: Iterable[str],
):
    for name in modalities:
        image_image = get_image_path(in_folder, name)
        case_id = os.path.basename(out_folder)
        out_path = os.path.abspath(os.path.join(out_folder, f"{case_id}_{name}.nii.gz"))
        perform_bias_correction = name not in no_bias_correction_modalities
        normalize_image(image_image, out_path, bias_correction=perform_bias_correction)

    truth_image = get_image_path(in_folder, truth_name)
    out_path = os.path.abspath(os.path.join(out_folder, f"{case_id}_truth.nii.gz"))
    shutil.copy(truth_image, out_path)


def preprocess_brats_data(
    brats_folder: Path,
    out_folder: Path,
    overwrite: bool = False,
    no_bias_correction_modalities: Iterable[str] = ("flair",),
    modalities: Iterable[str] = MODALITIES,
):
    for subject_folder in glob.glob(os.path.join(brats_folder, "*", "*")):
        if os.path.isdir(subject_folder):
            subject = os.path.basename(subject_folder)
            new_subject_folder = os.path.join(out_folder, os.path.basename(os.path.dirname(subject_folder)), subject)
            if not os.path.exists(new_subject_folder) or overwrite:
                if not os.path.exists(new_subject_folder):
                    os.makedirs(new_subject_folder)
                preprocess_brats_folder(
                    Path(subject_folder),
                    Path(new_subject_folder),
                    modalities=modalities,
                    truth_name="seg",
                    no_bias_correction_modalities=no_bias_correction_modalities,
                )


def parse_args():
    parser = argparse.ArgumentParser(description="Run N4 bias-field correction on BraTS data")
    parser.add_argument("--input-dir", required=True, help="Path to raw BraTS dataset")
    parser.add_argument("--output-dir", required=True, help="Where to write corrected data")
    parser.add_argument("--skip-modalities", nargs="*", default=["flair"], help="Modalities to skip bias correction")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing output")
    return parser.parse_args()


def main():
    args = parse_args()
    input_dir = Path(args.input_dir).expanduser().resolve()
    output_dir = Path(args.output_dir).expanduser().resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    preprocess_brats_data(
        brats_folder=input_dir,
        out_folder=output_dir,
        overwrite=args.overwrite,
        no_bias_correction_modalities=args.skip_modalities,
    )


if __name__ == "__main__":
    main()

--- src/models/unet.py ---
import tensorflow as tf
from tensorflow.keras.layers import Activation, Concatenate, Conv2D, Conv2DTranspose, Input, MaxPooling2D
from tensorflow.keras.optimizers import Adam


def dice_coefficient(y_true, y_pred, smooth: float = 1e-6):
    """Dice metric for one-hot encoded masks."""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    numerator = 2.0 * tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])
    denominator = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3])
    dice = (numerator + smooth) / (denominator + smooth)
    return tf.reduce_mean(dice)


def soft_dice_loss(y_true, y_pred, smooth: float = 1e-6):
    dice = dice_coefficient(y_true, y_pred, smooth)
    return 1.0 - dice


_cce = tf.keras.losses.CategoricalCrossentropy()


def combined_cce_dice_loss(y_true, y_pred):
    """Categorical crossentropy + soft dice to sharpen boundaries."""
    return _cce(y_true, y_pred) + soft_dice_loss(y_true, y_pred)


def build_unet(
    input_size=(256, 256, 1),
    num_classes: int = 4,
    base_filters: int = 32,
    learning_rate: float = 1e-4,
) -> tf.keras.Model:
    initializer = "he_normal"

    inputs = Input(shape=input_size)

    conv1 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(inputs)
    conv1 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool1)
    conv2 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool2)
    conv3 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool3)
    conv4 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    conv5 = Conv2D(base_filters * 16, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(pool4)
    conv5 = Conv2D(base_filters * 16, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv5)

    up6 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters * 8, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv5),
        conv4,
    ])
    conv6 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up6)
    conv6 = Conv2D(base_filters * 8, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv6)

    up7 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters * 4, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv6),
        conv3,
    ])
    conv7 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up7)
    conv7 = Conv2D(base_filters * 4, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv7)

    up8 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters * 2, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv7),
        conv2,
    ])
    conv8 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up8)
    conv8 = Conv2D(base_filters * 2, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv8)

    up9 = Concatenate(axis=3)([
        Conv2DTranspose(base_filters, (2, 2), strides=(2, 2), padding="same", kernel_initializer=initializer)(conv8),
        conv1,
    ])
    conv9 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(up9)
    conv9 = Conv2D(base_filters, (3, 3), activation="relu", padding="same", kernel_initializer=initializer)(conv9)

    conv10 = Conv2D(num_classes, (1, 1), activation="relu", kernel_initializer=initializer)(conv9)
    outputs = Activation("softmax")(conv10)

    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss=combined_cce_dice_loss,
        metrics=[dice_coefficient, "accuracy"],
    )
    return model

--- src/training/train.py ---
import argparse
import random
from pathlib import Path
import numpy as np
import tensorflow as tf

from src.data.augmentations import get_training_augmentation
from src.data.dataset import build_dataloader
from src.models.unet import build_unet
from src.utils.config import apply_overrides, load_config, resolve_paths


def set_random_seeds(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def create_callbacks(cfg):
    log_dir = cfg["training"]["log_dir"]
    checkpoint_dir = cfg["training"]["checkpoint_dir"]
    checkpoint_path = Path(checkpoint_dir) / cfg["training"]["checkpoint_filename"]

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)
    early_stopping = tf.keras.callbacks.EarlyStopping(
        patience=cfg["training"]["early_stopping_patience"], verbose=1, restore_best_weights=True
    )
    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True
    )
    return [tensorboard_callback, early_stopping, model_checkpoint]


def parse_args():
    parser = argparse.ArgumentParser(description="Train U-Net for brain tumor segmentation")
    parser.add_argument("--config", default="configs/config.yaml", help="Path to YAML config file")
    parser.add_argument("--data-root", dest="data_root", help="Override dataset root directory")
    parser.add_argument("--epochs", type=int, help="Override number of epochs")
    parser.add_argument("--batch-size", type=int, help="Override batch size")
    parser.add_argument("--learning-rate", type=float, dest="learning_rate", help="Override learning rate")
    return parser.parse_args()


def main():
    args = parse_args()
    cfg = load_config(args.config)
    cfg = apply_overrides(cfg, args)
    cfg = resolve_paths(cfg)

    set_random_seeds(cfg["training"].get("seed", 42))

    image_size = cfg["data"]["image_size"]
    input_channels = cfg["model"]["input_channels"]
    class_names = cfg["data"]["class_names"]

    train_aug = get_training_augmentation() if cfg.get("augmentation", {}).get("enable", False) else None

    train_loader = build_dataloader(
        cfg["data"]["train_images"],
        cfg["data"]["train_masks"],
        class_names,
        batch_size=cfg["training"]["batch_size"],
        image_size=image_size,
        augmentation=train_aug,
        shuffle=True,
    )
    val_loader = build_dataloader(
        cfg["data"]["val_images"],
        cfg["data"]["val_masks"],
        class_names,
        batch_size=cfg["training"]["batch_size"],
        image_size=image_size,
        augmentation=None,
        shuffle=False,
    )

    model = build_unet(
        input_size=(image_size, image_size, input_channels),
        num_classes=len(class_names),
        base_filters=cfg["model"].get("base_filters", 32),
        learning_rate=cfg["training"]["learning_rate"],
    )

    steps_per_epoch = cfg["training"].get("steps_per_epoch") or len(train_loader)
    validation_steps = cfg["training"].get("validation_steps") or len(val_loader)

    callbacks = create_callbacks(cfg)

    model.fit(
        train_loader,
        validation_data=val_loader,
        epochs=cfg["training"]["epochs"],
        steps_per_epoch=steps_per_epoch,
        validation_steps=validation_steps,
        callbacks=callbacks,
        use_multiprocessing=cfg["training"].get("use_multiprocessing", False),
        workers=cfg["training"].get("workers", 1),
    )


if __name__ == "__main__":
    main()

--- src/service/api.py ---
import io
from functools import lru_cache
from pathlib import Path

import numpy as np
from fastapi import FastAPI, File, HTTPException, UploadFile
from fastapi.responses import JSONResponse, StreamingResponse
from PIL import Image

from src.models.unet import build_unet

app = FastAPI(title="Brain Tumor Segmentation API", version="1.0")

WEIGHTS_PATH = Path("./weights/best_model_unet.h5").expanduser().resolve()
IMAGE_SIZE = 256
INPUT_CHANNELS = 1
NUM_CLASSES = 4


@lru_cache(maxsize=1)
def load_model():
    if not WEIGHTS_PATH.exists():
        raise FileNotFoundError(f"Model weights not found at {WEIGHTS_PATH}")
    model = build_unet(
        input_size=(IMAGE_SIZE, IMAGE_SIZE, INPUT_CHANNELS),
        num_classes=NUM_CLASSES,
        base_filters=32,
    )
    model.load_weights(WEIGHTS_PATH)
    return model


def preprocess_image(file_bytes: bytes) -> np.ndarray:
    try:
        image = Image.open(io.BytesIO(file_bytes)).convert("L")
    except Exception as exc:  # pillow-specific errors vary
        raise HTTPException(status_code=400, detail=f"Invalid image: {exc}")
    image = image.resize((IMAGE_SIZE, IMAGE_SIZE))
    array = np.asarray(image).astype("float32") / 255.0
    array = np.expand_dims(array, axis=(0, -1))  # shape: (1, H, W, 1)
    return array


def postprocess_mask(prediction: np.ndarray) -> Image.Image:
    """Convert softmax output to single-channel mask PNG."""
    # prediction shape: (1, H, W, num_classes)
    class_map = np.argmax(prediction, axis=-1)[0].astype("uint8")
    mask = Image.fromarray(class_map, mode="L")
    return mask


@app.get("/health")
def health():
    return {"status": "ok"}


@app.post("/predict")
def predict(file: UploadFile = File(...)):
    model = load_model()
    file_bytes = file.file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Empty file")

    input_tensor = preprocess_image(file_bytes)
    prediction = model.predict(input_tensor)
    mask_img = postprocess_mask(prediction)

    buffer = io.BytesIO()
    mask_img.save(buffer, format="PNG")
    buffer.seek(0)

    headers = {"Content-Disposition": f"inline; filename=\"mask_{file.filename or 'output'}.png\""}
    return StreamingResponse(buffer, media_type="image/png", headers=headers)


@app.post("/predict-json")
def predict_json(file: UploadFile = File(...)):
    """Alternative JSON response returning mask values."""
    model = load_model()
    file_bytes = file.file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Empty file")

    input_tensor = preprocess_image(file_bytes)
    prediction = model.predict(input_tensor)
    class_map = np.argmax(prediction, axis=-1)[0].astype(int).tolist()
    return JSONResponse({"mask": class_map})
